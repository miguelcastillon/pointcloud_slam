Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Modarress2007,
abstract = {This manuscript describes recent progress in the development and fielding of optical MEMS-based Mini-and Micro-sensors for surface and underwater applications. The non-intrusive sensors have been developed to measure vehicle speed, boundary layer velocity profile, and skin friction. The MiniLDVTM and Micro velocimeters have been used as a speed sensor in torpedoes, scaled submarines, and surface vessels at sea and in tow tanks. The same instruments have also been used for boundary layer profilometry at variety of Reynolds numbers. MicroSTM shear stress sensors have been used for drag reduction studies in full-scale sea trials. The small size and integrated electronics of the micro-sensors make them especially appropriate for applications in UUV and AUV. In all cases, particles present in the ocean water were found to be adequate both in concentration and size as scattering sources for the measurement. The building blocks for MEMS-based optical sensors are described and examples of sea trials are presented.},
author = {Modarress, D. and Svitek, Pavel and Modarress, Katy and Wilson, Daniel W.},
booktitle = {International Symposium on Underwater Technology, UT 2007 - International Workshop on Scientific Use of Submarine Cables and Related Technologies 2007},
doi = {10.1109/UT.2007.370801},
file = {:home/miguel/Dropbox/Mendeley Desktop/Modarress et al. - 2007 - Micro-optical sensors for underwater velocity measurement.pdf:pdf},
isbn = {1424412080},
month = {apr},
pages = {235--239},
publisher = {IEEE},
title = {{Micro-optical sensors for underwater velocity measurement}},
url = {http://ieeexplore.ieee.org/document/4231131/},
year = {2007}
}
@inproceedings{Kimoto2014,
abstract = {We have developed a new type of 3D LIDAR, which is small, light weight and low cost. The size and weight are important factors of the aimed sensor to use with wide range of robot vehicles, helicopters and airplane, and small vehicles. To achieve compact size and light weight construction the sensor has single pair of laser transmitter and receiver, and scanning motor on which a vibrating mirror is placed. The mirror is driven by the resonant frequency current supplied through contactless power supply unit. Experimental results of the prototype shows that the sensor of comparable size and weight with recent 2D sensors can obtain 3D range data of 8000 points in 20 Hz with 270 degree horizontal and 40 degree vertical views both in indoor and outdoor.},
author = {Kimoto, Katsumi and Asada, Norihiro and Mori, Toshihiro and Hara, Yoshitaka and Ohya, Akihisa and Yuta, Shin'ichi},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6907534},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kimoto et al. - 2014 - Development of small size 3D LIDAR.pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {10504729},
month = {may},
pages = {4620--4626},
publisher = {IEEE},
title = {{Development of small size 3D LIDAR}},
url = {http://ieeexplore.ieee.org/document/6907534/},
year = {2014}
}
@inproceedings{Kim2017,
abstract = {We designed a prototype for testing feasibility of a proposed light detection and ranging (LIDAR) system, which was designed to encode pixel location information in its laser pulses using the direct-sequence optical code division multiple access method in conjunction with a scanning-based microelectromechanical system (MEMS) mirror. The prototype was built using commercial o -the-shelf optical components and development kits. It comprised of an optical modulator, an amplified photodetector, an MEMS mirror development kit, an analog-to-digital converter evaluation module, a digital signal processor with ARM evaluation kit and a Windows personal computer. The prototype LIDAR system has capable of acquiring 120 x 32-pixel images at 5 frames/s. We measured a watering pot to demonstrate the imaging performance of the prototype LIDAR system.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1117/12.2251189},
editor = {Eldada, Louay A. and Lee, El-Hang and He, Sailing},
isbn = {9781628419863},
issn = {1996756X},
keywords = {DS-OCDMA,LIDAR,Laser pulse,Time-of-Flight},
month = {feb},
pages = {1010710},
publisher = {International Society for Optics and Photonics},
title = {{Design and implementation of 3D LIDAR based on pixel-by-pixel scanning and DS-OCDMA}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2251189},
volume = {10107},
year = {2017}
}
@article{Shin,
abstract = {We propose a biaxial scanning mirror with a large rotation angle and low resonance frequency for a compact and low-power-consuming LIDAR. The scanning mirror in LIDAR, which is driven by a rotating motor, requires a wide field of view and a low working frequency. To achieve these requirements, we develop an electromagnetic actuator for a biaxial scanning mirror that consists of two pairs of coils, one yoke with a cross shape, one rare-earth permanent magnet, and one gimbal structure frame. The gap distance between the permanent magnet and yoke is adjusted to find the optimum condition. The overall size of the developed system is 20 mm 9 20 mm 9 12 mm (width 9 depth 9 height) with a gap distance of 3 mm. Experiments and simulations are performed with various gap distances. The experimental results indicate that the maximum rotation angle is ± 51° at 37 Hz when the gap distance is 3 mm and the applied voltage is ± 5 V.},
author = {Shin, Buhyun and Oh, Dongho and min Lee, Kyung},
doi = {10.1007/s00542-018-3858-6},
file = {:home/miguel/Dropbox/Mendeley Desktop/Shin, Oh, Lee - 2018 - Biaxial scanning mirror with large rotation angle and low resonance frequency for LIDAR application.pdf:pdf},
isbn = {0123456789},
issn = {09467076},
journal = {Microsystem Technologies},
number = {11},
pages = {4631--4639},
title = {{Biaxial scanning mirror with large rotation angle and low resonance frequency for LIDAR application}},
url = {https://doi.org/10.1007/s00542-018-3858-6},
volume = {24},
year = {2018}
}
@article{Hornbeck2001,
abstract = {The possibility of an all-digital (sourceto-eye) projection display was realized in 1987 with the invention of the Digital Micromirror Device projection display chip at Texas Instruments (TI). The DMD chip is a microelectromechanical systems (MEMS) array of fast digital micromirrors, monolithically integrated onto and controlled by an underlying silicon memory chip. Digital Light Processing projection displays are based on the DMD chip. DLP projection displays present bright, seamless images to the eye that have high image fidelity, and stability.},
author = {Hornbeck, Larry J.},
doi = {10.1557/mrs2001.72},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hornbeck - 2001 - The DMD™ Projection Display Chip A MEMS-Based Technology.pdf:pdf},
issn = {19381425},
journal = {MRS Bulletin},
month = {apr},
number = {4},
pages = {325--327},
publisher = {Cambridge University Press},
title = {{The DMD™ Projection Display Chip: A MEMS-Based Technology}},
url = {http://www.journals.cambridge.org/abstract{\_}S0883769400023587},
volume = {26},
year = {2001}
}
@article{Nakamura2008,
abstract = {This paper describes the experimental and theoretical studies of an anomalous optical beam deflection phenomenon based on electroopticeffect and space-charge-controlled electrical conduction. In the experiment, a large deflection angle of 250 mrad ( = 14.3 ° ) has been observed by applying ± 250 V to a 0.5 ‐ mm -thick K Ta 1 − x Nb x O 3 crystal with a short interaction length of 5.0 mm . The crystal has a rectangular shape with uniform electrodes and there is no prism shape involved which is a common geometrical shape of crystal, electrode, or ferroelectric domain in the conventional electro-optic deflectors. The operating principle is investigated and it is found that the space-charge-controlled electrical conduction in the crystal plays an essential role in this deflection phenomenon. The electrical conduction is carried by electrons injected from the Ohmic contact of the electrode with the crystal. The injected electrons induce the space-charge effect and the electric field becomes nonuniform between the electrodes. The theoretical analysis shows that the electric field has a square-root dependence on the distance from the cathode. As a result, a linearly graded refractive index is induced by the electroopticKerr effect of the crystal and the optical beam is cumulatively deflected as it propagates in the crystal. We named this effect the “space-charge-controlled electro-optic effect” and the factors related to the onset of this effect are also discussed.},
author = {Nakamura, Koichiro and Miyazu, Jun and Sasaki, Yuzo and Imai, Tadayuki and Sasaura, Masahiro and Fujiura, Kazuo},
doi = {10.1063/1.2949394},
file = {:home/miguel/Dropbox/Mendeley Desktop/Nakamura et al. - 2008 - Space-charge-controlled electro-optic effect Optical beam deflection by electro-optic effect and space-charge-c.pdf:pdf},
issn = {00218979},
journal = {Journal of Applied Physics},
number = {1},
pages = {13105},
title = {{Space-charge-controlled electro-optic effect: Optical beam deflection by electro-optic effect and space-charge-controlled electrical conduction}},
url = {https://doi.org/10.1063/1.2949394},
volume = {104},
year = {2008}
}
@article{Gallego2018,
abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. These features, along with a very low power consumption, make event cameras an ideal complement to standard cameras for VR/AR and video game applications. With these applications in mind, this paper tackles the problem of accurate, low-latency tracking of an event camera from an existing photometric depth map (i.e., intensity plus depth information) built via classic dense reconstruction pipelines. Our approach tracks the 6-DOF pose of the event camera upon the arrival of each event, thus virtually eliminating latency. We successfully evaluate the method in both indoor and outdoor scenes and show that---because of the technological advantages of the event camera---our pipeline works in scenes characterized by high-speed motion, which are still unaccessible to standard cameras.},
author = {Gallego, Guillermo and Lund, Jon E.A. and Mueggler, Elias and Rebecq, Henri and Delbruck, Tobi and Scaramuzza, Davide},
doi = {10.1109/TPAMI.2017.2769655},
file = {:home/miguel/Dropbox/Mendeley Desktop/Gallego et al. - 2018 - Event-Based, 6-DOF Camera Tracking from Photometric Depth Maps.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {AR/VR,Bayes filter,Event-based vision,asynchronous processing,conjugate priors,dynamic vision sensor,high speed,low latency,pose tracking},
month = {oct},
number = {10},
pages = {2402--2412},
title = {{Event-Based, 6-DOF Camera Tracking from Photometric Depth Maps}},
url = {https://ieeexplore.ieee.org/document/8094962/},
volume = {40},
year = {2018}
}
@inproceedings{Stolkin2007,
author = {Stolkin, Rustam and Sheryll, Richard and Hotaling, Liesl},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/OCEANS.2007.4449202},
file = {:home/miguel/Dropbox/Mendeley Desktop/Stolkin, Sheryll, Hotaling - 2007 - Braitenbergian experiments with simple aquatic robots.pdf:pdf},
isbn = {0933957351},
issn = {01977385},
month = {sep},
pages = {1--7},
publisher = {IEEE},
title = {{Braitenbergian experiments with simple aquatic robots}},
url = {http://ieeexplore.ieee.org/document/4449202/},
year = {2007}
}
@inproceedings{Rumbaugh2013,
author = {Rumbaugh, Luke K. and Bollt, Erik M. and Jemison, William D. and Li, Yifei},
booktitle = {2013 OCEANS-San Diego},
file = {:home/miguel/Dropbox/Mendeley Desktop/Rumbaugh et al. - 2013 - A 532 nm Chaotic Lidar Transmitter for High Resolution Underwater Ranging and Imaging.pdf:pdf},
isbn = {9780933957404},
keywords = {Lidar,chaotic lidar,fiber lasers,hybrid lidar radar,noise radar,scattering},
pages = {1----6},
publisher = {IEEE},
title = {{A 532 nm Chaotic Lidar Transmitter for High Resolution Underwater Ranging and Imaging}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.409.716},
year = {2013}
}
@phdthesis{Tornblom2010,
abstract = {In this thesis project, an underwater 3D scanner based on structured light has been constructed and developed. Two other scanners, based on stereoscopy and a line-swept laser, were also tested. The target application is to examine objects inside the water filled reactor vessel of nuclear power plants. Structured light systems (SLS) use a projector to illuminate the surface of the scanned object, and a camera to capture the surfaces' reflection. By projecting a series of specific line-patterns, the pixel columns of the digital projector can be identified off the scanned surface. 3D points can then be triangulated using ray-plane intersection. These points form the basis the final 3D model. To construct an accurate 3D model of the scanned surface, both the projector and the camera need to be calibrated. In the implemented 3D scanner, this was done using the Camera Calibration Toolbox for Matlab. The codebase of this scanner comes from the Matlab implementation by Lanman {\&} Taubin at Brown University. The code has been modified and extended to meet the needs of this project. An examination of the effects of the underwater environment has been performed, both theoretically and experimentally. The performance of the scanner has been analyzed, and different 3D model visualization methods have been tested. In the constructed scanner, a small pico projector was used together with a high pixel count DSLR camera. Because these are both consumer level products, the cost of this system is just a fraction of commercial counterparts, which uses professional components. Yet, thanks to the use of a high pixel count camera, the measurement resolution of the scanner is comparable to the high-end of industrial structured light scanners.},
author = {T{\"{o}}rnblom, Nils},
file = {:home/miguel/Dropbox/Mendeley Desktop/T{\"{o}}rnblom - 2010 - Underwater 3D Surface Scanning using Structured Light.pdf:pdf},
number = {December},
pages = {54},
school = {Uppsala Universitet},
title = {{Underwater 3D Surface Scanning using Structured Light}},
url = {http://www.teknat.uu.se/student},
year = {2010}
}
@article{Kwasnitschka2016,
abstract = {Underwater photogrammetry and in particular systematic visual surveys of the deep sea are by far less developed than similar techniques on land or in space. The main challenges are the rough conditions with extremely high pressure, the accessibility of target areas (container and ship deployment of robust sensors, then diving for hours to the ocean floor), and the limitations of localization technologies (no GPS). The absence of natural light complicates energy budget considerations for deep diving flash-equipped drones. Refraction effects influence geometric image formation considerations with respect to field of view and focus, while attenuation and scattering degrade the radiometric image quality and limit the effective visibility. As an improvement on the stated issues, we present an AUV-based optical system intended for autonomous visual mapping of large areas of the seafloor (square kilometers) in up to 6000 m water depth. We compare it to existing systems and discuss tradeoffs such as resolution vs. mapped area and show results from a recent deployment with 90,000 mapped square meters of deep ocean floor.},
author = {Kwasnitschka, Tom and K{\"{o}}ser, Kevin and Sticklus, Jan and Rothenbeck, Marcel and Wei{\ss}, Tim and Wenzlaff, Emanuel and Schoening, Timm and Triebe, Lars and Steinf{\"{u}}hrer, Anja and Devey, Colin and Greinert, Jens},
doi = {10.3390/s16020164},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kwasnitschka et al. - 2016 - DeepSurveyCam—A deep ocean optical mapping system.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {AUV,Camera,Deep sea,Mapping,Mosaicking,Photogrammetry,Photoscan,Survey},
month = {jan},
number = {2},
pages = {164},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{DeepSurveyCam—A deep ocean optical mapping system}},
url = {http://www.mdpi.com/1424-8220/16/2/164},
volume = {16},
year = {2016}
}
@article{Cho2010,
abstract = {In this paper, we demonstrate three-dimensional (3D) visualization of objects in turbid water. The sensing is performed by integral imaging. Multi-perspectives images are degraded due to light scattering and are treated by statistical image processing and computational 3D reconstruction algorithms to remedy the effects of scattering and to visualize the 3D scene. Reconstruction distances and underwater elemental images are calculated according to Snell's law. To the best of our knowledge, this is the first report on 3D reconstruction of objects in turbid water using integral imaging. Experimental results are presented.},
author = {Cho, Myungjin and Javidi, Bahram},
doi = {10.1109/JDT.2010.2066546},
file = {:home/miguel/Dropbox/Mendeley Desktop/Cho, Javidi - 2010 - Three-Dimensional Visualization of Objects in Turbid Water Using Integral Imaging.pdf:pdf},
issn = {1551-319X},
journal = {Journal of Display Technology},
keywords = {Integral imaging,Light scattering,Reconstruction algorithms,Scattering,Three dimensional reconstruction},
month = {oct},
number = {10},
pages = {544--547},
publisher = {IEEE},
title = {{Three-Dimensional Visualization of Objects in Turbid Water Using Integral Imaging}},
url = {https://www.osapublishing.org/jdt/abstract.cfm?uri=jdt-6-10-544 http://ieeexplore.ieee.org/document/5571764/},
volume = {6},
year = {2010}
}
@article{Bosch2019,
author = {Bosch, Josep and Istenic, Klemen and Gracias, Nuno and Garcia, Rafael and Ridao, Pere},
doi = {10.1109/joe.2019.2924276},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bosch et al. - 2019 - Omnidirectional Multicamera Video Stitching Using Depth Maps.pdf:pdf},
issn = {0364-9059},
journal = {IEEE Journal of Oceanic Engineering},
pages = {1--16},
title = {{Omnidirectional Multicamera Video Stitching Using Depth Maps}},
url = {https://ieeexplore.ieee.org/document/8770080/},
year = {2019}
}
@inproceedings{Zandara2013,
abstract = {This paper describes a probabilistic surface matching method for pose-based bathymetry SLAM using a multibeam sonar profiler. The proposed algorithm compounds swath profiles of the seafloor with dead reckoning localization to build surface patches. Then, a probabilistic implementation of the ICP is used to deal with the uncertainty of the robot pose as well as the measured points in a two-stage process including point-to-point and point-to-plane metrics. A novel surface adaptation using octrees is proposed to have ICP-derived methods working in feature-poor or highly unstructured areas typical of bathymetric scenarios. Moreover, a heuristic based on the uncertainties of the surface points is used to improve the basic algorithm, decreasing the ICP complexity to O(n). The performance of the method is demonstrated with real data from a bathymetric survey.},
archivePrefix = {arXiv},
arxivId = {arXiv:0708.1820v1},
author = {Zandara, Simone and Ridao, Pere and Ribas, David and Mallios, Angelos and Palomer, Albert},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630554},
eprint = {arXiv:0708.1820v1},
isbn = {9781467356411},
issn = {10504729},
month = {may},
pages = {40--45},
pmid = {18047539},
publisher = {IEEE},
title = {{Probabilistic surface matching for bathymetry based SLAM}},
url = {http://ieeexplore.ieee.org/document/6630554/},
year = {2013}
}
@inproceedings{Weidemann2005,
abstract = {We present results from trials of the LUCIE 2 (Laser Underwater Camera Image Enhancer) conducted in Halifax Harbor, Nova Scotia, Canada and Esquimalt Harbor, Victoria, British Columbia, Canada. LUCIE 2 is a new compact laser range gated camera (10 inches in diameter, 24 inches in length, and neutrally buoyant in water) originally designed to improve search and recovery operations under eye safe restrictions. The flexibility and eye safety of this second generation LUCIE makes it a tool for improved hull searches and force protection operations when divers are in the water attempting to identify bottom lying objects. The camera is equipped with a full image geo-positioning system. To cover various environmental and targets size conditions, the gate-delay, gate width, polarization and viewing and illuminating angles can be varied as well. We present an analysis on the performance of the system in various water conditions using several target types and a comparison with diver and camera identification. Coincident in-situ optical properties of absorption and scattering were taken to help resolve the environmental information contained in the LUCIE image. Several new capabilities are currently being designed and tested, among them a differential polarization imaging system, a stabilized line of sight system with step-stare capability for high resolution mosaic area coverage, a precision dimensioning system and a diver guided and operated version.},
author = {Weidemann, Alan D and Fournier, Georges R. and Forand, J. Luc and Mathieu, Pierre},
doi = {10.1117/12.603601},
file = {:home/miguel/Dropbox/Mendeley Desktop/Weidemann et al. - 2005 - In harbor underwater threat detectionidentification using active imaging.pdf:pdf},
isbn = {978-3-88579-603-9},
issn = {0277786X},
keywords = {Active underwater imaging,range gating,underwater detection,underwater identification},
pages = {59},
title = {{In harbor underwater threat detection/identification using active imaging}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.603601},
year = {2005}
}
@article{Smith1981,
abstract = {A new UV submersible spectroradiometer has been employed to determine the diffuse attenuation coefficient for irradiance in the clearest natural waters [Kw($\lambda$)] with emphasis on the spectral region from 300 to 400 nm. Kw($\lambda$) can be related to the inherent optical properties of pure water, in particular the total absorption coefficient aw($\lambda$) and the molecular scattering coefficient bm($\lambda$), by means of equations derived from radiative transfer theory. We present an analysis showing that limiting values of Kw($\lambda$) can be estimated from aw($\lambda$) and vice versa. Published aw($\lambda$) data, which show discrepancies much larger than their estimated accuracies, are briefly reviewed and then compared, via our analysis, with Kw($\lambda$) data (our own new and previously published data as well as relevant data of others). This comparative analysis and new data allow a consistent and accurate set of optical properties for the clearest natural waters and for pure fresh water and saltwater to be estimated from 300 to 800 nm.},
author = {Smith, Raymond C. and Baker, Karen S.},
doi = {10.1364/AO.20.000177},
file = {:home/miguel/Dropbox/Mendeley Desktop/Smith, Baker - 1981 - Optical properties of the clearest natural waters (200–800 nm).pdf:pdf},
issn = {0003-6935},
journal = {Applied Optics},
keywords = {Absorption coefficient,Attenuation coefficient,Forward scattering,Molecular scattering,Particle scattering,Visible light},
month = {jan},
number = {2},
pages = {177----184},
publisher = {Optical Society of America},
title = {{Optical properties of the clearest natural waters (200–800 nm)}},
url = {https://www.osapublishing.org/abstract.cfm?URI=ao-20-2-177},
volume = {20},
year = {1981}
}
@inproceedings{Massot-Campos2015a,
abstract = {Stereo vision and structured light are compared in a common underwater environment with known dimensions and objects. Two different sensors are mounted on top of a Cartesian robot that moves with a known and programmed trajectory. The resulting point clouds from each sensor are compared in terms of distance from point to point, and measurements in the scanned objects, to determine which sensor is best suited depending on the environment and the survey purpose. The conclusions show that a stereo based reconstruction is best suited for long, high altitude surveys, always depending on having enough texture and light, whereas a structured light reconstruction can be better fitted in a short, close distance approach where accurate dimensions of an object or structure are needed.},
author = {Massot-Campos, Miquel and Oliver-Codina, Gabriel and Kemal, Hashim and Petillot, Yvan and Bonin-Font, Francisco},
booktitle = {MTS/IEEE OCEANS 2015 - Genova: Discovering Sustainable Ocean Energy for a New World},
doi = {10.1109/OCEANS-Genova.2015.7271433},
file = {:home/miguel/Dropbox/Mendeley Desktop/Massot-Campos et al. - 2015 - Structured light and stereo vision for underwater 3D reconstruction.pdf:pdf},
isbn = {9781479987368},
month = {may},
pages = {1--6},
publisher = {IEEE},
title = {{Structured light and stereo vision for underwater 3D reconstruction}},
url = {http://ieeexplore.ieee.org/document/7271433/},
year = {2015}
}
@article{Yaacobi2014,
abstract = {We demonstrate an on-chip optical phased array fabricated in a CMOS compatible process with continuous, fast (100 kHz), wide-angle (51°) beam-steering suitable for applications such as low-cost LIDAR systems. The device demonstrates the largest (51°) beam-steering and beam-spacing to date while providing the ability to steer continuously over the entire range. Continuous steering is enabled by a cascaded phase shifting architecture utilizing, low power and small footprint, thermo-optic phase shifters. We demonstrate these results in the telecom C-band, but the same design can easily be adjusted for any wavelength between 1.2 and 3.5 $\mu$m.},
author = {Yaacobi, Ami and Sun, Jie and Moresco, Michele and Leake, Gerald and Coolbaugh, Douglas and Watts, Michael R.},
doi = {10.1364/ol.39.004575},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yaacobi et al. - 2014 - Integrated phased array for wide-angle beam steering.pdf:pdf},
issn = {0146-9592},
journal = {Optics Letters},
number = {15},
pages = {4575},
title = {{Integrated phased array for wide-angle beam steering}},
url = {http://dx.doi.org/10.1364/OL.39.004575},
volume = {39},
year = {2014}
}
@article{Bonin2011,
abstract = {Exploration of the underwater environment either by human operated or by autonomous vehicles can highly benefit from using a visual imaging system. When a vision system for an underwater vehicle has to be designed, some specific characteristics of the image formation in sub-sea conditions should be taken into account. This paper presents an extensive survey of components, techniques and methods used to build underwater vision systems. First, most of the phenomena that affect the image formation in submersed conditions are described; second, some significant illumination techniques and light sources, including laser, are presented; and third, the review follows with a list of relevant underwater visual installations and submarine vehicles with vision-based infrastructures recently developed and commercially available. Furthermore, the paper finally introduces some techniques for improving the quality of underwater images. Among all these techniques, a special attention has been paid to the effect of employing polarized light to overcome the undesired scatter present in images. This last sec- tion includes some experiments carried out by the authors to test the usefulness of polarization-based methods in robotic applications.},
author = {Bonin, F. and Burguera, A. and Oliver, G.},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bonin, Burguera, Oliver - 2011 - Imaging systems for advanced underwater vehicles.pdf:pdf},
issn = {16974840},
journal = {Journal of Maritime Research},
keywords = {Autonomous underwater vehicles,Polarization,Robot vision,Underwater installation},
number = {1},
pages = {65--86},
title = {{Imaging systems for advanced underwater vehicles}},
url = {https://www.jmr.unican.es/index.php/jmr/article/view/146},
volume = {8},
year = {2011}
}
@inproceedings{Nakatani2011,
abstract = {Hydrothermal chimneys are indicators of hydrothermal activity and therefore important in ocean research. Because of the complex terrain and suspended solids in the water around hydrothermal chimneys, it is difficult to measure them with traditional methods. In this paper we propose an innovative method to measure the shape and color of them by a rotary laser scanning (RLS) system. By combining multiple scans taken from different angles and extracting the color information from the surface of hydrothermal chimneys, we can create visual color models of them as described in figure 1. The proposed method was implemented during sea experiments in Kagoshima Bay and we were successful in reconstructing the model of a hydrothermal chimney in post processing.},
author = {Nakatani, Takeshi and Li, Shuhao and Ura, Tamaki and Bodenmann, Adrian and Sakamaki, Takashi},
booktitle = {2011 IEEE Symposium on Underwater Technology, UT'11 and Workshop on Scientific Use of Submarine Cables and Related Technologies, SSC'11},
doi = {10.1109/UT.2011.5774140},
file = {:home/miguel/Dropbox/Mendeley Desktop/Nakatani et al. - 2011 - 3D visual modeling of hydrothermal chimneys using a rotary laser scanning system.pdf:pdf},
isbn = {9781457701641},
month = {apr},
pages = {1--5},
publisher = {IEEE},
title = {{3D visual modeling of hydrothermal chimneys using a rotary laser scanning system}},
url = {http://ieeexplore.ieee.org/document/5774140/},
year = {2011}
}
@article{Lirman2010,
abstract = {Vessel groundings are a major source of disturbance to coral reefs worldwide. Documenting the extent of damage caused by groundings is a crucial first step in the reef restoration process. Here, we describe the application of a novel survey methodology, landscape video mosaics, to assessment of the damage caused by vessel groundings. Video mosaics, created by merging thousands of video frames, combine quantitative and qualitative aspects of damage assessment and provide a georeferenced, landscape, high-resolution, spatially accurate permanent record of an injury. The scar in a Florida reef impacted by a 49-foot vessel, imaged in 2005 and 2006, covered an area of 150 m2 (total imaged area was {\{}{\{}{\}}{\{}{\textgreater}{\}}{\{}{\}}{\}}600 m2). The impacted coral community showed limited signs of coral recovery more than 3 years after the initial impact; the cover of corals was still significantly higher in the undamaged areas compared to the scar. However, seagrass colonization of the scar was observed. Finally, no evidence of further physical impacts was documented even when four hurricanes passed near the grounding site in 2005. The video mosaics developed in this study proved to be ideal tools to survey the grounding scars. Mosaics provide a means to collect information on the size of the damage area and the status and trends of the impacted biological communities and provide a permanent visual record of the damage, thereby expanding the quality and diversity of information that can be collected during field surveys. {\{}{\{}{\}}{\{}{\textcopyright}{\}}{\{}{\}}{\}} 2010, by the American Society of Limnology and Oceanography, Inc.},
author = {Lirman, D. and Gracias, Nuno and Gintert, B. and Gleason, A. C.R. and Deangelo, G. and Dick, M. and Martinez, E. and Reid, R. P.},
doi = {10.4319/lom.2010.8.0088},
file = {:home/miguel/Dropbox/Mendeley Desktop/Lirman et al. - 2010 - Damage and recovery assessment of vessel grounding injuries on coral reef habitats by use of georeferenced landsc.pdf:pdf},
issn = {15415856},
journal = {Limnology and Oceanography: Methods},
month = {mar},
number = {MAR},
pages = {88--97},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Damage and recovery assessment of vessel grounding injuries on coral reef habitats by use of georeferenced landscape video mosaics}},
url = {http://doi.wiley.com/10.4319/lom.2010.8.0088},
volume = {8},
year = {2010}
}
@inproceedings{Burns2017,
abstract = {Structural complexity in ecosystems creates an assortment of microhabitat types and has been shown to support greater diversity and abundance of associated organisms. The 3D structure of an environment also directly affects important ecological parameters such as habitat provisioning and light availability and can therefore strongly influence ecosystem function. Coral reefs are architecturally complex 3D habitats, whose structure is intrinsically linked to the ecosystem biodiversity, productivity, and function. The field of coral ecology has, however, been primarily limited to using 2-dimensional (2D) planar survey techniques for studying the physical structure of reefs. This conventional approach fails to capture or quantify the intricate structural complexity of corals that influences habitat facilitation and biodiversity. A 3-dimensional (3D) approach can obtain accurate measurements of architectural complexity, topography, rugosity, volume, and other structural characteristics that affect biodiversity and abundance of reef organisms. Structurefrom- Motion (SfM) photogrammetry is an emerging computer vision technology that provides a simple and cost-effective method for 3D reconstruction of natural environments. SfM has been used in several studies to investigate the relationship between habitat complexity and ecological processes in coral reef ecosystems. This study compared two commercial SfM software packages, Agisoft Photoscan Pro and Pix4Dmapper Pro 3.1, in order to assess the cpaability and spatial accuracy of these programs for conducting 3D modeling of coral reef habitats at three spatial scales.},
author = {Burns, J. H.R. and Delparte, D},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W3-127-2017},
file = {:home/miguel/Dropbox/Mendeley Desktop/Burns, Delparte - 2017 - Comparison of commercial structure-from-motion photogrammety software used for underwater three-dimensional mod.pdf:pdf},
issn = {16821750},
keywords = {3D topographic reconstruction,3D underwater modeling,Agisoft Photoscan,Coral ecology,Coral reefs,Habitat,Photogrammetry,Pix4D,Structural complexity,Structure-from-Motion},
number = {2W3},
pages = {127--131},
title = {{Comparison of commercial structure-from-motion photogrammety software used for underwater three-dimensional modeling of coral reef environments}},
url = {https://www.researchgate.net/publication/313962109},
volume = {42},
year = {2017}
}
@article{Brown1971,
abstract = {For highest accuracies it is necessary in close range photogrammetry to account for the variation of lens distortion within the photographic field. A theory to accomplish this is developed along with practical method for calibrating radial and decentering distortion of close-range cameras. This method, the analytical plumb line method, is applied in an experimental investigation leading to confirmation of the validity of the theoretical development accounting for variation of distortion with object distance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Brown, Duane C},
doi = {10.1.1.14.6358},
eprint = {arXiv:1011.1669v3},
file = {:home/miguel/Dropbox/Mendeley Desktop/Brown - 1971 - Close-range camera calibration.pdf:pdf},
isbn = {2006267816819},
issn = {03331024},
journal = {Photogrammetric Engineering},
keywords = {PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING},
number = {8},
pages = {855--866},
pmid = {16776696},
title = {{Close-range camera calibration}},
url = {https://www.asprs.org/wp-content/uploads/pers/1971journal/aug/1971{\_}aug{\_}855-866.pdf http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.6358},
volume = {37},
year = {1971}
}
@techreport{Sarafraz2010,
abstract = {Image contrast can deteriorate significantly in scattering media, such as underwater, due to backscatter. This affects the performance of many computer vision techniques developed for open-air conditions, including the stereo matching algorithms, when applied to images acquired in these environments. It has been demonstrated that the backscatter field embodies depth information, thus can potentially provide an effective visual cue for 3-D reconstruction. In this paper, we address the estimation of the backscatter component in stereo images, in order to employ it as an additional cue for disparity estimation. More precisely, we decouple the stereo images into signal and backscatter components, and thus are able to make use of depth cues offered by both components in order to devise a more robust technique for disparity computation. Our method is invariant to illumination setup, and requires neither lighting calibration nor the knowledge of medium optical properties. Results of experiments with synthetic and real data are provided to demonstrate the performance of our new method.},
author = {Sarafraz, Amin and Negahdaripour, Shahriar and Schechner, Yoav Y},
file = {:home/miguel/Dropbox/Mendeley Desktop/Sarafraz, Negahdaripour, Schechner - 2010 - Improving Stereo Correspondence in Scattering Media by Incorporating Backscatter Cue.pdf:pdf},
institution = {TECHNION - Israel Institute of Technology},
title = {{Improving Stereo Correspondence in Scattering Media by Incorporating Backscatter Cue}},
url = {https://webee.eedev.technion.ac.il/wp-content/uploads/2014/08/publication{\_}571.pdf},
volume = {772},
year = {2010}
}
@article{Yagi2009,
abstract = {By exploiting the giant electro-optic effect of potassium tantalate niobate (KTN) crystal, we have developed a wide-angle fast optical beam scanner and a fast varifocal lens. These devices are applicable not only to optical communications, but also to various other fields that use optical beams, so we are seeking sales scenarios for KTN devices.},
author = {Yagi, Shogo},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yagi - 2009 - Special Feature KTN Crystal Technologies KTN Crystals Open Up New Possibilities and Applications.pdf:pdf},
institution = {NTT Technical Review},
issn = {13483447},
journal = {NTT Technical Review},
pages = {1--5},
series = {12},
title = {{Special Feature: KTN Crystal Technologies KTN Crystals Open Up New Possibilities and Applications}},
url = {http://www.amstechnologies.com/fileadmin/amsmedia/downloads/4347{\_}ktncrystaltechnologies.pdf},
year = {2009}
}
@article{Matos2019,
abstract = {IEEE Underwater 3-D measurement has many applications, for example, in the oil and gas industry, archeology, and biology. Systems with laser triangulation sensors (LTSs) are being currently used underwater, even though they face some strong environment influences. Among these challenging influences are poor image quality and refraction, due to optical window interfaces between water and air inside the camera chamber. The refraction effect can be modeled knowing the distance from the camera pinhole center to the surface of refraction, the axis of refraction, the refractive index of the mediums, and the thickness of the optical window. This paper analyses a method for underwater LTS measurement using real experiments. The proposed method is based on the pinhole camera model, a refraction modeling, and a fitted mathematical plane for the projected laser light plane. After the in-air calibration, a step standard is measured underwater and the window distance from the camera is optimized. The method is evaluated according to guidelines for optical systems evaluation (VDI/VDE 2634).},
author = {Matos, Gabriel and Buschinelli, Pedro D.V. and Pinto, Tiago},
doi = {10.1109/JOE.2019.2891863},
file = {:home/miguel/Dropbox/Mendeley Desktop/Matos, Buschinelli, Pinto - 2019 - Underwater Laser Triangulation Sensor Model With Flat Refractive Interfaces.pdf:pdf},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {3-D reconstruction,Calibration,Cameras,Distortion,Laser modes,Laser triangulation,Mathematical model,Measurement by laser beam,Sea measurements,refraction modeling,sensor calibration},
pages = {1--9},
title = {{Underwater Laser Triangulation Sensor Model With Flat Refractive Interfaces}},
url = {https://ieeexplore.ieee.org/document/8644043/},
year = {2019}
}
@article{Ngoi2001,
abstract = {Ultrashort pulsed laser material processing is a new micromachining method that is gaining interest. Its capability of submicrometer machining has been proved. To obtain high speed and highly flexible beam steering, a two-axis acousto-optic deflector is employed. However, dispersion associated with acoustic-optic interaction will cause serious spatial deformation on the writing spot. The compensation for dispersion is proposed and studied. Experiments show promising results. An additional advantage of the proposed compensation method is that it can also precisely control the pulse number, and, hence improve the quality of ablation.},
author = {Ngoi, B.K. Ann and Venkatakrishnan, Krishnan and Lim, L.E.N. and Tan, B},
doi = {10.1364/oe.9.000200},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ngoi et al. - 2001 - Angular dispersion compensation for acousto-optic devices used for ultrashort-pulsed laser micromachining.pdf:pdf},
journal = {Optics Express},
keywords = {(2301040) Acousto-optical devices,(3207160) Ultrafast technology,OCIS codes},
number = {4},
pages = {200},
title = {{Angular dispersion compensation for acousto-optic devices used for ultrashort-pulsed laser micromachining}},
url = {https://www.osapublishing.org/DirectPDFAccess/BF4E69CD-0226-DFC1-921A63B679D2C237{\_}65004/oe-9-4-200.pdf?da=1{\&}id=65004{\&}seq=0{\&}mobile=no},
volume = {9},
year = {2001}
}
@article{DeVault2000,
abstract = {The research objective is to develop an automated robotic system$\backslash$nthat will enable safe and cost-effective underwater inspection of bridge$\backslash$nsubstructures. The system concept being developed is a semiautonomous$\backslash$nrobotic system that can carry a sensor platform underwater to detect$\backslash$nscour, deterioration, or damage to support columns. It provides$\backslash$npositional data and sensor information (video images) to the system$\backslash$noperator; these can be verbally annotated while being recorded. The$\backslash$noperator initiates basic commands and transmits them to the underwater$\backslash$napparatus. On-board microprocessor-based controllers automatically$\backslash$naccomplish the detailed control. The primary underwater apparatus has a$\backslash$nteam of two identical mobile robots designed to travel along opposite$\backslash$nsurfaces of the pier while connected to each another by a cable and$\backslash$nwinch system. Each robot has rubber tracks or wheels with cleats and is$\backslash$ndriven by internal motors. Tensioning the cables that connect the two$\backslash$nrobots provides traction. The robots can move both vertically and$\backslash$nhorizontally. While each robot operates its own drive motors and cable$\backslash$nwinches, coordination of movement and cable tensioning occurs$\backslash$nautomatically through feedback between the robots and the control$\backslash$nconsole. Multiple robots, evenly spaced around a support column, may$\backslash$ninspect larger structures},
author = {DeVault, James E.},
doi = {10.1109/5289.863909},
file = {:home/miguel/Dropbox/Mendeley Desktop/DeVault - 2000 - Robotic system for underwater inspection of bridge piers.pdf:pdf},
issn = {10946969},
journal = {IEEE Instrumentation and Measurement Magazine},
number = {3},
pages = {32--37},
title = {{Robotic system for underwater inspection of bridge piers}},
url = {http://ieeexplore.ieee.org/document/863909/},
volume = {3},
year = {2000}
}
@article{Li2017,
abstract = {Underwater cameras are widely used to observe the sea floor. They are usually included in autonomous underwater vehicles (AUVs), unmanned underwater vehicles (UUVs), and in situ ocean sensor networks. Despite being an important sensor for monitoring underwater scenes, there exist many issues with recent underwater camera sensors. Because of light's transportation characteristics in water and the biological activity at the sea floor, the acquired underwater images often suffer from scatters and large amounts of noise. Over the last five years, many methods have been proposed to overcome traditional underwater imaging problems. This paper aims to review the state-of-the-art techniques in underwater image processing by highlighting the contributions and challenges presented in over 40 papers. We present an overview of various underwater image-processing approaches, such as underwater image de-scattering, underwater image color restoration, and underwater image quality assessments. Finally, we summarize the future trends and challenges in designing and processing underwater imaging sensors.},
author = {Lu, Huimin and Li, Yujie and Zhang, Yudong and Kim, Hyoungseop and Serikawa, Seiichi and Chen, Min},
doi = {10.1007/s11036-017-0863-4},
file = {:home/miguel/Dropbox/Mendeley Desktop/Lu et al. - 2017 - Underwater Optical Image Processing a Comprehensive Review.pdf:pdf},
issn = {1383-469X},
journal = {Mobile Networks and Applications},
keywords = {de-scattering,ocean observing,ocean sensor networks,underwater imaging},
number = {6},
pages = {1204--1211},
title = {{Underwater Optical Image Processing: a Comprehensive Review}},
url = {https://arxiv.org/ftp/arxiv/papers/1702/1702.03600.pdf},
volume = {22},
year = {2017}
}
@article{Kaul,
abstract = {The ability to generate accurate and detailed three-dimensional (3D) maps of a scene from a mobile platform is an essential technology for a wide variety of applications from robotic navigation to geological surveying. In many instances, the best vantage point is from above, and as a result, there is a growing demand for low-altitude mapping solutions from micro aerial vehicles such as small quadcopters. Existing lidar-based 3D airborne mapping solutions rely on GPS/INS solutions for positioning, or focus on producing relatively low-fidelity or locally focused maps for the purposes of autonomous navigation. We have developed a general-purpose airborne 3D mapping system capable of continuously scanning the environment during flight to produce accurate and dense point clouds without the need for a separate positioning system. A key feature of the system is a novel passively driven mechanism to rotate a lightweight 2D laser scanner using the rotor downdraft from a quadcopter. The data generated from the spinning laser is input into a continuous-time simultaneous localization and mapping (SLAM) solution to produce an accurate 6 degree-of-freedom trajectory estimate and a 3D point cloud map. Extensive results are presented illustrating the versatility of the platform in a variety of environments including forests, caves, mines, heritage sites, and industrial facilities. Comparison with conventional surveying methods and equipment demonstrates the high accuracy and precision of the proposed solution.},
author = {Kaul, Lukas and Zlot, Robert and Bosse, Michael},
doi = {10.1002/rob.21614},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kaul, Zlot, Bosse - 2016 - Continuous-Time Three-Dimensional Mapping for Micro Aerial Vehicles with a Passively Actuated Rotating Laser.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {1},
pages = {103--132},
title = {{Continuous-Time Three-Dimensional Mapping for Micro Aerial Vehicles with a Passively Actuated Rotating Laser Scanner}},
url = {http://xactmaps.com},
volume = {33},
year = {2016}
}
@article{Maas2015,
abstract = {Underwater applications of photogrammetric measurement techniques usually need to deal with multimedia photogrammetry aspects, which are characterized by the necessity of handling optical rays that are refracted at interfaces between optical media with different refractive indices according to Snell's Law. This so-called multimedia geometry has to be incorporated into geometric models in order to achieve correct measurement results. The paper shows a flexible yet strict geometric model for the handling of refraction effects on the optical path, which can be implemented as a module into photogrammetric standard tools such as spatial resection, spatial intersection, bundle adjustment or epipolar line computation. The module is especially well suited for applications, where an object in water is observed by cameras in air through one or more planar glass interfaces, as it allows for some simplifications here. In the second part of the paper, several aspects, which are relevant for an assessment of the accuracy potential in underwater/multimedia photogrammetry, are discussed. These aspects include network geometry and interface planarity issues as well as effects caused by refractive index variations and dispersion and diffusion under water. All these factors contribute to a rather significant degradation of the geometric accuracy potential in underwater/multimedia photogrammetry. In practical experiments, a degradation of the quality of results by a factor two could be determined under relatively favorable conditions.},
author = {Maas, Hans-Gerd},
doi = {10.3390/s150818140},
file = {:home/miguel/Dropbox/Mendeley Desktop/Maas - 2015 - On the accuracy potential in underwatermultimedia photogrammetry.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Accuracy analysis,Geometric modeling,Multimedia photogrammetry,Underwater photogrammetry},
number = {8},
pages = {18140--18152},
title = {{On the accuracy potential in underwater/multimedia photogrammetry}},
url = {www.mdpi.com/journal/sensorsArticle},
volume = {15},
year = {2015}
}
@article{Wang2007,
abstract = {This paper describes the development of an underwater camera system with a laser line source to measure seafloor features at millimeter scales. The quality of underwater photography is limited by the visibility of the water column. In real underwater environments, there are always suspended particles in the water column and light is scattered by these particles. As a result, photographic images taken under such conditions will be blurred. The stronger the light source is, the more severe the blurring will be. Therefore, it is difficult to discern the features of the intended target directly from the underwater images. To overcome this problem, a laser stripe can be projected onto the target and the profile of the target can be inferred from the displacement of the laser scan lines relative to a reference baseline. With a calibrated camera, the displacement expressed in pixels can be converted into the dimensions of the target in engineering units. To obtain a broader view in a closer distance, a wide-angle lens is usually used. As a result, the image taken with the wide-angle lens is nonlinear and is strongly distorted at the edges of the image. Calibration of a camera involves finding the optical and geometrical parameters of the camera and the environment in which it works. In this paper, a modified coordinate mapping calibration procedure is used. We divide the scope of the camera into several regions and build linear mappings between the world coordinate system and the pixels in the regions. We lay vertical and horizontal grid lines separated by 50 mm on an acrylic board that is aligned with the laser scanning sheet. These grid lines serve as longitude and latitude lines of a map. On the captured image, we curve-fit the grid points in pixel coordinates. A pair of interpolated longitude and latitude lines which pass through the target point are used to estimate the location of the point in the world coordinate system. We assess the accuracy of this procedure with- test pieces (grooved blocks and seabed ripples) fabricated by a computer numerical control milling machine. Our measurements show that the error is less than 1.5 mm when the target is scanned from a distance of 1 m.},
author = {Wang, Chau Chang and Cheng, Min Shine},
doi = {10.1109/JOE.2006.880391},
file = {:home/miguel/Dropbox/Mendeley Desktop/Wang, Cheng - 2007 - Nonmetric camera calibration for underwater laser scanning system.pdf:pdf},
isbn = {9781467317368},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Charge-coupled devices (CCDs) calibration,Laser scan,Seabed roughness},
month = {apr},
number = {2},
pages = {383--399},
title = {{Nonmetric camera calibration for underwater laser scanning system}},
url = {http://ieeexplore.ieee.org/document/4383227/},
volume = {32},
year = {2007}
}
@article{Pascoal2000,
abstract = {The key objective of the ASIMOV project is the development and integration of advanced technological systems to achieve coordinated operation of an Autonomous Surface Craft (ASC) and an Autonomous Underwater Vehicle (AUV) while ensuring a fast communication link between the two vehicles. The ASC/AUV ensemble is being used to study the extent of shallow water hydrothermalism and to determine the patterns of community diversity at vents in the D. Joao de Castro (DJC) bank in the Azores},
author = {Pascoal, Ant{\'{o}}nio and Oliveira, Paulo and Silvestre, Carlos and Sebasti{\~{a}}o, Luis and Rufino, Manuel and Barroso, Victor and Gomes, Jo{\~{a}}o and Ayela, Gerard and Coince, Pascal and Cardew, Marcus and Ryan, Anne and Braithwaite, Hugh and Cardew, Nicholas and Trepte, Jonathan and Seube, Nicolas and Champeau, J and Dhaussy, P and Sauce, V and Moiti{\'{e}}, R. and Santos, Ricardo and Cardigos, Frederico and Brussieux, Marc and Dando, Paul},
doi = {10.1109/OCEANS.2000.881293},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pascoal et al. - 2000 - Robotic ocean vehicles for marine science applications The european ASIMOV project.pdf:pdf},
issn = {01977385},
journal = {Oceans Conference Record (IEEE)},
pages = {409--415},
title = {{Robotic ocean vehicles for marine science applications: The european ASIMOV project}},
url = {https://s3.amazonaws.com/academia.edu.documents/43239361/Robotic{\_}ocean{\_}vehicles{\_}for{\_}marine{\_}scienc20160301-32306-1nl0mro.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A{\&}Expires=1555072135{\&}Signature=c6zbZkGvn1ePldVwX10utAizyfs{\%}3D{\&}response-content-disposition=inline},
volume = {1},
year = {2000}
}
@article{Agishev2013,
abstract = {The purpose of the work is to demonstrate real capabilities and to give examples of SiPMs application in lidar technology in both analog and photon counting modes. The experimental research within an operating lidar complex adapted to implement the analog mode and photon counting measurements with subsequent inversions of atmospheric extinction and backscattering coefficients was conducted. Theoretical evaluations of potential limitations of atmospheric lidar by use of real day-time background parameters and features of SiPM-photodetectors studied experimentally were carried out with comparison of the extent of sensitivity decreasing for different detectors used and estimations of their operation range reduction. {\textcopyright} 2012 Elsevier Ltd.},
author = {Agishev, Ravil and Comer{\'{o}}n, Adolfo and Bach, Jordi and Rodriguez, Alejandro and Sicard, Micha{\"{e}}l and Riu, Jordi and Royo, Santiago},
doi = {10.1016/j.optlastec.2012.12.024},
file = {:home/miguel/Dropbox/Mendeley Desktop/Agishev et al. - 2013 - Lidar with SiPM Some capabilities and limitations in real environment.pdf:pdf},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {Lidar,PMT,SiPM},
pages = {86--90},
title = {{Lidar with SiPM: Some capabilities and limitations in real environment}},
url = {http://dx.doi.org/10.1016/j.optlastec.2012.12.024},
volume = {49},
year = {2013}
}
@article{Bystrov2018,
abstract = {An underwater imaging system was investigated for automotive use in highly scattered underwater environments. The purpose of the system is the driver's information about hidden obstacles, such as stones, driftwood, open sewer hatches. A comparison of various underwater vision methods was presented by the way they are implemented, the range reached, and the cost of implementation. It has been experimentally shown that a conventional active system can provide a maximum visibility range of up to three light attenuation lengths. In most practical cases of turbid waters during floods, this corresponds to distances of about 1 meter. From the presented analysis it follows that advanced extended range imaging methods allow increasing of the visibility range up to 2 meters.},
author = {Bystrov, Aleksandr and Hoare, Edward and Gashinova, Marina and Cherniakov, Mikhail and Tran, Thuy Yung},
doi = {10.3390/s18124476},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bystrov et al. - 2018 - Underwater optical imaging for automotive wading.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Advanced driver assistance systems,Image analysis,Object detection,Optical propagation,Optical sensors},
month = {dec},
number = {12},
pages = {4476},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Underwater optical imaging for automotive wading}},
url = {http://www.mdpi.com/1424-8220/18/12/4476},
volume = {18},
year = {2018}
}
@article{Chantler1997,
abstract = {We describe the calibration and operation of a high-speed underwater scanning depth finder that has been designed for workspace sensing (that is, for ranges of 2 m or less). It uses standard components mounted in an underwater housing fitted with a planar viewing port. The use of a laser line forming element coupled with a single galvanometer mirror system enables fast data capture but introduces alignment problems with standard domed ports. Hence a planar interface is used. This results in the triangulation baseline varying as a function of scan angle. We present a simple but novel solution for the calibration and use of such a device. The effect of the varying baseline is modeled, and a calibration/depth calculation process is presented. Results obtained from using the device in a test tank demonstrate the performance of the system.},
annote = {Albert: "only three published works use a triangulation method with a steered laser"},
author = {Chantler, Michael J.},
doi = {10.1117/1.601500},
issn = {0091-3286},
journal = {Optical Engineering},
month = {sep},
number = {9},
pages = {2604},
publisher = {International Society for Optics and Photonics},
title = {{Calibration and operation of an underwater laser triangulation sensor: the varying baseline problem}},
url = {http://opticalengineering.spiedigitallibrary.org/article.aspx?doi=10.1117/1.601500},
volume = {36},
year = {1997}
}
@inproceedings{McLean1999,
abstract = {A Streak Tube Imaging Lidar (STIL) system has recently demonstrated high resolution 3D imaging in ocean water. Lateral resolution of better than 1/2 inch has been obtained through 36 feet of ocean water, with range resolutions of better than 1 inch. The resulting 3D data may be processed for imaging of floating, moored, and bottom objects. Processed results include both conventional contrast maps of bottom targets, as well as range maps which represent the height of the object above the surrounding bottom. Data analysis and modeling was performed to process and interpret the resulting high resolution 3D images. This paper describes the concept, design, implementation, and performance of STIL for ocean imaging, and discusses applications of this high resolution 3D imaging capability to ocean surveillance.},
author = {McLean, John W.},
doi = {10.1117/12.366483},
editor = {Gilbert, Gary D.},
month = {oct},
pages = {10--19},
publisher = {International Society for Optics and Photonics},
title = {{High-resolution 3D underwater imaging}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=995569},
volume = {3761},
year = {1999}
}
@article{Magnusson,
abstract = {Scan registration is an essential subtask when building maps based on range finder data from mobile robots. The problem is to deduce how the robot has moved between consecutive scans, based on the shape of overlapping portions of the scans. This paper presents a new algorithm for registration of 3D data. The algorithm is a generalization and improvement of the normal distributions transform (NDT) for 2D data developed by Biber and Strasser, which allows for accurate registration using a memory-efficient representation of the scan surface. A detailed quantitative and qualitative comparison of the new algorithm with the 3D version of the popular ICP (iterative closest point) algorithm is presented. Results with actual mine data, some of which were collected with a new prototype 3D laser scanner, show that the presented algorithm is faster and slightly more reliable than the standard ICP algorithm for 3D registration, while using a more memory-efficient scan surface representation. {\textcopyright} 2007 Wiley Periodicals, Inc.},
author = {Magnusson, Martin and Lilienthal, Achim and Duckett, Tom},
doi = {10.1002/rob.20204},
file = {:home/miguel/Dropbox/Mendeley Desktop/Magnusson, Lilienthal, Duckett - 2007 - Scan registration for autonomous mining vehicles using 3D-NDT.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {10},
pages = {803--827},
title = {{Scan registration for autonomous mining vehicles using 3D-NDT}},
url = {http://www.diva-portal.orghttp//urn.kb.se/resolve?urn=urn:nbn:se:oru:diva-4258},
volume = {24},
year = {2007}
}
@inproceedings{Jiang2017,
abstract = {{\textcopyright} 2017 IEEE. This paper proposes an underwater 3D reconstruction method based on laser line scanning. 3D reconstruction of underwater target is an important technology in ocean development. Due to the complicated underwater environment, underwater 3D reconstruction is still a challenge. The traditional underwater object detection is mainly based on acoustic technology, but acoustic technology can't meet the demands of high-precision underwater reconstruction. Laser line scanning is a popular technology which can be applied into complex underwater environments. It has stable antijamming capability and high-precision scanning capability which can help us to finish 3D reconstruction delicately. In our method, We combine some advanced methods to finish this reconstruction. There are two important links. The first one is calibration method called direct calibration. This method can get the relationship between 2D pix coordinate and 3D world coordinate directly, so we don't need to calculate complex camera parameters. The second one is extraction of line laser. Getting clear images and processing them well is beneficial to extract the center line of the laser, so we use smoothing to reduce the interferences first, then direction template technology combined with gray centroid method is used to get the information of center line. In order to verify the correctness of this methods, experiment was done on the land firstly and got a good result. Then the experiment was done in a sink. We use a disk and a keyboard as the target. With our adjustment, we have got a clear shape of the underwater object. At the same time, structure from motion is used as a compared method.},
author = {Jiang, Shanchen and Sun, Fengna and Gu, Zhaorui and Zheng, Haiyong and Nan, Wang and Yu, Zhibin},
booktitle = {OCEANS 2017 - Aberdeen},
doi = {10.1109/OCEANSE.2017.8084737},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jiang et al. - 2017 - Underwater 3D reconstruction based on laser line scanning.pdf:pdf},
isbn = {9781509052783},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Underwater 3D reconstruction based on laser line scanning}},
url = {http://ieeexplore.ieee.org/document/8084737/},
volume = {2017-Octob},
year = {2017}
}
@article{Heck2017,
abstract = {Technologies for efficient generation and fast scanning of narrow free-space laser beams find major applications in three-dimensional (3D) imaging and mapping, like Lidar for remote sensing and navigation, and secure free-space optical communications. The ultimate goal for such a system is to reduce its size, weight, and power consumption, so that it can be mounted on, e.g. drones and autonomous cars. Moreover, beam scanning should ideally be done at video frame rates, something that is beyond the capabilities of current opto-mechanical systems. Photonic integrated circuit (PIC) technology holds the promise of achieving low-cost, compact, robust and energy-efficient complex optical systems. PICs integrate, for example, lasers, modulators, detectors, and filters on a single piece of semiconductor, typically silicon or indium phosphide, much like electronic integrated circuits. This technology is maturing fast, driven by high-bandwidth communications applications, and mature fabrication facilities. State-of-the-art commercial PICs integrate hundreds of elements, and the integration of thousands of elements has been shown in the laboratory. Over the last few years, there has been a considerable research effort to integrate beam steering systems on a PIC, and various beam steering demonstrators based on optical phased arrays have been realized. Arrays of up to thousands of coherent emitters, including their phase and amplitude control, have been integrated, and various applications have been explored. In this review paper, I will present an overview of the state of the art of this technology and its opportunities, illustrated by recent breakthroughs.},
author = {Heck, Martijn J.R.},
doi = {10.1515/nanoph-2015-0152},
file = {:home/miguel/Dropbox/Mendeley Desktop/Heck - 2017 - Highly integrated optical phased arrays Photonic integrated circuits for optical beam shaping and beam steering.pdf:pdf},
isbn = {2192-8606},
issn = {21928614},
journal = {Nanophotonics},
keywords = {III/V photonics,free-space optical communications,lidar,photonic integrated circuits,silicon photonics},
number = {1},
pages = {93----107},
title = {{Highly integrated optical phased arrays: Photonic integrated circuits for optical beam shaping and beam steering}},
url = {https://www.degruyter.com/downloadpdf/j/nanoph.2017.6.issue-1/nanoph-2015-0152/nanoph-2015-0152.pdf},
volume = {6},
year = {2017}
}
@inproceedings{Smith2002,
abstract = {This paper describes the philosophy applied for the control of a five degree of freedom, electrically driven manipulator mounted on a six degree of freedom remotely operated vehicle (Aquasphere). Guidance commands for the manipulator could be provided either by the operator, using joysticks, or from a 3D vision system which could determine the full position and orientation of objects. The range information was generated by a dual spot laser triangulation system which was capable of providing range and location information to an accuracy of ±2 mm. An alternative range finder based upon the time of flight system (TOF) is also discussed. Transputers were used as the processing elements to solve the inverse kinematics required for the manipulator whilst a hybrid dedicated hardware-transputer system was used for the image processing. Once the vehicle was placed in the vicinity of the object the complete system was demonstrated by using the vision system to locate and then autonomously guide the manipulator to retrieve the object},
author = {Smith, J.S. and Yu, R. and Sarafis, I. and Lucas, J.},
booktitle = {Proceedings of OCEANS'94},
doi = {10.1109/oceans.1994.363849},
file = {:home/miguel/Dropbox/Mendeley Desktop/Smith et al. - 2002 - Computer vision control of an underwater manipulator.pdf:pdf},
isbn = {0-7803-2056-5},
pages = {I/187--I/192},
publisher = {IEEE},
title = {{Computer vision control of an underwater manipulator}},
url = {http://ieeexplore.ieee.org/document/363849/},
volume = {1},
year = {2002}
}
@article{Chua2017,
abstract = {Accuracy is an important measure of system performance and remains a challenge in 3D range gated reconstruction despite the advancement in laser and sensor technology. The weighted average model that is commonly used for range estimation is heavily influenced by the intensity variation due to various factors. Accuracy improvement in term of range estimation is therefore important to fully optimise the system performance. In this paper, a 3D range gated reconstruction model is derived based on the operating principles of range gated imaging and time slicing reconstruction, fundamental of radiant energy, Laser Detection And Ranging (LADAR), and Bidirectional Reflection Distribution Function (BRDF). Accordingly, a new range estimation model is proposed to alleviate the effects induced by distance, target reflection, and range distortion. From the experimental results, the proposed model outperforms the conventional weighted average model to improve the range estimation for better 3D reconstruction. The outcome demonstrated is of interest to various laser ranging applications and can be a reference for future works.},
author = {Chua, Sing Yee and Guo, Ningqun and Tan, Ching Seong and Wang, Xin},
doi = {10.3390/s17092031},
file = {:home/miguel/Dropbox/Mendeley Desktop/Chua et al. - 2017 - Improved range estimation model for three-dimensional (3D) range gated reconstruction.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {3D reconstruction,Range gated imaging,TOF range imaging},
month = {sep},
number = {9},
pages = {2031},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Improved range estimation model for three-dimensional (3D) range gated reconstruction}},
url = {http://www.mdpi.com/1424-8220/17/9/2031},
volume = {17},
year = {2017}
}
@article{Dalgleish2004,
abstract = {The Cranfield University torpedo-shaped underwater vehicle has been modified to accommodate a laser stripe illumination system. As well as providing enhanced viewing capabilities, this system derives real-time navigational data during the mission and gathers images to produce a post mission enhanced optical waterfall image of a surveyed area. This paper describes a preliminary set of constrained motion trials at the IFREMER wave basin in Brest, where the system was towed through the 50 m test tank at different altitudes and orientations whilst the true trajectory was measured. A comparison is made between ground truth trajectory generated from these external measurements and that derived from the video camera and rotation sensors internal to the vehicle. {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
author = {Dalgleish, Fraser R. and Tetlow, S and Allwood, R L},
doi = {10.1016/j.conengprac.2003.11.009},
file = {:home/miguel/Dropbox/Mendeley Desktop/Dalgleish, Tetlow, Allwood - 2004 - Experiments in laser-assisted visual sensing for AUV navigation.pdf:pdf},
isbn = {0967-0661},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Autonomous,Data acquisition,Image distortion,Image sensors,Navigation,Real-time,Trajectories},
number = {12 SPEC. ISS.},
pages = {1561--1573},
title = {{Experiments in laser-assisted visual sensing for AUV navigation}},
url = {https://ac.els-cdn.com/S0967066103002594/1-s2.0-S0967066103002594-main.pdf?{\_}tid=7d62f36a-e123-4310-a955-cff1e6446ed1{\&}acdnat=1548408210{\_}461d9c24e11c75c9fce5244ac192b6e6},
volume = {12},
year = {2004}
}
@article{Bodenmann2017a,
abstract = {3D visual mapping of the seafloor has found applications ranging from environment monitoring and survey of marine minerals to underwater archaeology and inspection of modern artificial structures. However, the attenuation of light is significantly more pronounced in water than in air or in space, and so in order to obtain underwater images in colour, it is typically necessary to be within 2–3 m of the seafloor. In addition to the high risk of collision when operating underwater vehicles at such low altitudes, the limited area of the seafloor covered in each image means large area surveys require a significant investment of time. In this research, we aim to increase the efficiency of mapping large areas of the seafloor by developing an underwater imaging system that can take colour images at ranges of more than 10 m, so that each image can cover a larger area, together with the necessary algorithms to automatically process the data it obtains. The system was deployed to map artificial hydrothermal vents in Iheya North field using the ROV Hyper-Dolphin in October 2012. The surveyed area is of particular interest to the research community, as multiple artificial vent holes were drilled during a mission in 2010, which locally impacted the flow of hydrothermal fluids. In this paper, we describe the methods used to process the data that the imaging system obtains and demonstrate how the mapping data can be used in quantitative studies of the seafloor. Habitats of Shinkaia crosnieri squat lobsters, which are abundant in the hydrothermally active areas, are identified in the maps and their population density calculated, and the amount of hydrothermal deposits that have grown on the artificial vent is derived from the mapping data. The work demonstrates how 3D visual mapping can be applied to benthic biology and geological studies.},
author = {Bodenmann, Adrian and Thornton, Blair and Nakajima, Ryota and Ura, Tamaki},
doi = {10.1186/s40648-017-0091-5},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bodenmann et al. - 2017 - Methods for quantitative studies of seafloor hydrothermal systems using 3D visual reconstructions.pdf:pdf},
issn = {21974225},
journal = {ROBOMECH Journal},
keywords = {3D mapping,Deep-sea hydrothermal vents,Information extraction,Underwater},
month = {dec},
number = {1},
pages = {22},
publisher = {SpringerOpen},
title = {{Methods for quantitative studies of seafloor hydrothermal systems using 3D visual reconstructions}},
url = {http://robomechjournal.springeropen.com/articles/10.1186/s40648-017-0091-5},
volume = {4},
year = {2017}
}
@article{Jaffe2001,
abstract = {As any backyard stargazer knows, one simply has to look up at the sky on a cloudless night to see light whose origin was quite a long time ago. Here, due to the fact that the mean scattering and absorption lengths are greater in size than the observable uni- verse, one can record light from stars whose origin occurred around the time of the big bang. Unfortunately for oceanographers, the opacity of sea water to light far exceeds these intergalactic limits, making the job of collecting optical images in the ocean a difficult task. Although recent advances in optical imaging tech- nology have permitted researchers working in this area to accomplish projects which could only be dreamed of in years past, it makes sense to have a humble attitude and to realize that it is likely that the most sophisticat- ed imaging systems in the sea are those of the animals, who depend upon their visual receptors in order to find prey, to mate, and to escape harm. Nevertheless, the recent decade has witnessed a large increase in our abilities to image objects in the sea. This is due to the current revolution in electronics and sensing technolo- gy coupled with advances in signal and image pro- cessing. In this article, we intend to provide a brief history of underwater optical imaging and a brief summary of its relationship to other fields of ocean optics. However, our major task is to inform the reader about advances in underwater imaging that have occurred in the last decade. Without doubt, the bulk of our roots trace back to the work of Seibert Q. Duntley who was first at the Massachusetts Institute of Technology (MIT) and then at the Scripps Institution of Oceanograph{\~{}} where the Visibility Laboratory of the University of California San Diego was established. Duntley's classic article, "Light in the Sea" (Duntley, 1963), created a baseline of under- standing which drew upon 20 years of studying light propagation for many uses including "photosynthesis, 64 vision, and photography". Subsequent to that, there have been several books which have summarized a technical understanding of underwater imaging such as Merten's book entitled "In Water Photography" (Mertens, 1970) and a monograph edited by Ferris Smith (Smith, 1984). An interesting conference which took place in 1970 resulted in the publication of several papers on optics of the sea, including the air water inter- face and the in water transmission and imaging of objects (Agard, 1973). In this decade, several books by Russian authors have appeared which treat these prob- lems either in the context of underwater vision theory (Dolin and Levin, 1991) or imaging through scattering media (Zege et al., 1991). Recent years have also seen the development of many new types of imaging sys- tems. The desire to image underwater objects is a goal shared (among others) by pelagic and benthic ecolo- gists, geomorphologists and marine resources manage- ment personnel.},
author = {Jaffe, Jules S. and Moore, Karl D and McLean, John W. and Strand, Michael},
doi = {10.5670/oceanog.2001.24},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jaffe et al. - 2001 - Underwater Optical Imaging Status and Prospects.pdf:pdf},
isbn = {1042-8275},
issn = {10428275},
journal = {Oceanography},
number = {3},
pages = {64--75},
title = {{Underwater Optical Imaging: Status and Prospects}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.608.2513{\&}rep=rep1{\&}type=pdf https://tos.org/oceanography/article/underwater-optical-imaging-status-and-prospects},
volume = {14},
year = {2001}
}
@article{Fusiello2004,
abstract = {In this paper, underwater scene modeling from multisensor data is addressed. Acoustic and optical devices aboard an underwater vehicle are used to sense the environment in order to produce an output that is readily understandable even by an inexperienced operator. The main idea is to integrate multiple-sensor data by geometrically registering such data to a model. The geometrical structure of this model is a priori known but not ad hoc designed for this purpose. As a result, the vehicle pose is derived and model objects can be superimposed upon actual images, thus generating an augmented-reality representation. Results on a real underwater scene are reported, showing the effectiveness of the proposed approach. {\textcopyright} 2004 IEEE.},
author = {Fusiello, Andrea and Murino, Vittorio},
doi = {10.1109/TVCG.2004.38},
file = {:home/miguel/Dropbox/Mendeley Desktop/Fusiello, Murino - 2004 - Augmented scene modeling and visualization by optical and acoustic sensor integration.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Acoustic imaging,Augmented reality,Enhanced vision,Model-view registration,Multisensor integration,Teleoperation,Underwater applications},
month = {nov},
number = {6},
pages = {625--635},
title = {{Augmented scene modeling and visualization by optical and acoustic sensor integration}},
url = {http://ieeexplore.ieee.org/document/1333661/},
volume = {10},
year = {2004}
}
@inproceedings{Ancuti2018,
author = {Ancuti, Codruta O. and Ancuti, Cosmin and {De Vleeschouwer}, Christophe and Neumann, Laszlo and Garcia, Rafael},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296370},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ancuti et al. - 2018 - Color transfer for underwater dehazing and depth estimation.pdf:pdf},
isbn = {9781509021758},
issn = {15224880},
keywords = {Color constancy,Color transfer,Depth estimation,Underwater visibility enhancement},
month = {sep},
pages = {695--699},
publisher = {IEEE},
title = {{Color transfer for underwater dehazing and depth estimation}},
url = {http://ieeexplore.ieee.org/document/8296370/},
volume = {2017-Septe},
year = {2018}
}
@inproceedings{Kunz2008,
abstract = {We examine the challenges to underwater vision that are caused by light refracting through both hemispherical and planar pressure housing interfaces. As light travels through water, into a pressure housing, and finally into a camera, it is bent according to Snell's law, rendering the typically-used perspective cameramodel invalid. Through numerical simulation, we examine the degree to which the incorrect perspective model results in 2-D image distortion, and errors in 3-D scene recon- struction computed using stereo vision or structure from motion. We focus on the use of hemispherical pressure housing interfaces with imperfectlymounted cameras, and include comparisons with cameras behind planar interfaces.We also address the problemof calibrating a camera model which takes into account the physical parameters of the pressure housing interface.},
author = {Kunz, Clayton and Singh, Hanumant},
booktitle = {OCEANS 2008},
doi = {10.1109/OCEANS.2008.5151967},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kunz, Singh - 2008 - Hemispherical refraction and camera calibration in underwater vision.pdf:pdf},
isbn = {9781424426201},
pages = {1--7},
publisher = {IEEE},
title = {{Hemispherical refraction and camera calibration in underwater vision}},
url = {http://ieeexplore.ieee.org/document/5151967/},
year = {2008}
}
@inproceedings{Forest2004,
abstract = {The accuracy of a 3D reconstruction using laser scanners is significantly determined by the detection of the laser stripe. Since the energy pattern of such a stripe corresponds to a Gaussian profile, it makes sense to detect the point of maximum light intensity (or peak) by computing the zero-crossing point of the first derivative of such Gaussian profile. However, because noise is present in every physical process, such as electronic image formation, it is not sensitive to perform the derivative of the image of the stripe in almost any situation, unless a previous filtering stage is done. Considering that stripe scanning is an inherently row-parallel process, every row of a given image must be processed independently in order to compute its corresponding peak position in the row. This paper reports on the use of digital filtering techniques in order to cope with the scanning of different surfaces with different optical properties and different noise levels, leading to the proposal of a more accurate numerical peak detector, even at very low signal-to-noise ratios.},
author = {Forest, Josep and Salvi, Joaquim and Cabruja, Enric and Pous, Carles},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2004.1334612},
file = {:home/miguel/Dropbox/Mendeley Desktop/Forest et al. - 2004 - Laser stripe peak detector for 3D scanners. A FIR filter approach.pdf:pdf},
isbn = {0769521282},
issn = {10514651},
pages = {646--649},
publisher = {IEEE},
title = {{Laser stripe peak detector for 3D scanners. A FIR filter approach}},
url = {http://ieeexplore.ieee.org/document/1334612/},
volume = {3},
year = {2004}
}
@inproceedings{Hildebrandt2008,
abstract = {A number of attempts have been made to use the benefits of 3D-Laserscanning techniques in the underwater environment. Unfortunately, due to a number of operative problems with such devices, their accuracy and therefore applicability remains quite low. This paper specifically focuses on these practical issues by expanding on previous works in this area and improving their usability. The result is a calibration procedure for triangulation-based 3D-laserscanners for the underwater environment which provides a very promising precision and reliability, but at the same time does not demand exaggerated deployment overhead.},
annote = {Albert: "only three published works use a triangulation method with a steered laser"},
author = {Hildebrandt, Marc and Kerdels, Jochen and Albiez, Jan and Kirchner, Frank},
booktitle = {OCEANS 2008},
doi = {10.1109/OCEANS.2008.5151964},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hildebrandt et al. - 2008 - A practical underwater 3D-Laserscanner.pdf:pdf},
isbn = {9781424426201},
pages = {1--5},
publisher = {IEEE},
title = {{A practical underwater 3D-Laserscanner}},
url = {http://ieeexplore.ieee.org/document/5151964/},
year = {2008}
}
@inproceedings{Swanson1992,
abstract = {Research is currently underway to investigate the physical mechanisms which cause loss of coherence in scattered light. It has been shown that, for Brownian motion of particles in a scattering medium, the coherence of incident light decreases rapidly with path length and diffusion coefficient. Experiments confirm that laser light scattered by a water column loses coherence as a function of path length and water turbidity. An experiment has been performed which measures coherence length versus temperature, optical path length, and water quality. The results are reported here. Based on these results and research into the causes of spectral broadening, experiments are proposed to measure each type of broadening mechanism with a much higher degree of accuracy.},
author = {Swanson, Nancy L.},
booktitle = {Proc. SPIE 1750, Ocean Optics XI, 397 (December 31, 1992)},
doi = {10.1117/12.140667},
editor = {Gilbert, Gary D.},
month = {dec},
number = {1},
pages = {397--406},
publisher = {International Society for Optics and Photonics},
title = {{Coherence-loss of laser light propagated through simulated coastal waters}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=997310},
volume = {1750},
year = {1992}
}
@inproceedings{Detry2018,
abstract = {This paper studies underwater perception and short-range scene reconstruction. Our work enables autonomous manipulation behaviors that support the autonomous maintenance of subsea infrastructure. We present a system that leverages assisted stereo to reconstruct the geometry of textured or untex-tured structures immersed in turbid water. Our package projects a random binary pattern in the cameras' field of view, which facilitates stereopsis in areas that are not naturally textured. We discuss the design and assembly of the package, and we quantify the accuracy and coverage of our method in turbid water ranging from 1 to 2.5 NTU.},
author = {Detry, R and Koch, J and Pailevanian, T and Garrett, M and Levine, D and Yahnker, C and Gildner, M},
booktitle = {2018 OCEANS - MTS/IEEE Kobe Techno-Oceans, OCEANS - Kobe 2018},
doi = {10.1109/OCEANSKOBE.2018.8559091},
file = {:home/miguel/Dropbox/Mendeley Desktop/Detry et al. - 2018 - Turbid-water subsea infrastructure 3D reconstruction with assisted stereo.pdf:pdf},
isbn = {9781538616543},
title = {{Turbid-water subsea infrastructure 3D reconstruction with assisted stereo}},
url = {http://www.renaud-detry.net/publications/Detry-2018-Oceans.pdf},
year = {2018}
}
@inproceedings{Bardow2016a,
abstract = {Event cameras are bio-inspired vision sensors which mimic retinas to measure per-pixel intensity change rather than outputting an actual intensity image. This proposed paradigm shift away from traditional frame cameras of-fers significant potential advantages: namely avoiding high data rates, dynamic range limitations and motion blur. Unfortunately, however, established computer vision algo-rithms may not at all be applied directly to event cameras. Methods proposed so far to reconstruct images, estimate optical flow, track a camera and reconstruct a scene come with severe restrictions on the environment or on the mo-tion of the camera, e.g. allowing only rotation. Here, we propose, to the best of our knowledge, the first algorithm to simultaneously recover the motion field and brightness im-age, while the camera undergoes a generic motion through any scene. Our approach employs minimisation of a cost function that contains the asynchronous event data as well as spatial and temporal regularisation within a sliding win-dow time interval. Our implementation relies on GPU opti-misation and runs in near real-time. In a series of examples, we demonstrate the successful operation of our framework, including in situations where conventional cameras suffer from dynamic range limitations and motion blur.},
author = {Bardow, Patrick and Davison, Andrew J and Leutenegger, Stefan},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.102},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bardow, Davison, Leutenegger - 2016 - Simultaneous Optical Flow and Intensity Estimation from an Event Camera.pdf:pdf},
isbn = {978-1-4673-8851-1},
pages = {884--892},
title = {{Simultaneous Optical Flow and Intensity Estimation from an Event Camera}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2016/papers/Bardow{\_}Simultaneous{\_}Optical{\_}Flow{\_}CVPR{\_}2016{\_}paper.pdf http://ieeexplore.ieee.org/document/7780471/},
year = {2016}
}
@article{Mariani2018,
abstract = {High-quality video observations are very much needed in underwater environments for the monitoring of several ecosystem indicators and to support the sustainable development and management of almost all activities in the ocean. Reliable video observations are however challenging to collect, because of the generally poor visibility conditions and the difficulties to deploy cost-effective sensors and platforms in the marine environment. Visibility in water is regulated by natural light availability at different depths, and by the presence of suspended particles, scattering incident light in all directions. Those elements are also largely variable in time and space, making it difficult to identify technological solutions that can be used in all conditions. By combining state-of-the-art “time of flight” (ToF) image sensors and innovative pulsed laser illumination, we have developed a range-gated camera system (UTOFIA) that enables affordable and enhanced 3D underwater imaging at high resolution. This range-gated solution allows users to eliminate close-range backscattering, improving quality of the images and providing information on the distance of each illuminated object, hence giving access to real-time 3D measurements. Furthermore, as the system is based on pulsed laser light, it is almost independent of natural light conditions and can achieve similar performances at an extended depth range. We use this system to collect observations in different oceanographic conditions and for different applications, including aquaculture monitoring, seafloor mapping, litter identifications and structure inspection. Performances are evaluated by comparing images to regular cameras and by using standard targets to assess accuracy and precision of distance measurements. We suggest that this type of technology can become a standard in underwater 3D imaging to support the future development of the ocean economy.},
author = {Mariani, Patrizio and Quincoces, I{\~{n}}aki and Haugholt, Karl Henrik and Chardard, Yves and Visser, Andre W. and Yates, Chris and Piccinno, Giuliano and Reali, Giancarlo and Risholm, Petter and Thielemann, Jens T.},
doi = {10.3390/su11010162},
file = {:home/miguel/Dropbox/Mendeley Desktop/Mariani et al. - 2018 - Range-Gated Imaging System for Underwater Monitoring in Ocean Environment.pdf:pdf},
issn = {2071-1050},
journal = {Sustainability},
keywords = {LIDAR technology,aquaculture,marine life,ocean observations},
month = {dec},
number = {1},
pages = {162},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Range-Gated Imaging System for Underwater Monitoring in Ocean Environment}},
url = {http://www.mdpi.com/2071-1050/11/1/162},
volume = {11},
year = {2018}
}
@inproceedings{Bleiera,
author = {Bleier, Michael and Lucht, Joschka Van Der and Andreas, N},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bleier, Lucht, Andreas - Unknown - Towards an Underwater 3D Laser Scanning System for Mobile Mapping.pdf:pdf},
title = {{Towards an Underwater 3D Laser Scanning System for Mobile Mapping}}
}
@article{Risholm2019,
abstract = {High-precision underwater 3D cameras are required to automate many of the traditional subsea inspection, maintenance and repair (IMR) operations. In this paper we introduce a novel multi-frequency phase stepping (structured light) method for high-precision 3D estimation even in turbid water. We introduce an adaptive phase-unwrapping procedure which uses the phase-uncertainty to determine the highest frequency that can be reliably unwrapped. Light scattering adversely affects the phase estimate. We propose to remove the effect of forward scatter with an unsharp filter and a model-based method to remove the backscatter effect. Tests in varying turbidity show that the scatter correction removes the adverse effect of scatter on the phase estimates. The adaptive frequency unwrapping with scatter correction results in images with higher accuracy and precision and less phase unwrap errors than the Gray-Code Phase Stepping (GCPS) approach.},
author = {Risholm, Petter and Kirkhus, Trine and Thielemann, Jens T. and Thorstensen, Jostein},
doi = {10.3390/s19051043},
file = {:home/miguel/Dropbox/Mendeley Desktop/Risholm et al. - 2019 - Adaptive Structured Light with Scatter Correction for High-Precision Underwater 3D Measurements.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {multi-frequency phase stepping,scatter correction,structured light,uncertainty,underwater},
month = {mar},
number = {5},
pages = {1043},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Adaptive Structured Light with Scatter Correction for High-Precision Underwater 3D Measurements}},
url = {https://www.mdpi.com/1424-8220/19/5/1043},
volume = {19},
year = {2019}
}
@article{Maccarone2019,
abstract = {Three-dimensional imaging in underwater environments was investigated using a picosecond resolution silicon single-photon avalanche diode (SPAD) detector array fabricated in complementary metal-oxide semiconductor (CMOS) technology. Each detector in the 192 × 128 SPAD array had an individual time-to-digital converter allowing rapid, simultaneous acquisition of data for the entire array using the time-correlated single-photon counting approach. A picosecond pulsed laser diode source operating at a wavelength of 670 nm was used to illuminate the underwater scenes, emitting an average optical power up to 8 mW. Both stationary and moving targets were imaged under a variety of underwater scattering conditions. The acquisition of depth and intensity videos of moving targets was demonstrated in dark laboratory conditions through scattering water, equivalent to having up to 6.7 attenuation lengths between the transceiver and target. Data were analyzed using a pixel-wise approach, as well as an image processing algorithm based on a median filter and polynomial approximation.},
author = {Maccarone, Aurora and Mattioli, Francesco and Rocca, Della and Mccarthy, Aongus and Henderson, Robert and Buller, Gerald S},
doi = {10.1364/OE.27.028437},
file = {:home/miguel/Dropbox/Mendeley Desktop/Maccarone et al. - 2019 - Three-dimensional imaging of stationary and moving targets in turbid underwater environments using a single-ph.pdf:pdf},
journal = {Optics Express},
number = {20},
pages = {28437},
title = {{Three-dimensional imaging of stationary and moving targets in turbid underwater environments using a single-photon detector array}},
url = {https://doi.org/10.1364/OE.27.028437},
volume = {27},
year = {2019}
}
@article{Zhang2016,
abstract = {Microelectromechanical (MEMS) mirrors have extended vision capabilities onto small, low-power platforms. However, the field-of-view (FOV) of these MEMS mirrors is usually less than 90° and any increase in the MEMS mirror scanning angle has design and fabrication trade-offs in terms of power, size, speed and stability. Therefore, we need techniques to increase the scanning range while still maintaining a small form factor. In this paper we exploit our recent breakthrough that has enabled the immersion of MEMS mirrors in liquid. While allowing the MEMS to move, the liquid additionally provides a “Snell's window” effect and enables an enlarged FOV (≈ 150°). We present an optimized MEMS mirror design and use it to demonstrate applications in extreme wide-angle structured light.},
author = {Zhang, Xiaoyang and Koppal, Sanjeev J and Zhang, Rui and Zhou, Liang and Butler, Elizabeth and Xie, Huikai},
doi = {10.1364/OE.24.003479},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zhang et al. - 2016 - Wide-angle structured light with a scanning MEMS mirror in liquid.pdf:pdf},
issn = {1094-4087},
journal = {Optics Express},
number = {4},
pages = {3479},
publisher = {CRC Press},
title = {{Wide-angle structured light with a scanning MEMS mirror in liquid}},
url = {www.sunnex.com https://www.osapublishing.org/abstract.cfm?URI=oe-24-4-3479},
volume = {24},
year = {2016}
}
@inproceedings{Duda2015,
abstract = {Sensing of environment geometry and texture is$\backslash$na key requirement for mobile robotic systems. In the underwater$\backslash$ndomain, difficult environmental conditions restrict$\backslash$nthe applicability of many existing methods for 3D sensing.$\backslash$nA new method is proposed, which uses a visible laser line$\backslash$nprojected onto a monocular camera image to perform 3D$\backslash$nscene reconstruction. The method fuses Structured Light with$\backslash$nStructure from Motion in an integrated process, which allows$\backslash$nfor the capturing of dense 3D point clouds on moving systems$\backslash$nin situations with low texture and minimal scene structure. The$\backslash$nsystem is evaluated in three experiment scenarios and provides$\backslash$nan average translation error of 2{\%} in the KITTI Benchmark$\backslash$nfor monocular visual odometry and an average model error of$\backslash$n1:7mm when compared with a tabletop structured light system.},
author = {Duda, Alexander and Schwendner, Jakob and Gaudig, Christopher},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353451},
file = {:home/miguel/Dropbox/Mendeley Desktop/Duda, Schwendner, Gaudig - 2015 - SRSL Monocular self-referenced line structured light.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
keywords = {Adaptive optics,Cameras,Feature extraction,Optical imaging,Robot sensing systems,Three-dimensional displays,Visualization},
month = {sep},
pages = {717--722},
publisher = {IEEE},
title = {{SRSL: Monocular self-referenced line structured light}},
url = {http://ieeexplore.ieee.org/document/7353451/},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Chua2018,
author = {Chua, Sing Yee and Chew, Kuew Wai and Guo, Ningqun and Wang, Xin},
booktitle = {2018 IEEE 7th International Conference on Photonics, ICP 2018},
doi = {10.1109/ICP.2018.8533195},
file = {:home/miguel/Dropbox/Mendeley Desktop/Chua et al. - 2018 - Three-dimensional (3D) Reconstruction of Range Gated Imaging.pdf:pdf},
isbn = {9781538611876},
keywords = {3D reconstruction,Image processing,Range gated imaging},
month = {apr},
pages = {1--3},
publisher = {IEEE},
title = {{Three-dimensional (3D) Reconstruction of Range Gated Imaging}},
url = {https://ieeexplore.ieee.org/document/8533195/},
year = {2018}
}
@inproceedings{OMahony2018,
author = {O'Mahony, Niall and Campbell, Sean and Krpalkova, Lenka and Riordan, Daniel and Walsh, Joseph and Murphy, Aidan and Ryan, Conor},
booktitle = {Proceedings of SAI Intelligent Systems Conference},
file = {:home/miguel/Dropbox/Mendeley Desktop/O'Mahony et al. - 2018 - Computer Vision for 3D Perception a review.pdf:pdf},
organization = {Springer},
pages = {788--804},
title = {{Computer Vision for 3D Perception: a review}},
year = {2018}
}
@inproceedings{Caimi2008,
abstract = {Obtaining satisfactory visibility of undersea objects has been historically difficult due to the absorptive and scattering properties of seawater. Mitigating these effects has been a long term research focus, but recent advancements in hardware, software, and algorithmic methods have led to noticeable improvement in system operational range. This paper is intended to provide a summary of recently reported research in the area of Underwater Optics and Vision and briefly covers advances in the following areas: 1) Image formation and image processing methods; 2) Extended range imaging techniques; 3) Imaging using spatial coherency (e.g. holography); and 4) Multipledimensional image acquisition and image processing.},
author = {Caimi, Frank Michael and Kocak, Donna M. and Dalgleish, Fraser R. and Watson, John},
booktitle = {OCEANS 2008},
doi = {10.1109/OCEANS.2008.5152118},
file = {:home/miguel/Dropbox/Mendeley Desktop/Caimi et al. - 2008 - Underwater imaging and optics Recent advances.pdf:pdf},
isbn = {9781424426201},
pages = {1--9},
publisher = {IEEE},
title = {{Underwater imaging and optics: Recent advances}},
url = {http://ieeexplore.ieee.org/document/5289438/},
year = {2008}
}
@inproceedings{Fujimura2018,
abstract = {Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.},
archivePrefix = {arXiv},
arxivId = {1804.02836},
author = {Fujimura, Yuki and Iiyama, Masaaki and Hashimoto, Atsushi and Minoh, Michihiko},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
eprint = {1804.02836},
file = {:home/miguel/Dropbox/Mendeley Desktop/Fujimura et al. - 2018 - Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter.pdf:pdf},
pages = {7445----7453},
title = {{Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Fujimura{\_}Photometric{\_}Stereo{\_}in{\_}CVPR{\_}2018{\_}paper.pdf http://arxiv.org/abs/1804.02836},
year = {2018}
}
@inproceedings{Hess2016,
abstract = {Portable laser range-finders, further referred to as LIDAR, and simultaneous localization and mapping (SLAM) are an efficient method of acquiring as-built floor plans. Generating and visualizing floor plans in real-time helps the operator assess the quality and coverage of capture data. Building a portable capture platform necessitates operating under limited computational resources. We present the approach used in our backpack mapping platform which achieves real-time mapping and loop closure at a 5 cm resolution. To achieve realtime loop closure, we use a branch-and-bound approach for computing scan-to-submap matches as constraints. We provide experimental results and comparisons to other well known approaches which show that, in terms of quality, our approach is competitive with established techniques.},
author = {Hess, Wolfgang and Kohler, Damon and Rapp, Holger and Andor, Daniel},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487258},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hess et al. - 2016 - Real-time loop closure in 2D LIDAR SLAM.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
month = {jun},
pages = {1271--1278},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Real-time loop closure in 2D LIDAR SLAM}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Romer2014,
abstract = {Optical solid state deflectors rely on the electro-optical or acousto-optic effect. These Electro-Optical Deflectors (EODs) and Acousto-Optical Deflectors (AODs) do not contain moving parts and therefore exhibit high deflection velocities and are free of drawbacks associated with mechanical scanners. A description of the principles of operation of EODs and AODs is presented. In addition, characteristics, properties and the (dis)advantages of EODs and AODs, when compared to mirror based mechanical deflectors, is discussed. Deflection angles, speed and accuracy are discussed in terms of resolvable spots and related quantities. Also, response time, damage threshold, efficiency and the type and magnitude of beam distortions is addressed. Optical deflectors are characterized by high angular deflection velocities, but small deflection angles. Whereas mechanical mechanical scanners are characterized by relatively small deflection velocities, but large deflection angles. Arranging an optical deflector and a mechanical scanner in series allows to take advantage of the best of both worlds.},
author = {R{\"{o}}mer, G. R.B.E. and Bechtold, Peter},
booktitle = {Physics Procedia},
doi = {10.1016/j.phpro.2014.08.092},
file = {:home/miguel/Dropbox/Mendeley Desktop/R{\"{o}}mer, Bechtold - 2014 - Electro-optic and acousto-optic laser beam scanners.pdf:pdf},
issn = {18753892},
keywords = {Acousto-optical deflector,Electro-optical deflector,Large stroke small stroke,Laser beam scanner,Mirror-based deflection,Optical solid state deflectors,review},
mendeley-tags = {review},
number = {C},
pages = {29--39},
title = {{Electro-optic and acousto-optic laser beam scanners}},
url = {www.sciencedirect.com},
volume = {56},
year = {2014}
}
@phdthesis{Tengesdal2012,
author = {Tengesdal, Oyvind Aasen},
file = {:home/miguel/Dropbox/Mendeley Desktop/Tengesdal - 2012 - Measurement of seawater refractive index and salinity by means of optical refraction.pdf:pdf},
number = {May},
pages = {134},
school = {University of Bergen},
title = {{Measurement of seawater refractive index and salinity by means of optical refraction}},
url = {http://bora.uib.no/bitstream/handle/1956/6540/95091835.pdf;jsessionid=C0BFDAD4A0C41E5FC958774F7FBF8E20.bora-uib{\_}worker?sequence=1},
year = {2012}
}
@inproceedings{Dalgleish2007,
abstract = {Laser line scan (LLS) underwater imaging is a serial imaging technique that involves the optical scanning of a narrow instantaneous field of view (IFOV) receiver in a synchronous fashion with a highly collimated laser source over a wide swath. It is widely regarded as the optimal technology for extended range underwater optical imaging, with up to 6 attenuation lengths achievable in turbid seawater. These imagers, which utilize high power green CW lasers, require an adequate source- receiver separation to reduce near field backscattering image degradation. They have been successfully deployed onboard towed bodies, and have potential as AUV-deployed imagers except their large size and high sensitivity to changes in operating conditions and environment makes them difficult to use successfully with these platforms. Harnessing recent developments in high power, high repetition rate green pulsed lasers, high speed gate-able photomultiplier tubes, rapid parallel digital signal processors and compact underwater scanning systems, this paper describes the latest approach to demonstrating a prototype pulsed laser line scan (PLLS) imager which has the required compactness and ease of operability to make it more compatible for implementation onboard the common classes of AUV.},
author = {Dalgleish, Fraser R. and Caimi, Frank Michael and Britton, W. B. and Andren, C. F.},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/OCEANS.2007.4449184},
file = {:home/miguel/Dropbox/Mendeley Desktop/Dalgleish et al. - 2007 - An AUV-deployable pulsed laser line scan (PLLS) imaging sensor.pdf:pdf},
isbn = {0933957351},
issn = {01977385},
keywords = {Laser imaging systems,Laser line scan,Pulsed laser line scan,Underwater imaging},
month = {sep},
pages = {1--5},
publisher = {IEEE},
title = {{An AUV-deployable pulsed laser line scan (PLLS) imaging sensor}},
url = {http://ieeexplore.ieee.org/document/4449184/},
year = {2007}
}
@article{Bernardina2016,
abstract = {Action sport cameras (ASC) are currently adopted mainly for entertainment purposes but their uninterrupted technical improvements, in correspondence of cost decreases, are going to disclose them for three-dimensional (3D) motion analysis in sport gesture study and athletic performance evaluation quantitatively. Extending this technology to sport analysis however still requires a methodologic step-forward to making ASC a metric system, encompassing ad-hoc camera setup, image processing, feature tracking, calibration and 3D reconstruction. Despite traditional laboratory analysis, such requirements become an issue when coping with both indoor and outdoor motion acquisitions of athletes. In swimming analysis for example, the camera setup and the calibration protocol are particularly demanding since land and underwater cameras are mandatory. In particular, the underwater camera calibration can be an issue affecting the reconstruction accuracy. In this paper, the aim is to evaluate the feasibility of ASC for 3D underwater analysis by focusing on camera setup and data acquisition protocols. Two GoPro Hero3+ Black (frequency: 60Hz; image resolutions: 1280×720/1920×1080 pixels) were located underwater into a swimming pool, surveying a working volume of about 6m3. A two-step custom calibration procedure, consisting in the acquisition of one static triad and one moving wand, carrying nine and one spherical passive markers, respectively, was implemented. After assessing camera parameters, a rigid bar, carrying two markers at known distance, was acquired in several positions within the working volume. The average error upon the reconstructed inter-marker distances was less than 2.5mm (1280×720) and 1.5mm (1920×1080). The results of this study demonstrate that the calibration of underwater ASC is feasible enabling quantitative kinematic measurements with accuracy comparable to traditional motion capture systems.},
author = {Bernardina, Gustavo R.D. and Cerveri, Pietro and Barros, Ricardo M.L. and Marins, Jo{\~{a}}o C.B. and Silvatti, Amanda P},
doi = {10.1371/journal.pone.0160490},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bernardina et al. - 2016 - Action sport cameras as an instrument to perform a 3D underwater motion analysis.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
title = {{Action sport cameras as an instrument to perform a 3D underwater motion analysis}},
url = {www.capes.gov.br},
volume = {11},
year = {2016}
}
@inproceedings{Montagu1986,
abstract = {This paper discusses methods of achieving the optimum in high resolution galvanometric scanning. When scanning systems have problems of repeatability, jitter, and wobble, the difficulties are often caused by inadequate mirror design, bonding, and/or alignment and the dynamic forces engendered by the driving torque. These issues are examined, and integral metal mirror mounts are compared with the conventional composite mirrors bonded to metal mounts. Galvanometer bearing suspension and drive electronics and how they relate to optimal high resolution are also addressed.},
author = {Montagu, Jean I},
booktitle = {Infrared Technology and Applications},
doi = {10.1117/12.951964},
file = {:home/miguel/Dropbox/Mendeley Desktop/Montagu - 1986 - Achieving optimal high resolution in galvanometric scanning systems.pdf:pdf},
month = {may},
pages = {47--53},
publisher = {International Society for Optics and Photonics},
title = {{Achieving optimal high resolution in galvanometric scanning systems}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.951964},
volume = {590},
year = {1986}
}
@article{Levoy2004,
abstract = {An imaging system that combines synthetic-aperture imaging, holography, and an optical chirp with confocal imaging is described and analyzed. Comparisons are made with synthetic-aperture radar systems. Adaptation of several synthetic-aperture radar techniques to the optical counterparts is suggested.},
author = {Levoy, Marc and Chen, Billy and Vaish, Vaibhav and Horowitz, Mark and McDowall, Ian and Bolas, Mark},
doi = {10.1145/1015706.1015806},
file = {:home/miguel/Dropbox/Mendeley Desktop/Levoy et al. - 2004 - Synthetic aperture confocal imaging.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {ACM Transactions on Graphics},
number = {3},
pages = {825},
pmid = {16463735},
title = {{Synthetic aperture confocal imaging}},
url = {https://graphics.stanford.edu/papers/confocal/levoy-confocal-sig04.pdf http://portal.acm.org/citation.cfm?doid=1015706.1015806},
volume = {23},
year = {2004}
}
@phdthesis{Riu2018,
author = {Riu, Jordi},
file = {:home/miguel/Dropbox/Mendeley Desktop/Riu - 2018 - C{\'{a}}mara LiDAR de escaneo MEMS para imagen 3D de resoluci{\'{o}}n espacial variable.pdf:pdf},
school = {Universitat Polit{\`{e}}cnica de Catalunya},
title = {{C{\'{a}}mara LiDAR de escaneo MEMS para imagen 3D de resoluci{\'{o}}n espacial variable}},
year = {2018}
}
@inproceedings{Jordt-Sedlazeck2012,
abstract = {In underwater computer vision, images are inﬂuenced by the water in two diﬀerent ways. First, while still traveling through the water, light is absorbed and scattered, both of which are wavelength dependent, thus create the typical green or blue hue in underwater images. Secondly, when entering the underwater housing, the rays are refracted, aﬀecting image formation geometrically. When using underwater images in for ex- ample Structure-from-Motion applications, both eﬀects need to be taken into account. Therefore, we present a novel method for calibrating the parameters of an underwater camera housing. An evolutionary optimiza- tion algorithm is coupled with an analysis-by-synthesis approach, which allows to calibrate the parameters of a light propagation model for the local water body. This leads to a highly accurate calibration method for camera-glass distance and glass normal with respect to the optical axis. In addition, a model for the distance dependent eﬀect of water on light propagation is parametrized and can be used for color correction.},
author = {Jordt-Sedlazeck, Anne and Koch, Reinhard},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33715-4_61},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jordt-Sedlazeck, Koch - 2012 - Refractive calibration of underwater cameras.pdf:pdf},
isbn = {9783642337147},
issn = {03029743},
number = {PART 5},
pages = {846--859},
title = {{Refractive calibration of underwater cameras}},
url = {http://www.mip.informatik.uni-kiel.de},
volume = {7576 LNCS},
year = {2012}
}
@incollection{bourouina2005,
author = {Bourouina, T and Fujita, H and Reyne, G},
booktitle = {MOEMS: Micro-Opto-Electro-Mechanical Systems},
publisher = {SPIE Press, Bellingham, Washington},
title = {{Optical scanning}},
year = {2005}
}
@misc{Sakai2000,
author = {Sakai, Hiroshi},
file = {:home/miguel/Dropbox/Mendeley Desktop/Sakai - 2000 - Underwater laser imaging apparatus, USOO6115511A.pdf:pdf},
title = {{Underwater laser imaging apparatus, USOO6115511A}},
url = {https://patentimages.storage.googleapis.com/f7/4f/ad/bd0865153c7d8c/US6115511.pdf},
year = {2000}
}
@article{Li1997,
abstract = {This paper presents a photogrammetric model for digital underwater$\backslash$nvideo imagery, which has been mostly applied to qualitative analysis in$\backslash$nthe marine environment. With this model, quantitative analysis of$\backslash$nunderwater images is possible, e.g., to locate positions, calculate$\backslash$nsizes, and measure shapes of objects from image features. The underwater$\backslash$nphotogrammetric model is based on a three-dimensional optical ray$\backslash$ntracing technique which rigorously models imaging systems with multilens$\backslash$nconfigurations and multiple refractions. The calibration procedure with$\backslash$ntwo independent phases has been proven to be efficient in simplifying$\backslash$nthe computation and improving the calibration accuracy. With the current$\backslash$nimaging system configuration and photogrammetric model, an accuracy of$\backslash$n0.8 cm in lateral directions and 1.2 cm along the depth direction for$\backslash$nobjects located about 2-3 m from the camera system in the object space$\backslash$nis attainable. A PC-based digital underwater photogrammetric prototype$\backslash$nsystem has been developed to implement the underwater photogrammetric$\backslash$nmodel},
author = {Li, Rongxing and Li, Haihao and Zou, Weihong and Smith, Robert G. and Curran, Terry A.},
doi = {10.1109/48.585955},
file = {:home/miguel/Dropbox/Mendeley Desktop/Li et al. - 1997 - Quantitative photogrammetric analysis of digital underwater video imagery.pdf:pdf},
isbn = {0364-9059 VO - 22},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {CCD image processing,Underwater imaging,Underwater photogrammetry,Underwater vision},
month = {apr},
number = {2},
pages = {364--375},
title = {{Quantitative photogrammetric analysis of digital underwater video imagery}},
url = {http://ieeexplore.ieee.org/document/585955/},
volume = {22},
year = {1997}
}
@article{Izquierdo1999,
abstract = {A method for sub-pixel measurement of 3D surfaces based on vision cameras and structured light is presented. The method includes algorithms for sub-pixel calibration, laser stripe detection and 3D reconstruction. In this work, different strategies for sub-pixel calibration and control point extraction are implemented and compared obtaining accuracy around 0.2 pixels for the 3D coordinates. Reconstruction algorithms have been selected to fulfill the accuracy and processing speed demanded in industrial applications.},
author = {Izquierdo, M. A.G. and Sanchez, M. T. and Iba{\~{n}}ez, A. and Ullate, L. G.},
doi = {10.1016/S0924-4247(98)00283-0},
file = {:home/miguel/Dropbox/Mendeley Desktop/Izquierdo et al. - 1999 - Sub-pixel measurement of 3D surfaces by laser scanning.pdf:pdf},
issn = {09244247},
journal = {Sensors and Actuators, A: Physical},
month = {aug},
number = {1-3},
pages = {1--8},
title = {{Sub-pixel measurement of 3D surfaces by laser scanning}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0924424798002830},
volume = {76},
year = {1999}
}
@article{Imaki2016,
abstract = {We developed an underwater three-dimensional (3-D) imaging sensor using a 532-nm laser. The sensor system combines a dome lens with coaxial optics to realize a wide-scanning angle of 120 deg ðhorizontalÞ × 30 deg ðverticalÞ while having a compact size of 25-cm diameter and 60-cm length. A detector sensitivity time control circuit and a time-to-digital converter are used to detect a small signal and suppress the unwanted backscattered signals due to marine snow. 3-D imaging of the seafloor with 20-m width and 60-m length was demonstrated in the sea around Ishigaki Island, Japan.},
author = {Imaki, Masaharu and Ochimizu, Hideaki and Tsuji, Hidenobu and Kameyama, Shumpei and Saito, Takashi and Ishibashi, Shojiro and Yoshida, Hiroshi},
doi = {10.1117/1.oe.56.3.031212},
file = {:home/miguel/Dropbox/Mendeley Desktop/Imaki et al. - 2016 - Underwater three-dimensional imaging laser sensor with 120-deg wide-scanning angle using the combination of a dome.pdf:pdf},
issn = {0091-3286},
journal = {Optical Engineering},
keywords = {ranging,time-of-flight,underwater imaging},
month = {oct},
number = {3},
pages = {031212},
publisher = {International Society for Optics and Photonics},
title = {{Underwater three-dimensional imaging laser sensor with 120-deg wide-scanning angle using the combination of a dome lens and coaxial optics}},
url = {http://opticalengineering.spiedigitallibrary.org/article.aspx?doi=10.1117/1.OE.56.3.031212},
volume = {56},
year = {2016}
}
@article{Han2010,
abstract = {Reported is a numerical calculation of near-field and far-field intensities for two electrowetting microprism designs. This includes an investigation of one-dimensional, two-dimensional, and bi-prism pixellated arrays. The diffraction efficiency of an incident Gaussian beam is investigated. Far-field deflection angles are extended to more than 10°. For the phased arrays the diffraction angles are discrete and the angles between the diffraction peaks can be covered by applying a tilted phase to the input field. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Han, Wei and Haus, Joseph W and McManamon, Paul and Heikenfeld, Jason and Smith, Neil and Yang, Jia},
doi = {10.1016/j.optcom.2009.11.013},
file = {:home/miguel/Dropbox/Mendeley Desktop/Han et al. - 2010 - Transmissive beam steering through electrowetting microprism arrays.pdf:pdf},
issn = {00304018},
journal = {Optics Communications},
keywords = {Beam steering,Electrowetting microprism,Far-field diffraction efficiency},
number = {6},
pages = {1174--1181},
title = {{Transmissive beam steering through electrowetting microprism arrays}},
url = {https://pdf.sciencedirectassets.com/271557/1-s2.0-S0030401810X00025/1-s2.0-S0030401809011420/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEH0aCXVzLWVhc3QtMSJGMEQCIA0q06EpupXoHUVDcNWsIrvvSragLlLW6LW3UEKtfrqTAiBNr0pdjPh3Ws25Cq6y{\%}2BIUrG8nmHAhQo2r6oVLl4sE0Ey},
volume = {283},
year = {2010}
}
@inproceedings{DeDominicis2013,
author = {de Dominicis, Luigi and Fornetti, G and Guarneri, M and de Collibus, M Ferri and Francucci, M and Nuvoli, M and Al-Obaidi, A and Mcstay, D},
booktitle = {Offshore Mediterranean Conference and Exhibition},
file = {:home/miguel/Dropbox/Mendeley Desktop/de Dominicis et al. - 2013 - Structural Monitoring Of Offshore Platforms By 3d Subsea Laser Profilers.pdf:pdf},
isbn = {9788894043617},
month = {mar},
publisher = {Offshore Mediterranean Conference},
title = {{Structural Monitoring Of Offshore Platforms By 3d Subsea Laser Profilers.}},
url = {https://www.onepetro.org/conference-paper/OMC-2013-004 http://www.researchgate.net/publication/259298916{\_}STRUCTURAL{\_}MONITORING{\_}OF{\_}OFFSHORE{\_}PLATFORMS{\_}BY{\_}3D{\_}SUBSEA{\_}LASER{\_}PROFILERS/file/72e7e52aea74c40d49.pdf},
year = {2013}
}
@article{OByrne2018,
abstract = {{\textcopyright} 2017 Computer-Aided Civil and Infrastructure Engineering Underwater inspections stand to gain from using stereo imaging systems to collect three-dimensional measurements. Although many stereo-matching algorithms have been devised to solve the correspondence problem, that is, find the same points in multiple images, these algorithms often perform poorly when applied to images of underwater scenes due to the poor visibility and the complex underwater light field. This article presents a new stereo-matching algorithm, called PaLPaBEL (Pyramidal Loopy Propagated BELief) that is designed to operate on challenging imagery. At its core, PaLPaBEL is a semiglobal method based on a loopy belief propagation message passing algorithm applied on a Markov random field. A pyramidal scheme is adopted that enables wide disparity ranges and high-resolution images to be handled efficiently. For performance evaluation, PaLPaBEL is applied to underwater stereo images captured under various visibility conditions in a laboratory setting, and to synthetic imagery created in a virtual underwater environment. The technique is also demonstrated on stereo images obtained from a real-world inspection. The successful results indicate that PaLPaBEL is well suited for underwater application and has value as a tool for the cost-effective inspection of marine structures.},
author = {O'Byrne, Michael and Pakrashi, Vikram and Schoefs, Franck and Ghosh, Bidisha},
doi = {10.1111/mice.12307},
file = {:home/miguel/Dropbox/Mendeley Desktop/O'Byrne et al. - 2018 - A Stereo-Matching Technique for Recovering 3D Information from Underwater Inspection Imagery.pdf:pdf},
issn = {14678667},
journal = {Computer-Aided Civil and Infrastructure Engineering},
number = {3},
pages = {193--208},
title = {{A Stereo-Matching Technique for Recovering 3D Information from Underwater Inspection Imagery}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/mice.12307},
volume = {33},
year = {2018}
}
@inproceedings{Ishibashi2017,
abstract = {{\textcopyright} 2017 Marine Technology Society. There is a possibility that vast amounts of undersea resources are buried beneath Japanese territorial sea. In order to find these undersea resources, a detailed topography including specific objects and events of seabed should be carefully surveyed. One of effective methods for this is to use seabed visualization technologies, which are applied to an autonomous underwater vehicle (AUV). Therefore, Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has been developing two kinds of optical imaging sensors: (1)a stereo vision system and (2)a 3-dimensional underwater laser scanner (3dULS). Currently, the second prototype of 3dULS has been developed and some remarkable results were shown in field tests applying an AUV.},
author = {Ishibashi, Shojiro and Ohta, Y. and Sugesawa, M. and Tanaka, K. and Yoshida, H. and Choi, S.K.},
booktitle = {OCEANS 2017 - Anchorage},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ishibashi et al. - 2017 - Seabed 3D images created by an underwater laser scanner applied to an AUV.pdf:pdf},
isbn = {9780692946909},
pages = {1----5},
title = {{Seabed 3D images created by an underwater laser scanner applied to an AUV}},
url = {https://ieeexplore.ieee.org/abstract/document/8232100},
volume = {2017-Janua},
year = {2017}
}
@article{Palomer2018,
abstract = {{\textcopyright} 2018 by the authors. Licensee MDPI, Basel, Switzerland. Nowadays, research in autonomous underwater manipulation has demonstrated simple applications like picking an object from the sea floor, turning a valve or plugging and unplugging a connector. These are fairly simple tasks compared with those already demonstrated by the mobile robotics community, which include, among others, safe arm motion within areas populated with a priori unknown obstacles or the recognition and location of objects based on their 3D model to grasp them. Kinect-like 3D sensors have contributed significantly to the advance of mobile manipulation providing 3D sensing capabilities in real-time at low cost. Unfortunately, the underwater robotics community is lacking a 3D sensor with similar capabilities to provide rich 3D information of the work space. In this paper, we present a new underwater 3D laser scanner and demonstrate its capabilities for underwater manipulation. In order to use this sensor in conjunction with manipulators, a calibration method to find the relative position between the manipulator and the 3D laser scanner is presented. Then, two different advanced underwater manipulation tasks beyond the state of the art are demonstrated using two different manipulation systems. First, an eight Degrees of Freedom (DoF) fixed-base manipulator system is used to demonstrate arm motion within a work space populated with a priori unknown fixed obstacles. Next, an eight DoF free floating Underwater Vehicle-Manipulator System (UVMS) is used to au tonomously grasp an object from the bottom of a water tank.},
author = {Palomer, Albert and Ridao, Pere and Youakim, Dina and Ribas, David and Forest, Josep and Petillot, Yvan and {Pe{\~{n}}alver Monfort}, Antonio and Sanz, Pedro J.},
doi = {10.3390/s18041086},
file = {:home/miguel/Dropbox/Mendeley Desktop/Palomer et al. - 2018 - 3D Laser Scanner for Underwater Manipulation.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {3d,laser,manipulation,point clouds,underwater},
number = {4},
pages = {1--14},
title = {{3D Laser Scanner for Underwater Manipulation.}},
volume = {18},
year = {2018}
}
@inproceedings{Constantinou2016,
abstract = {In this paper we described the development of both the hardware and the algorithms for a novel laser vision system suitable for measuring distances from both solid and mesh-like targets in underwater environments. The system was developed as a part of the AQUABOT project that developed an underwater robotic system for autonomous inspection of offshore aquaculture installation. The system takes into account the hemispherical optics typical in underwater vehicle designs and implements an array of line-lasers to ensure that mesh-like targets provide reflections in a consistent manner. The developed algorithms for the laser vision system are capable of providing either raw pointcloud data sets from each laser or with additional processing high level information like distance and relative orientation of the target with respect to the ROV can be recovered. An automatic calibration procedure along with the accompanying hardware that was developed, is described in this paper, to reduce the calibration overhead required by regular maintenance operations as is typical for underwater vehicles operating in sea-water. A set of experimental results in controlled laboratory environment as well as at offshore aquaculture installations demonstrate the performance of the system.},
author = {Constantinou, Christos C. and Loizou, Savvas G. and Georgiades, George P.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759320},
file = {:home/miguel/Dropbox/Mendeley Desktop/Constantinou, Loizou, Georgiades - 2016 - An underwater laser vision system for relative 3-D posture estimation to mesh-like targets.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
month = {oct},
pages = {2036--2041},
publisher = {IEEE},
title = {{An underwater laser vision system for relative 3-D posture estimation to mesh-like targets}},
url = {http://ieeexplore.ieee.org/document/7759320/},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{Sedlazeck2012,
abstract = {When capturing images underwater, image formation is affected in two major ways. First, the light rays traveling underwater are absorbed and scattered depending on their wavelength, creating effects on the image colors. Secondly, the glass interface between air and water refracts the ray entering the camera housing because of a different index of refraction of water, hence the ray is also affected in a geometrical way. This paper examines different camera models and their capabilities to deal with geometrical effects caused by refraction. Using imprecise camera models leads to systematic errors when computing 3D reconstructions or otherwise exploiting geometrical properties of images. In the literature, many authors have published work on underwater imaging by using the perspective pinhole camera model (single viewpoint model - SVP) with a different effective focal length and distortion to compensate for the error induced by refraction at the camera housing. On the other hand, methods were proposed, where refraction is modeled explicitly or where generic, non-single-view-point camera models are used. In addition to discussing all three model categories, an accuracy analysis of using the perspective model on underwater images is given and shows that the perspective model leads to systematic errors that compromise measurement accuracy.},
author = {Sedlazeck, Anne and Koch, Reinhard},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34091-8_10},
file = {:home/miguel/Dropbox/Mendeley Desktop/Sedlazeck, Koch - 2012 - Perspective and non-perspective camera models in underwater imaging - Overview and error analysis.pdf:pdf},
isbn = {9783642340901},
issn = {03029743},
pages = {212--242},
title = {{Perspective and non-perspective camera models in underwater imaging - Overview and error analysis}},
url = {http://www.mip.informatik.uni-kiel.de},
volume = {7474 LNCS},
year = {2012}
}
@article{Yoerger2007,
abstract = {This paper reports the development and at-sea deployment of a set of algorithms that have enabled the autonomous underwater vehicle ABE to conduct near-bottom surveys in the deep sea. Algorithms for long baseline acoustic positioning, terrain-following, and automated nested surveys are reported.},
author = {Yoerger, Dana R and Jakuba, Michael and Bradley, Albert M and Bingham, Brian},
doi = {10.1177/0278364907073773},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yoerger et al. - 2007 - Techniques for deep sea near bottom survey using an autonomous underwater vehicle.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {AUV,Automated nested survey,Hydrothermal vent discovery,Long baseline acoustic navigation,Terrain-following},
number = {1},
pages = {41--54},
title = {{Techniques for deep sea near bottom survey using an autonomous underwater vehicle}},
url = {https://journals.sagepub.com/doi/pdf/10.1177/0278364907073773},
volume = {26},
year = {2007}
}
@article{Hale1973,
abstract = {Extinction coefficients k(lambda) for water at 25 degrees C were determined through a broad spectral region by manually smoothing a point by point graph of k(lambda) vs wavelength lambda that was plotted for data obtained from a review of the scientific literature on the optical constants of water. Absorption bands representing k(lambda) were postulated where data were not available in the vacuum uv and soft x-ray regions. A subtractive Kramers-Kronig analysis of the combined postulated and smoothed portions of the k(lambda) spectrum provided the index of refraction n(lambda) for the spectral region 200 nm {\textless}/= lambda {\textless}/= 200 microm.},
author = {Hale, George M. and Querry, Marvin R.},
doi = {10.1364/ao.12.000555},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hale, Querry - 1973 - Optical Constants of Water in the 200-nm to 200-$\mu$m Wavelength Region.pdf:pdf},
issn = {0003-6935},
journal = {Applied Optics},
keywords = {Absorption coefficient,Optical constants,Optical properties,Refractive index,Soft x rays,Vacuum ultraviolet},
month = {mar},
number = {3},
pages = {555},
publisher = {Optical Society of America},
title = {{Optical Constants of Water in the 200-nm to 200-$\mu$m Wavelength Region}},
url = {https://www.osapublishing.org/abstract.cfm?URI=ao-12-3-555},
volume = {12},
year = {1973}
}
@article{Peng2017,
abstract = {Underwater images often suffer from color distor-tion and low contrast, because light is scattered and absorbed when traveling through water. Such images with different color tones can be shot in various lighting conditions, making restora-tion and enhancement difficult. We propose a depth estimation method for underwater scenes based on image blurriness and light absorption, which can be used in the image formation model (IFM) to restore and enhance underwater images. Previous IFM-based image restoration methods estimate scene depth based on the dark channel prior or the maximum intensity prior. These are frequently invalidated by the lighting conditions in underwater images, leading to poor restoration results. The pro-posed method estimates underwater scene depth more accurately. Experimental results on restoring real and synthesized underwa-ter images demonstrate that the proposed method outperforms other IFM-based underwater image restoration methods.},
author = {Peng, Yan Tsung and Cosman, Pamela C.},
doi = {10.1109/TIP.2017.2663846},
file = {:home/miguel/Dropbox/Mendeley Desktop/Peng, Cosman - 2017 - Underwater Image Restoration Based on Image Blurriness and Light Absorption.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Underwater image,blurriness,depth estimation,image enhancement,image restoration,light absorption},
month = {apr},
number = {4},
pages = {1579--1594},
title = {{Underwater Image Restoration Based on Image Blurriness and Light Absorption}},
url = {http://ieeexplore.ieee.org/document/7840002/},
volume = {26},
year = {2017}
}
@inproceedings{Busck2004a,
abstract = {We have developed a mono-static staring 3-D laser radar based on gated viewing with range accuracy below 1 mm at 10 m and 1 cm at 100 m. We use a high sensitivity, fast, intensified CCD camera, and a Nd:YAG passively Q-switched 32.4 kHz pulsed green laser at 532 nm. The CCD has 752×582 pixels. Camera shutter is controlled in steps of 100 ps. Camera delay is controlled in step of 100 ps. Each laser pulse triggers the camera delay and shutter. A 3-D image is constructed from a sequence of 50-100 2-D reflectivity images, where each frame integrates {\~{}}700 laser pulses on the CCD. In 50 Hz video mode we record a 2-D sequence in a second and process a 3-D image in few seconds. We compare 3-D images with a system performance model.},
author = {Busck, Jens and Heiselberg, Henning},
doi = {10.1117/12.545397},
editor = {Kamerman, Gary W.},
issn = {0277786X},
month = {sep},
pages = {257},
publisher = {International Society for Optics and Photonics},
title = {{High accuracy 3D laser radar}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.545397},
volume = {5412},
year = {2004}
}
@article{Anwer2017,
abstract = {Commercial RGB-D cameras provide the possibility of fast, accurate, and cost-effective 3-D scanning solution in a single package. These economical depth cameras provide several advantages over conventional depth sensors, such as sonars and lidars, in specific usage scenarios. In this paper, we analyze the performance of Kinect v2 time-of-flight camera while operating fully submerged underwater in a customized waterproof housing. Camera calibration has been performed for Kinect's RGB and NIR cameras, and the effect of calibration on the generated 3-D mesh is discussed in detail. To overcome the effect of refraction of light due to the sensor housing and water, we propose a time-of-flight correction method and a fast, accurate and intuitive refraction correction method that can be applied to the acquired depth images, during 3-D mesh generation. Experimental results show that the Kinect v2 can acquire point cloud data up to 650 mm. The reconstruction results have been analyzed qualitatively and quantitatively, and confirm that the 3-D reconstruction of submerged objects at small distances is possible without the requirement of any external NIR light source. The proposed algorithms successfully generated 3-D mesh with a mean error of 6 mm at a frame rate of nearly 10 fps. We acquired a large data set of RGB, IR and depth data from a submerged Kinect v2. The data set covers a large variety of objects scanned underwater and is publicly available for further use, along with the Kinect waterproof housing design and correction filter codes. The research is aimed toward small-scale research activities and economical solution for 3-D scanning underwater. Applications such as coral reef mapping and underwater SLAM in shallow waters for ROV's can be a viable application area that can benefit from results achieved.},
author = {Anwer, Atif and {Azhar Ali}, Syed Saad and Khan, Amjad and Meriaudeau, Fabrice},
doi = {10.1109/ACCESS.2017.2733003},
file = {:home/miguel/Dropbox/Mendeley Desktop/Anwer et al. - 2017 - Underwater 3-D Scene Reconstruction Using Kinect v2 Based on Physical Models for Refraction and Time of Flight Cor.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {3D reconstruction,Kinect v2,refraction correction,underwater},
pages = {15960--15970},
title = {{Underwater 3-D Scene Reconstruction Using Kinect v2 Based on Physical Models for Refraction and Time of Flight Correction}},
url = {http://ieeexplore.ieee.org/document/8000305/},
volume = {5},
year = {2017}
}
@article{Bianco2013,
abstract = {In some application fields, such as underwater archaeology or marine biology, there is the need to collect three-dimensional, close-range data from objects that cannot be removed from their site. In particular, 3D imaging techniques are widely employed for close-range acquisitions in underwater environment. In this work we have compared in water two 3D imaging techniques based on active and passive approaches, respectively, and whole-field acquisition. The comparison is performed under poor visibility conditions, produced in the laboratory by suspending different quantities of clay in a water tank. For a fair comparison, a stereo configuration has been adopted for both the techniques, using the same setup, working distance, calibration, and objects. At the moment, the proposed setup is not suitable for real world applications, but it allowed us to conduct a preliminary analysis on the performances of the two techniques and to understand their capability to acquire 3D points in presence of turbidity. The performances have been evaluated in terms of accuracy and density of the acquired 3D points. Our results can be used as a reference for further comparisons in the analysis of other 3D techniques and algorithms.},
author = {Bianco, Gianfranco and Gallo, Alessandro and Bruno, Fabio and Muzzupappa, Maurizio},
doi = {10.3390/s130811007},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bianco et al. - 2013 - A comparative analysis between active and passive techniques for underwater 3D reconstruction of close-range obje.pdf:pdf},
isbn = {1100711031},
issn = {14248220},
journal = {Sensors},
keywords = {3D reconstruction,Active and passive 3D techniques,Underwater imaging},
month = {aug},
number = {8},
pages = {11007--11031},
pmid = {23966193},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{A comparative analysis between active and passive techniques for underwater 3D reconstruction of close-range objects}},
url = {http://www.mdpi.com/1424-8220/13/8/11007},
volume = {13},
year = {2013}
}
@article{Xiang2009,
abstract = {In the field of lidar system design, there is a need for laser scanners that offer fast linear scanning, are small size and have small a rotational inertia moment. Currently, laser scanners do not meet the above needs. A new laser scanner based on two amplified piezoelectric actuators is designed in this paper. The laser scanner has small size, high mechanical resonance frequencies and a small rotational inertia moment. The size of the mirror is 20 mm×15 mm. To achieve fast linear scanning performance, an open-loop controller is designed to compensate the hysteresis behavior and to restrain oscillations that are caused by the mechanical resonances of the scanner's mechanical structure. By comparing measured scanning waveforms, nonlinearities and scan line images between the uncontrolled and controlled scanner, it was found that the scanning linearity of linear scanning was improved The open-loop controlled laser scanner realizes linear scanning at 250 Hz with optical scan angle of ±12 mrad. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Xiang, Sihua and Chen, Sihai and Wu, Xin and Xiao, Ding and Zheng, Xiawei},
doi = {10.1016/j.optlastec.2009.04.019},
file = {:home/miguel/Dropbox/Mendeley Desktop/Xiang et al. - 2010 - Study on fast linear scanning for a new laser scanner.pdf:pdf},
isbn = {0030-3992},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {Fast linear scanning,Laser scanner,Mechanical resonance},
number = {1},
pages = {42--46},
title = {{Study on fast linear scanning for a new laser scanner}},
url = {www.elsevier.com/locate/optlastec},
volume = {42},
year = {2010}
}
@inproceedings{Massot-Campos2016,
abstract = {This article presents a bathymetric SLAM (simultaneous localization and mapping) solution for underwater vehicles by addressing the registration of point clouds gathered from single line laser-based structured light systems. While structured light can be applied to generate millimetre resolution seafloor bathymetry, the accuracy of the maps generated is typically constrained by the localization accuracy of the vehicles used. In this work, relative uncertainties in vehicle localisation are reduced by implementing bathymetric SLAM using temporally constrained submaps. We demonstrate that the method described can overcome misalignments by correcting errors in localisation and can be used to generate self-consistent high-resolution seafloor bathymetric maps.},
author = {Massot-Campos, Miquel and Oliver, Gabriel and Bodenmann, Adrian and Thornton, Blair},
booktitle = {Autonomous Underwater Vehicles 2016, AUV 2016},
doi = {10.1109/AUV.2016.7778669},
file = {:home/miguel/Dropbox/Mendeley Desktop/Massot-Campos et al. - 2016 - Submap bathymetric SLAM using structured light in underwater environments.pdf:pdf},
isbn = {9781509024421},
pages = {181--188},
title = {{Submap bathymetric SLAM using structured light in underwater environments}},
url = {https://eprints.soton.ac.uk/400196/1/Miguel{\_}2016{\_}AUV.pdf},
year = {2016}
}
@article{Bodenmann2017,
abstract = {{\textcopyright} 2016 The Authors. Journal of Field Robotics published by Wiley Periodicals, Inc. Visual maps of the seafloor can provide objective information to characterize benthic ecosystems and survey the distribution of mineral deposits on spatial scales that cannot be otherwise assessed. This paper proposes a three-dimensional mapping method based on light sectioning that enables the simultaneous capture of both structure and color from the images of a single camera. The advantages of the method include high and consistent resolution of the bathymetry, and the simplicity of the setup and the algorithm used to process the data it obtains. The hardware requirements for collecting the data are a single camera, a line laser, and a light, making it possible to deploy the mapping device along with other sensors and devices on underwater platforms such as autonomous underwater vehicles and remotely operated vehicles that can log navigation data. The system has been deployed on a total of 11 cruises, among others, to survey manganese-rich crust deposits on the slopes of Takuyo {\#}5 seamount in the Pacific at depths of more than 2,000 m. In this paper, we present the data that were obtained on one of these cruises.},
author = {Bodenmann, Adrian and Thornton, Blair and Ura, Tamaki},
doi = {10.1002/rob.21682},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bodenmann, Thornton, Ura - 2017 - Generation of High-resolution Three-dimensional Reconstructions of the Seafloor in Color using a Singl.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {aug},
number = {5},
pages = {833--851},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Generation of High-resolution Three-dimensional Reconstructions of the Seafloor in Color using a Single Camera and Structured Light}},
url = {http://doi.wiley.com/10.1002/rob.21682},
volume = {34},
year = {2017}
}
@inproceedings{Menna2013,
abstract = {The article presents an innovative methodology for the 3D surveying and modeling of floating and semi-submerged objects. Photogrammetry is used for surveying both the underwater and emerged parts of the object and the two surveys are combined together by means of special rigid orientation devices. The proposed methodology is firstly applied to a small pleasure boats (approximately 6 meters long) - hence a free floating case - and then to a large shipwreck (almost 300 meters long) interested by a 52 m long leak at the waterline. The article covers the entire workflow, starting from the camera calibration and data acquisition down to the assessment of the achieved accuracy, the realization of the digital 3D model by means of dense image matching procedures as well as deformation analyses and comparison with the craft original plane. {\textcopyright} 2013 SPIE.},
author = {Menna, Fabio and Nocerino, Erica and Troisi, Salvatore and Remondino, Fabio},
booktitle = {Videometrics, Range Imaging, and Applications XII; and Automated Visual Inspection},
doi = {10.1117/12.2020464},
file = {:home/miguel/Dropbox/Mendeley Desktop/Menna et al. - 2013 - A photogrammetric approach to survey floating and semi-submerged objects.pdf:pdf},
keywords = {3D Reconstruction,Accuracy,Photogrammetry,Underwater},
pages = {87910H},
title = {{A photogrammetric approach to survey floating and semi-submerged objects}},
url = {http://3dom.fbk.eu/},
volume = {8791},
year = {2013}
}
@inproceedings{Prats2012,
abstract = {Autonomous grasping of unknown objects by a robot is a highly challenging skill that is receiving increasing attention in the last years. This problem becomes still more chal- lenging (and less explored) in underwater environments, with highly unstructured scenarios, limited availability of sensors and, in general, adverse conditions that affect in different degree the robot perception and control systems. This paper describes an approach for semi-autonomous grasping and recovery on underwater unknown objects from floating vehicles. A laser stripe emitter is attached to a robot forearm that performs a scan of a target of interest. This scan is captured by a camera that also estimates the motion of the floating vehicle while doing the scan. The scanned points are triangulated and transformed according to the motion estimation, thus recovering partial 3D structure of the scene with respect to a fixed frame. A user then indicates the part where to grab the object, and the final grasp is automatically planned on that area. The approach herein presented is tested and validated in water tank conditions},
author = {Prats, Mario and Fernandez, Jos{\'{e}} Javier and Sanz, Pedro J.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385792},
file = {:home/miguel/Dropbox/Mendeley Desktop/Prats, Fernandez, Sanz - 2012 - Combining template tracking and laser peak detection for 3D reconstruction and grasping in underwater en.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
pages = {106--112},
title = {{Combining template tracking and laser peak detection for 3D reconstruction and grasping in underwater environments}},
url = {http://www.irs.uji.es/sites/default/files/publications/combining{\_}template{\_}tracking{\_}and{\_}laser{\_}peak{\_}detection{\_}for{\_}3d{\_}reconstruction{\_}and{\_}grasping{\_}in{\_}underwater{\_}environments.pdf},
year = {2012}
}
@article{Palomer2019,
author = {Palomer, Albert and Ridao, Pere and Forest, Josep and Ribas, David},
doi = {10.1109/TMECH.2019.2929652},
file = {:home/miguel/Dropbox/Mendeley Desktop/Palomer et al. - 2019 - Underwater Laser Scanner Ray-based Model and Calibration.pdf:pdf},
journal = {IEEE/ASME Transactions on Mechatronics},
number = {5},
pages = {1986----1997},
title = {{Underwater Laser Scanner: Ray-based Model and Calibration}},
volume = {24},
year = {2019}
}
@article{Foley2002,
abstract = {New technologies allow archaeologists to explore the human past in the depths of the ocean, far beyond the 50 meter depth boundary set by SCUBA diving. Using robots and advanced sensors originally developed for other applications, social scientists now are following the path of marine scientists, adapting deep submergence technologies for their own research. Remotely Operated Vehicles (ROVs) and Autonomous Underwater Vehicles (AUVs) allow archaeologists to survey the sea floor to depths of 6000 m. This brings 98{\%} of the world's ocean floor within reach, and increases dramatically the number of underwater sites available for archaeological study. Several projects in the past five years in the Mediterranean and Black Seas have proven the scientific merit of archaeology in deep water and trained an international cadre of archaeologists in the new technology. Experience shows it is imperative that work in deep water be collaborative. Projects are particularly fruitful when they bring together as a team technologists familiar with the systems, archaeologists trained in the methods of deep water work, and archaeologists specializing in the period, cultures, and geographical regions pertinent to the shipwrecks. A key lesson is that while technology plays a significant part in this work, it must be combined with the research designs, methodology, and insights of archaeologists to form deep water archaeology into a rigorous scientific practice. Toward this goal, underwater vehicles, precision navigation, and remote sensors designed specifically for archaeology will allow archaeologists to make fundamental discoveries about ancient cultures. Several recent expeditions explored and documented cultural resources in deep water using advanced robotics, remote sensing, and imaging technology. These projects, in which the authors participated, offer compelling reasons to continue examining deep sites. A series of wrecks off Skerki Bank in the central Mediterranean Sea documented a previously unknown open sea trade route between Carthage and Rome. Countering the argument that ancient mariners hugged the coast, the project's scientists discovered and surveyed five wrecks spanning a period from the second century B.C. to the fourth century A.D. at depths averaging 800 meters [Ballard, et al, 2000; McCann 2001;},
author = {Foley, Brendan and Mindell, David},
file = {:home/miguel/Dropbox/Mendeley Desktop/Foley, Mindell - 2002 - Precision Survey and Archaeological Methodology in Deep Water.pdf:pdf},
journal = {ENALIA: The Journal of the Hellenic Institute of Marine Archaeology},
number = {1},
pages = {49----56},
title = {{Precision Survey and Archaeological Methodology in Deep Water}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.544.374{\&}rep=rep1{\&}type=pdf},
volume = {6},
year = {2002}
}
@article{Salome2006,
abstract = {Two-photon scanning microscopy (TPSM) is a powerful tool for imaging deep inside living tissues with sub-cellular resolution. The temporal resolution of TPSM is however strongly limited by the galvanometric mirrors used to steer the laser beam. Fast physiological events can therefore only be followed by scanning repeatedly a single line within the field of view. Because acousto-optic deflectors (AODs) are non-mechanical devices, they allow access at any point within the field of view on a microsecond time scale and are therefore excellent candidates to improve the temporal resolution of TPSM. However, the use of AOD-based scanners with femtosecond pulses raises several technical difficulties. In this paper, we describe an all-digital TPSM setup based on two crossed AODs. It includes in particular an acousto-optic modulator (AOM) placed at 45° with respect to the AODs to pre-compensate for the large spatial distortions of femtosecond pulses occurring in the AODs, in order to optimize the spatial resolution and the fluorescence excitation. Our setup allows recording from freely selectable point-of-interest at high speed (1 kHz). By maximizing the time spent on points of interest, random-access TPSM (RA-TPSM) constitutes a promising method for multiunit recordings with millisecond resolution in biological tissues. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Salom{\'{e}}, R and Kremer, Y and Dieudonn{\'{e}}, S and L{\'{e}}ger, J. F. and Krichevsky, O and Wyart, C and Chatenay, D and Bourdieu, L},
doi = {10.1016/j.jneumeth.2005.12.010},
file = {:home/miguel/Dropbox/Mendeley Desktop/Salom{\'{e}} et al. - 2006 - Ultrafast random-access scanning in two-photon microscopy using acousto-optic deflectors.pdf:pdf},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {Acousto-optical deflectors (AODs),Calcium imaging,Multiunit recording,Random-access two-photon scanning microscopy (RA-T,Spatial pre-compensation,Temporal pre-compensation,Two-photon scanning microscopy (TPSM),Ultrafast scanning},
number = {1-2},
pages = {161--174},
title = {{Ultrafast random-access scanning in two-photon microscopy using acousto-optic deflectors}},
url = {https://pdf.sciencedirectassets.com/271055/1-s2.0-S0165027006X03308/1-s2.0-S0165027005004565/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEBUaCXVzLWVhc3QtMSJIMEYCIQDAMSdMdKEjROlHRtJitgpuG82rJmL9aktUlktqMEh{\%}2BwQIhAJQ9zBwC{\%}2BUsKEPTkcFo{\%}2BXRCJZPxAlmJEzKYOJJ},
volume = {154},
year = {2006}
}
@inproceedings{Duda2016,
abstract = {Geometric distortion associated with underwater flat port housings considerably affects the accuracy of scene depth and visual pose estimation. To compensate for this, a new solution for the forward projection is proposed based on the refractive camera model and Taylor expansion. This allows for an easy integration of the flat refractive camera model into non-linear optimization problems like bundle adjustment.},
author = {Duda, Alexander and Gaudig, Christopher},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759318},
file = {:home/miguel/Dropbox/Mendeley Desktop/Duda, Gaudig - 2016 - Refractive forward projection for underwater flat port cameras.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
month = {oct},
pages = {2022--2027},
publisher = {IEEE},
title = {{Refractive forward projection for underwater flat port cameras}},
url = {http://ieeexplore.ieee.org/document/7759318/},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{Narasimhan2005,
abstract = {Virtually all structured light methods assume that the scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing application in underwater and aerial imaging, where scattering effects cannot be ignored. In this paper, we present a comprehensive analysis of two representative methods - light stripe range scanning and photometric stereo - in the presence of scattering. For both methods, we derive physical models for the appearances of a surface immersed in a scattering medium. Based on these models, we present results on (a) the condition for object detectability in light striping and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while traditional methods fail when scattering is significant, our methods accurately recover the scene (depths, normals, albedos) as well as the properties of the medium. These results are in turn used to restore the appearances of scenes as if they were captured in clear air. Although we have focused on light striping and photometric stereo, our approach can also be extended to other methods such as grid coding, gated and active polarization imaging.},
author = {Narasimhan, Srinivasa G. and Nayar, Shree K. and Sun, Bo and Koppal, Sanjeev J},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2005.232},
file = {:home/miguel/Dropbox/Mendeley Desktop/Narasimhan et al. - 2005 - Structured light in scattering media.pdf:pdf},
isbn = {076952334X},
issn = {1550-5499},
pages = {420--427},
title = {{Structured light in scattering media}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.4564{\&}rep=rep1{\&}type=pdf},
volume = {I},
year = {2005}
}
@inproceedings{Gracias2008,
abstract = {A common problem in video surveys in very shallow waters is the presence of strong light fluctuations, due to sun light refraction. Refracted sunlight casts fast moving patterns, which can significantly degrade the quality of the acquired data. Motivated by the growing need to improve the quality of shallow water imagery, we propose a method to remove sunlight patterns in video sequences. The method exploits the fact that video sequences allow several observations of the same area of the sea floor, over time. It is based on computing the image difference between a given reference frame and the temporal median of a registered set of neighboring images. A key observation is that this difference will have two components with separable spectral content. One is related to the illumination field (lower spatial frequencies) and the other to the registration error (higher frequencies). The illumination field, recovered by lowpass filtering, is used to correct the reference image. In addition to removing the sunflickering patterns, an important advantage of the approach is the ability to preserve the sharpness in corrected image, even in the presence of registration inaccuracies. The effectiveness of the method is illustrated in image sets acquired under strong camera motion containing non-rigid benthic structures. The results testify the good performance and generality of the approach. {\textcopyright} 2008 IEEE.},
author = {Gracias, Nuno and Negahdaripour, Shahriar and Neumann, Laszlo and Prados, Ricard and Garcia, Rafael},
booktitle = {OCEANS 2008},
doi = {10.1109/OCEANS.2008.5152111},
file = {:home/miguel/Dropbox/Mendeley Desktop/Gracias et al. - 2008 - A motion compensated filtering approach to remove sunlight flicker in shallow water images.pdf:pdf},
isbn = {9781424426201},
pages = {1--7},
publisher = {IEEE},
title = {{A motion compensated filtering approach to remove sunlight flicker in shallow water images}},
url = {http://ieeexplore.ieee.org/document/5152111/},
year = {2008}
}
@inproceedings{Giguere2009,
abstract = {We describe a navigation and coverage system based on unsupervised learning driven by visual input. Our objective is to allow a robot to remain continuously moving above a terrain of interest using visual feedback to avoid leaving this region. As a particular application domain, we are interested in doing this in open water, but the approach makes few domain-specific assumptions. Specifically, our system employed an unsupervised learning technique to train a k-Nearest Neighbor classifier to distinguish between images of different terrain types through image segmentation. A simple random exploration strategy was used with this classifier to allow the robot to collect data while remaining confined above a coral reef, without the need to maintain pose estimates. We tested the technique in simulation, and a live deployment was conducted in open water. During the latter, the robot successfully navigated autonomously above a coral reef during a 20 minutes period.},
author = {Giguere, Philippe and Dudek, Gregory and Prahacs, Christopher and Plamondon, Nicolas and Turgeon, Katrine},
booktitle = {Proceedings of the 2009 Canadian Conference on Computer and Robot Vision, CRV 2009},
doi = {10.1109/CRV.2009.41},
file = {:home/miguel/Dropbox/Mendeley Desktop/Giguere et al. - 2009 - Unsupervised learning of terrain appearance for automated coral reef exploration.pdf:pdf},
isbn = {9780769536514},
month = {may},
pages = {268--275},
publisher = {IEEE},
title = {{Unsupervised learning of terrain appearance for automated coral reef exploration}},
url = {http://ieeexplore.ieee.org/document/5230508/},
year = {2009}
}
@inproceedings{Thielemann2012,
abstract = {This paper presents a real-time contour tracking and object segmentation algorithm for 3D range images. The algorithm is used to control a novel micro-mirror based imaging laser scanner, which provides a dynamic trade-off between resolution and frame rate. The micro-mirrors are controllable, enabling us to speed up acquisition significantly by only sampling on the object that is tracked and of interest. As the hardware is under development, we benchmark our algorithms on data from a SICK LMS100-10000 laser scanner mounted on a tilting platform. We find that objects are tracked and segmented well on pixel-level; that frame rate/resolution can be increased 3-4 times through our approach compared to scanners having static scan trajectories, and that the algorithm runs in 30 ms/image on a Intel Core i7 CPU using a single core.},
author = {Thielemann, Jens T. and Berge, Asbj{\o}rn and Skotheim, {\O}ystein and Kirkhus, Trine},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385501},
file = {:home/miguel/Dropbox/Mendeley Desktop/Thielemann et al. - 2012 - Fast high resolution 3D laser scanning by real-time object tracking and segmentation.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {3899--3906},
publisher = {IEEE},
title = {{Fast high resolution 3D laser scanning by real-time object tracking and segmentation}},
url = {http://ieeexplore.ieee.org/document/6385501/},
year = {2012}
}
@inproceedings{Kyo1995,
abstract = {Japan Marine Science and Technology center (JAMSTEC) developed a full ocean depth research ROV (remotely operated vehicle), "KAIKO", whose construction started in January, 1991, and whose sea trial was carried out from May, 1993 to March, 1995. The sea trial had three separated phases, that is, the first test at 1000 m deep ocean near the main land of Japan, the second test at Nansei Shoto Trench of 6500 m deep, and the final test at Mariana Trench in order to confirm the capability of the system at the deepest ocean bottom on Earth. The original plan for the final sea trial had been scheduled in July, 1993, however, it was delayed by some troubles. The main trouble was on the data transmission through a secondary cable between a launcher and a vehicle of "KAIKO". After clearing up all causes of the troubles, the authors carried out the sea trial again at Mariana Trench in March, 1995, and succeeded in the deepest challenge. This paper described the feature of "KAIKO" and the result of the sea trial.},
author = {Kyo, M. and Hiyazaki, E. and Tsukioka, S. and Ochi, H. and Amitani, Y. and Tsuchiya, T. and Aoki, T. and Takagawa, S.},
booktitle = {'Challenges of Our Changing Global Environment'. Conference Proceedings. OCEANS '95 MTS/IEEE},
doi = {10.1109/oceans.1995.528882},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kyo et al. - 1995 - The sea trial of KAIKO, the full ocean depth research ROV.pdf:pdf},
isbn = {0-933957-14-9},
pages = {1991--1996},
publisher = {IEEE},
title = {{The sea trial of "KAIKO", the full ocean depth research ROV}},
url = {http://ieeexplore.ieee.org/document/528882/},
volume = {3},
year = {1995}
}
@techreport{Seitz1999,
author = {Seitz, Steven},
file = {:home/miguel/Dropbox/Mendeley Desktop/Seitz - 1999 - An overview of passive vision techniques.ps:ps},
institution = {The Robotics Institute, Carnegie Mellon University},
title = {{An overview of passive vision techniques}},
year = {1999}
}
@inproceedings{Rauscher2016,
abstract = {3D Sensors are used for many different applications, e.g. scene reconstruction, object detection and mobile robots, etc. Several studies on usability and accuracy have been done for different sensors. However, all these studies have used different settings for the different sensors. For this reason we compare five 3D sensors, including the struc-tured light sensors Microsoft Kinect and ASUS Xtion Pro Live and the time of flight sensors Fotonic E70P, IFM O3D200 and Nippon Signal FX6, using the same settings. The sensor noise, absolute error and point detection rates are compared for different depth values, environmental illumination and different surfaces. Also simple models of the noise de-pending on the measured depth are proposed. It is found that the struc-tured light sensors are very accurate for close ranges. The time of flight sensors have more noise, but the noise does not increase as strongly with the measured distance. Further, it is found that these sensors can be used for outdoor applications.},
author = {Rauscher, Gerald and Dube, Daniel and Zell, Andreas},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-08338-4_3},
file = {:home/miguel/Dropbox/Mendeley Desktop/Rauscher, Dube, Zell - 2016 - A comparison of 3D sensors for wheeled mobile robots.pdf:pdf},
isbn = {9783319083377},
issn = {21945357},
pages = {29--41},
title = {{A comparison of 3D sensors for wheeled mobile robots}},
url = {http://www.cogsys.cs.uni-tuebingen.de/publikationen/2014/rauscher2014ias.pdf},
volume = {302},
year = {2016}
}
@article{Fournier1993,
abstract = {A careful analysis of a scattering and absorption database of the waters off the coasts of Canada shows that a laser-assisted camera system will have a significantly improved viewing performance over conventional systems. The laser underwater camera image enhancer system is a range-gated laser system that can be mounted on a remotely operated vehicle. The system uses a 2-kHz diode-pumped frequency-doubled Nd:YAG laser as an illumination source. The light is collected by a 10-cm-diam zoom lens. The detector is a gated image intensifier with a 7-ns gate and a gain that is continuously variable from 500 to 1,000,000. The system has been tested in a water tank facility at Defence Research Establishment Valcartier and has been mounted on the HYSUB 5000 remotely operated vehicle for sea trials. In the strongly scattering waters typical of harbor approaches, this system has a range of from three to five times that of a conventional camera with floodlights.},
author = {Fournier, Georges R. and Bonnier, Deni and Forand, J. Luc and Pace, Paul W.},
doi = {10.1117/12.143954},
isbn = {0091-3286},
issn = {00913286},
journal = {Optical Engineering},
keywords = {imaging,lasers,range gating},
number = {9},
pages = {2185--2190},
publisher = {International Society for Optics and Photonics},
title = {{Range-gated underwater laser imaging system}},
url = {http://opticalengineering.spiedigitallibrary.org/article.aspx?doi=10.1117/12.143954},
volume = {32},
year = {1993}
}
@inproceedings{Schechner2004,
abstract = {Underwater imaging is important for scientific research and technology, as well as for popular activities. We present a computer vision approach which easily removes degrada-tion effects in underwater vision. We analyze the physical ef-fects of visibility degradation. We show that the main degra-dation effects can be associated with partial polarization of light. We therefore present an algorithm which inverts the image formation process, to recover a good visibility image of the object. The algorithm is based on a couple of images taken through a polarizer at different orientations. As a by product, a distance map of the scene is derived as well. We successfully used our approach when experimenting in the sea using a system we built. We obtained great improvement of scene contrast and color correction, and nearly doubled the underwater visibility range.},
author = {Schechner, Yoav Y and Karpel, N.},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
doi = {10.1109/cvpr.2004.1315078},
file = {:home/miguel/Dropbox/Mendeley Desktop/Schechner, Karpel - 2004 - Clear underwater vision.pdf:pdf},
isbn = {0-7695-2158-4},
pages = {536--543},
publisher = {IEEE},
title = {{Clear underwater vision}},
url = {http://ieeexplore.ieee.org/document/1315078/},
volume = {1},
year = {2004}
}
@article{Dalgleish2006,
author = {Dalgleish, Fraser R.},
file = {:home/miguel/Dropbox/Mendeley Desktop/Dalgleish - 2006 - Model-Based Evaluation of Pulsed Lasers for an Underwater Laser Line Scan Imager.pdf:pdf},
journal = {Business},
pages = {1--7},
title = {{Model-Based Evaluation of Pulsed Lasers for an Underwater Laser Line Scan Imager}},
url = {https://fau.digital.flvc.org/islandora/object/fau{\%}3A6377/datastream/OBJ/view/Model-based{\_}evaluation{\_}of{\_}pulsed{\_}lasers{\_}for{\_}an{\_}underwater{\_}laser{\_}line{\_}scan{\_}imager.pdf},
year = {2006}
}
@inproceedings{Breivik2011,
abstract = {We present an implementation of a novel foveating 3D sensor concept, inspired by the human eye, which intends to allow future robots to better interact with their surroundings. The sensor is based on a time-of-flight laser scanning technology, where each range distance measurement is performed individually for increased quality. Micro-mirrors enable detailed control on where and when each sample point is acquired in the scene. By finding regions-of-interest (ROIs) and mainly concentrating the data acquisition here, the spatial resolution or frame rate of these ROIs can be significantly increased compared to a non-foveating system. Foveation is enabled through a real-time implementation of a feed-back control loop for the sensor hardware, based on vision algorithms for 3D scene analysis. In this paper, we describe and apply an algorithm for detecting ROIs based on motion detection in range data using background modeling. Heuristics are incorporated to cope with camera motion. We report first results applying this algorithm to scenes with moving objects, and show that the foveation capability allows the frame rate to be increased by up to 8.2 compared to a non-foveating sensor, utilizing up to 99{\%} of the potential frame rate increase. The incorporated heuristics significantly improves the foveation's performance for moving camera scenes.},
author = {Breivik, Goril M. and Thielemann, Jens T. and Berge, Asbj{\o}rn and Skotheim, {\O}ystein and Kirkhus, Trine},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2011.5981797},
file = {:home/miguel/Dropbox/Mendeley Desktop/Breivik et al. - 2011 - A motion based real-time foveation control loop for rapid and relevant 3D laser scanning.pdf:pdf},
isbn = {9781457705298},
issn = {21607508},
month = {jun},
pages = {28--35},
publisher = {IEEE},
title = {{A motion based real-time foveation control loop for rapid and relevant 3D laser scanning}},
url = {http://ieeexplore.ieee.org/document/5981797/},
year = {2011}
}
@techreport{TexasInstruments2008,
abstract = {1 The Challenge Historically, the primary application of digital micromirror device (DMD) technology has been in display systems, although in the last five years many new applications are being explored by DLP embedded customers. Many of these applications contemplate the use of lasers with DMDs. Laser applications use continuous and pulsed mode operation. One of the many advantages of pulsed operation is that during the pulse very high peak powers can be reached with relatively low average power consumption. This mode of operation enables various ablation modes (thermal and non-thermal) for deposition, medical and other applications. (1) For lamp (broadband), LED, and continuous laser illumination, steady-state thermal models can be used to predict the temperature of the array and pixels, given the ambient temperature and the optical power spectrum or laser wavelength of the illumination light. With such models, it is fairly straightforward to determine the limit on the optical illumination power. The limit is determined by the maximum allowable array/pixel temperature. Therefore, for a given set of environmental conditions and optical spectrum, the power limit can be calculated. These limits form in part the basis of the maximum illumination power density specification on DMD data sheets. However, when considering pulsed laser illumination, the transient temperature of the pixels can no longer be ignored. Large temperature differentials and high temperatures present significant challenges for semiconductor devices that can affect device lifetime. Therefore, it is desirable to keep pixel surface temperature below a critical temperature of 150°C. The effect is often cumulative, so that even if the pixels reach these temperatures for very short periods of time, over many cycles of operation damage may be evidenced. Therefore, a more robust pseudo-transient model is needed to predict the peak transient temperatures of DMD pixels in pulsed laser systems. With such a model, it then becomes possible to determine a limit on pulsed laser power based on duty cycle, repetition rate (pulse frequency), wavelength, and peak laser power in conjunction with the environmental conditions that a DMD is operating in. The challenge then is to develop a model that predicts the temperature of a DMD pixel for a given illumination power, wavelength, frequency, and duty cycle (or pulse power/energy, duration, and repetition rate). 2 The Model The purpose of the model is to predict the peak momentary temperature that the pixels reach during pulsed laser operation. Regardless of the average areal input density, the temperature of individual pixels must remain below 150°C. Above this temperature, it is possible for thermo-chemistry to occur. The following assumptions are made to simplify the model: 1. The reflectivity of the pixels follows the reflectivity curve of bulk aluminum within the useful range of the device (400 nm-2500 nm). 2. The repetition rate is high enough ({\textgreater} 100 Hz) and the thermal mass of the underlying substrate is large enough ({\textgreater}{\textgreater} thermal mass of the pixel) so that the temperature of the substrate (array) is just the temperature that the array would reach with continuous illumination with the same average power. (1) Dr. R{\"{u}}diger Paschotta, Pulsed Laser Deposition,},
author = {{Texas Instruments}},
file = {:home/miguel/Dropbox/Mendeley Desktop/Texas Instruments - 2008 - Laser Power Handling for DMDs.pdf:pdf},
title = {{Laser Power Handling for DMDs}},
url = {http://www.rp-photonics.com/pulsed{\_}laser{\_}deposition.html},
year = {2008}
}
@inproceedings{Jaffe2005,
abstract = {In this article a performance bound is derived for feasible image resolution among a class of imaging systems that can be referred to as synchronous laser line scan systems. Most often, these systems use a narrow beam projected source (typically a laser) in conjunction with a very small field of view receiver that is synchronously scanned. Here, a bound on the maximum system resolution is derived when both source and receiver are “delta function like”. The bound demonstrates that the best achievable overall system point spread function is the square of the one way point spread function for the medium.},
author = {Jaffe, Jules S.},
booktitle = {Oceans 2005 - Europe},
doi = {10.1109/OCEANSE.2005.1511744},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jaffe - 2005 - Performance bounds on synchronous laser line scan systems.pdf:pdf},
isbn = {0780391039},
issn = {1094-4087},
pages = {379--383},
pmid = {19494934},
publisher = {Academic Press},
title = {{Performance bounds on synchronous laser line scan systems}},
url = {https://www.osapublishing.org/DirectPDFAccess/F9471ED6-988C-ED5B-A027B1BFDFEC4E9E{\_}82536/oe-13-3-738.pdf?da=1{\&}id=82536{\&}seq=0{\&}mobile=no},
volume = {1},
year = {2005}
}
@incollection{Garcia2017,
address = {Chichester, UK},
author = {Garc{\'{i}}a, Rafael and Gracias, Nuno and Nicosevici, Tudor and Prados, Ricard and Hurt{\'{o}}s, Natalia and Campos, Ricard and Escartin, Javier and Elibol, Armagan and Hegedus, Ramon and Neumann, Laszlo},
booktitle = {Computer Vision in Vehicle Technology},
doi = {10.1002/9781118868065.ch4},
file = {:home/miguel/Dropbox/Mendeley Desktop/Garc{\'{i}}a et al. - 2017 - Exploring the Seafloor with Underwater Robots.pdf:pdf},
keywords = {autonomous underwater vehicles,light scattering,ocean floor,remotely operated vehicles,single image dehazing,underwater vehicles,unmanned underwater vehicles,vision‐based navigation},
month = {feb},
pages = {75--99},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Exploring the Seafloor with Underwater Robots}},
url = {http://doi.wiley.com/10.1002/9781118868065.ch4},
year = {2017}
}
@article{Schops2017,
abstract = {This paper presents an approach for reconstructing large-scale outdoor scenes through monocular motion stereo at interactive frame rates on a modern mobile device (Google Project Tango Development Kit Tablet). The device's fisheye camera enables a user to reconstruct large scenes in only a few minutes by simply walking through the scene. We utilize the device's GPU to compute depth maps via plane sweep stereo. In contrast to reconstructing small objects, we observe that in large-scale scenarios using motion stereo, free-space measurements are less effective for suppressing outliers due to limited possibilities for camera placement and an unbounded reconstruction volume. Furthermore, the outlier ratio in depth maps from stereo matching is much higher compared to images from depth sensors. Consequently, we propose a set of filtering steps to detect and discard unreliable depth measurements. The remaining parts of the depth maps are then integrated into a volumetric representation of the scene using a truncated signed distance function. Ours is the first method to enable live reconstruction of large outdoor scenes on a mobile device. We extensively evaluate our approach, demonstrating the benefit of rigorously filtering depth maps.},
author = {Sch{\"{o}}ps, Thomas and Sattler, Torsten and H{\"{a}}ne, Christian and Pollefeys, Marc},
doi = {10.1016/j.cviu.2016.09.007},
file = {:home/miguel/Dropbox/Mendeley Desktop/Sch{\"{o}}ps et al. - 2017 - Large-scale outdoor 3D reconstruction on a mobile device.pdf:pdf},
isbn = {10773142},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Dense 3D reconstruction,Mobile 3D reconstruction,Plane sweep stereo,Video-based stereo},
month = {apr},
pages = {151--166},
title = {{Large-scale outdoor 3D reconstruction on a mobile device}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314216301412},
volume = {157},
year = {2017}
}
@article{Maccarone2015,
abstract = {{\textcopyright}2015 Optical Society of America. A depth imaging system, based on the time-of-flight approach and the time-correlated single-photon counting (TCSPC) technique, was investigated for use in highly scattering underwater environments. The system comprised a pulsed supercontinuum laser source, a monostatic scanning transceiver, with a silicon single-photon avalanche diode (SPAD) used for detection of the returned optical signal. Depth images were acquired in the laboratory at stand-off distances of up to 8 attenuation lengths, using per-pixel acquisition times in the range 0.5 to 100 ms, at average optical powers in the range 0.8 nW to 950 $\mu$W. In parallel, a LiDAR model was developed and validated using experimental data. The model can be used to estimate the performance of the system under a variety of scattering conditions and system parameters.},
author = {Maccarone, Aurora and McCarthy, Aongus and Ren, Ximing and Warburton, Ryan E and Wallace, Andy M and Moffat, James and Petillot, Yvan and Buller, Gerald S},
doi = {10.1364/oe.23.033911},
file = {:home/miguel/Dropbox/Mendeley Desktop/Maccarone et al. - 2015 - Underwater depth imaging using time-correlated single-photon counting.pdf:pdf},
journal = {Optics Express},
keywords = {(0104450) Oceanic optics,(0305260) Photon counting,(1100113) Imaging through turbid media,(2803640) Lidar,OCIS codes: (1106880) Three-dimensional image acqu},
number = {26},
pages = {33911},
title = {{Underwater depth imaging using time-correlated single-photon counting}},
url = {http://arxiv.org/abs/1507.02511},
volume = {23},
year = {2015}
}
@inproceedings{Massot-Campos2014,
abstract = {A Laser-based Structured Light System (LbSLS) has been designed to perform underwater close-range 3D reconstructions even with high turbidity conditions and outperform conventional systems. The system uses a camera and a 532 nm green laser projector. The optical technique used is based on the projection of a pattern obtained placing a Diffractive Optical Element (DOE) in front of the laser beam. In the experiments described in this manuscript, the DOE used diffracts the laser beam in 25 parallel lines providing enough information in a single camera frame to perform a 3D reconstruction.},
annote = {Albert: "Although this technique is useful for online 3D reconstruction, it has the limitation that the pattern cannot be changed online. Therefore, the resolution decreases as the distance to the scene increases."},
author = {Massot-Campos, Miquel and Oliver-Codina, Gabriel},
booktitle = {Proceedings of IEEE Sensors},
doi = {10.1109/ICSENS.2014.6985208},
file = {:home/miguel/Dropbox/Mendeley Desktop/Massot-Campos, Oliver-Codina - 2014 - Underwater laser-based structured light system for one-shot 3D reconstruction.pdf:pdf},
isbn = {978-1-4799-0162-3},
issn = {21689229},
month = {nov},
number = {December},
pages = {1138--1141},
publisher = {IEEE},
title = {{Underwater laser-based structured light system for one-shot 3D reconstruction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6985208},
volume = {2014-Decem},
year = {2014}
}
@inproceedings{Ye2017,
author = {Ye, Liangchen and Zhang, Gaofei and You, Zhen and Zhang, Chi},
booktitle = {Proceedings of IEEE Sensors},
doi = {10.1109/ICSENS.2016.7808932},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ye et al. - 2017 - A 2D resonant MEMS scanner with an ultra-compact wedge-like multiplied angle amplification for miniature LIDAR applic.pdf:pdf},
isbn = {9781479982875},
issn = {21689229},
keywords = {LIDAR,MEMS scanner,micromirror,multiplied amplification,optical angle amplification},
month = {oct},
pages = {1--3},
publisher = {IEEE},
title = {{A 2D resonant MEMS scanner with an ultra-compact wedge-like multiplied angle amplification for miniature LIDAR application}},
url = {http://ieeexplore.ieee.org/document/7808932/},
year = {2017}
}
@article{Moore1976,
author = {Moore, E. J.},
doi = {10.1111/j.1477-9730.1976.tb00852.x},
file = {:home/miguel/Dropbox/Mendeley Desktop/Moore - 1976 - Underwater photogrammetry.pdf:pdf},
issn = {14779730},
journal = {The Photogrammetric Record},
month = {aug},
number = {48},
pages = {748--763},
title = {{Underwater photogrammetry}},
url = {http://doi.wiley.com/10.1111/j.1477-9730.1976.tb00852.x},
volume = {8},
year = {1976}
}
@inproceedings{Prats2012a,
abstract = {Autonomous grasping of unknown objects by a robot is a highly challenging skill that is receiving increasing attention in the last years. This problem becomes still more challenging (and less explored) in underwater environments, with highly unstructured scenarios, limited availability of sensors and, in general, adverse conditions that affect in different degree the robot perception and control systems. This paper describes an approach for semi-autonomous grasping and recovery on underwater unknown objects. A laser stripe emitter is attached to a robot forearm that performs a scan of a target of interest. This scan is captured by a camera and the partial 3D structure of the scene is recovered. A user then indicates the part where to grab the object, and the final grasp is automatically planned on that area. The methods herein presented are tested and validated in real conditions in a water tank.},
author = {Prats, Mario and Fernandez, Jos{\'{e}} Javier and Sanz, Pedro J.},
booktitle = {Proceedings of the International Conference on Optimisation of Electrical and Electronic Equipment, OPTIM},
doi = {10.1109/OPTIM.2012.6231874},
file = {:home/miguel/Dropbox/Mendeley Desktop/Prats, Fernandez, Sanz - 2012 - An approach for semi-autonomous recovery of unknown objects in underwater environments.pdf:pdf},
isbn = {978-1-4673-1653-8},
issn = {18420133},
month = {may},
pages = {1452--1457},
publisher = {IEEE},
title = {{An approach for semi-autonomous recovery of unknown objects in underwater environments}},
url = {http://ieeexplore.ieee.org/document/6231874/},
year = {2012}
}
@incollection{Giancola2018,
abstract = {"This book is a valuable resource to deeply understand the technology used in 3D cameras. In this book, the authors summarize and compare the specifications of the main 3D cameras available in the mass market. The authors present a deep metrological analysis of the main camera based on the three main technologies: Time-of-Flight, Structured-Light and Active Stereoscopy, and provide qualitative results for any user to understand the underlying technology within 3D camera, as well as practical guidance on how to get the most of them for a given application."--},
address = {Cham},
author = {Giancola, Silvio and Valenti, Matteo and Sala, Remo},
booktitle = {SpringerBriefs in Computer Science},
doi = {10.1007/978-3-319-91761-0},
file = {:home/miguel/Dropbox/Mendeley Desktop/Giancola, Valenti, Sala - 2018 - A survey on 3D cameras Metrological comparison of time-of-flight, structured-light and active stereosco.pdf:pdf},
isbn = {978-3-319-91760-3},
issn = {21915776},
pages = {89--90},
publisher = {Springer International Publishing},
series = {SpringerBriefs in Computer Science},
title = {{A survey on 3D cameras: Metrological comparison of time-of-flight, structured-light and active stereoscopy technologies}},
url = {http://link.springer.com/10.1007/978-3-319-91761-0},
year = {2018}
}
@inproceedings{Droeschel,
abstract = {Modern 3D laser-range scanners have a high data rate, making online simultaneous localization and mapping (SLAM) computationally challenging. Recursive state estimation techniques are efficient but commit to a state estimate immediately after a new scan is made, which may lead to misalignments of measurements. We present a 3D SLAM approach that allows for refining alignments during online mapping. Our method is based on efficient local mapping and a hierarchical optimization back-end. Measurements of a 3D laser scanner are aggregated in local multiresolution maps by means of surfel-based registration. The local maps are used in a multi-level graph for allocentric mapping and localization. In order to incorporate corrections when refining the alignment, the individual 3D scans in the local map are modeled as a sub-graph and graph optimization is performed to account for drift and misalignments in the local maps. Furthermore, in each sub-graph, a continuous-time representation of the sensor trajectory allows to correct measurements between scan poses. We evaluate our approach in multiple experiments by showing qualitative results. Furthermore, we quantify the map quality by an entropy-based measure.},
archivePrefix = {arXiv},
arxivId = {1810.06802},
author = {Droeschel, David and Behnke, Sven},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8461000},
eprint = {1810.06802},
file = {:home/miguel/Dropbox/Mendeley Desktop/Droeschel, Behnke - 2018 - Efficient continuous-time SLAM for 3D lidar-based online mapping.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
pages = {5000--5007},
title = {{Efficient continuous-time SLAM for 3D lidar-based online mapping}},
year = {2018}
}
@inproceedings{Digumarti2016,
abstract = {This paper presents underwater 3D capture using a commercial depth camera. Previous underwater capture systems use ordinary cameras, and it is well-known that a calibration procedure is needed to handle refraction. The same is true for a depth camera being used underwater. We describe a calibration method that corrects the depth maps of refraction effects. Another challenge is that depth cameras use infrared light (IR) which is heavily attenuated in water. We demonstrate scanning is possible with commercial depth cameras for ranges up to 20 cm in water. The motivation for using a depth camera under water is the same as in air - it provides dense depth data and higher quality 3D reconstruction than multi-view stereo. Underwater 3D capture is being increasingly used in marine biology and oceanology; our approach offers exciting prospects for such applications. To the best of our knowledge, ours is the first approach that successfully demonstrates underwater 3D capture using low cost depth cameras like Intel RealSense. We describe a complete system, including protective housing for the depth camera which is suitable for handheld use by a diver. Our main contribution is an easy-to-use calibration method, which we evaluate on exemplar data as well as 3D reconstructions in a lab aquarium. We also present initial results of ocean deployment.},
author = {Digumarti, Sundara Tejaswi and Chaurasia, Gaurav and Taneja, Aparna and Siegwart, Roland and Thomas, Amber and Beardsley, Paul},
booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
doi = {10.1109/WACV.2016.7477644},
file = {:home/miguel/Dropbox/Mendeley Desktop/Digumarti et al. - 2016 - Underwater 3D capture using a low-cost commercial depth camera.pdf:pdf},
isbn = {9781509006410},
month = {mar},
pages = {1--9},
publisher = {IEEE},
title = {{Underwater 3D capture using a low-cost commercial depth camera}},
url = {http://ieeexplore.ieee.org/document/7477644/},
year = {2016}
}
@inproceedings{Martins2019,
author = {Martins, Alfredo and Almeida, Jose and Almeida, Carlos and Dias, Andre and Dias, Nuno and Aaltonen, Jussi and Heininen, Arttu and Koskinen, Kari T. and Rossi, Claudio and Dominguez, Sergio and Voros, Csaba and Henley, Stephen and McLoughlin, Mike and van Moerkerk, Hilco and Tweedie, James and Bodo, Balazs and Zajzon, Norbert and Silva, Eduardo},
booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/iros.2018.8593999},
file = {:home/miguel/Dropbox/Mendeley Desktop/Martins et al. - 2019 - UX 1 system design - A robotic system for underwater mining exploration.pdf:pdf},
isbn = {978-1-5386-8094-0},
month = {oct},
pages = {1494--1500},
publisher = {IEEE},
title = {{UX 1 system design - A robotic system for underwater mining exploration}},
url = {https://ieeexplore.ieee.org/document/8593999/},
year = {2019}
}
@article{Massot-Campos2015,
abstract = {This paper presents a survey on optical sensors and methods for 3D reconstruction in underwater environments. The techniques to obtain range data have been listed and explained, together with the different sensor hardware that makes them possible. The literature has been reviewed, and a classification has been proposed for the existing solutions. New developments, commercial solutions and previous reviews in this topic have also been gathered and considered.},
author = {Massot-Campos, Miquel and Oliver-Codina, Gabriel},
doi = {10.3390/s151229864},
file = {:home/miguel/Dropbox/Mendeley Desktop/Massot-Campos, Oliver-Codina - 2015 - Optical sensors and methods for underwater 3D reconstruction.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {3D reconstruction,Laser stripe,LiDAR,Stereo vision,Structure from motion,Structured light,Underwater robotics},
month = {dec},
number = {12},
pages = {31525--31557},
pmid = {26694389},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Optical sensors and methods for underwater 3D reconstruction}},
url = {http://www.mdpi.com/1424-8220/15/12/29864},
volume = {15},
year = {2015}
}
@techreport{Austin1976,
author = {Austin, Roswell W . and Halikas, George},
file = {:home/miguel/Dropbox/Mendeley Desktop/Austin, Halikas - 1976 - The index of refraction of seawater.pdf:pdf},
institution = {Visibility Laboratory, University of California},
title = {{The index of refraction of seawater}},
year = {1976}
}
@phdthesis{PenalverMonfort2018,
address = {Castell{\'{o}} de la Plana},
author = {{Pe{\~{n}}alver Monfort}, Antonio},
doi = {10.6035/14101.2018.192020},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pe{\~{n}}alver Monfort - 2018 - Framework for Autonomous Underwater Grasping of Unknown Objects.pdf:pdf},
month = {nov},
school = {Universitat Jaume I},
title = {{Framework for Autonomous Underwater Grasping of Unknown Objects}},
url = {http://hdl.handle.net/10803/664743},
year = {2018}
}
@phdthesis{ForestCollado2005,
author = {{Forest Collado}, Josep},
file = {:home/miguel/Dropbox/Mendeley Desktop/Forest Collado - 2005 - New methods for triangulation-based shape acquisition using laser scanners.pdf:pdf},
isbn = {84-689-3091-1},
school = {Universitat de Girona},
title = {{New methods for triangulation-based shape acquisition using laser scanners}},
url = {https://dugi-doc.udg.edu/bitstream/handle/10256/4930/Tjfc.pdf?sequence=5},
year = {2005}
}
@techreport{JCGM2008,
author = {JCGM},
file = {:home/miguel/Dropbox/Mendeley Desktop/JCGM - 2008 - International vocabulary of metrology--Basic and general concepts and associated terms (VIM).pdf:pdf},
institution = {Joint Committee for Guides in Metrology},
keywords = {VIM 3,international vocabulary of metrology,vocabulaire international de m{\'{e}}trologie},
title = {{International vocabulary of metrology--Basic and general concepts and associated terms (VIM)}},
url = {https://www.bipm.org/utils/common/documents/jcgm/JCGM{\_}200{\_}2008.pdf},
year = {2008}
}
@book{Hecht2017,
abstract = {For these Global Editions, the editorial team at Pearson has collaborated with educators across the world to address a wide range of subjects and requirements, equipping students with the best possible learning tools. This Global Edition preserves the cutting-edge approach and pedagogy of the original, but also features alterations, customization, and adaptation from the North American version.},
author = {Hecht, Eugene},
edition = {5th},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hecht - 2017 - Optics.pdf:pdf},
isbn = {978-1-292-09693-3},
publisher = {Pearson Education Limited},
title = {{Optics}},
url = {www.pearsonglobaleditions.com},
year = {2017}
}
@inproceedings{Mullen2009,
abstract = {A system based on the time-varying intensity (TVI) approach was built in the early 1970's at the Scripps Visibility Laboratory and experimental data collected by this prototype system showed an imaging capability of between 15 and 20 attenuation lengths at 640 nm. Researchers at the Naval Air Systems Command have developed an updated version of this original system with state-of-the-art components. This new TVI system uses a modulated laser illuminator to convey information about the scan to the distant receiver instead of using a separate optical trigger as was used in the original system. Laboratory water tank experiments were conducted with a prototype modulated TVI system to evaluate the effect of system and environmental variables on the system performance. In parallel with the experiments, an interactive computer simulation was developed to help evaluate the effect of the many variables on the TVI performance. Results from experiments and simulations will be discussed and compared.},
author = {Mullen, L. and Laux, A. and Cochenour, B. and McBride, W.},
booktitle = {MTS/IEEE Biloxi - Marine Technology for Our Future: Global and Local Challenges, OCEANS 2009},
file = {:home/miguel/Dropbox/Mendeley Desktop/Mullen et al. - 2009 - Extended range underwater imaging using a Time Varying Intensity (TVI) approach.pdf:pdf},
isbn = {9781424449606},
title = {{Extended range underwater imaging using a Time Varying Intensity (TVI) approach}},
year = {2009}
}
@article{Melo2017,
abstract = {The autonomy of robotic underwater vehicles is dependent on the ability to perform long-term and long-range missions without need of human intervention. While current state-of-the-art underwater navigation techniques are able to provide sufficient levels of precision in positioning, they require the use of support vessels or acoustic beacons. This can pose limitations on the size of the survey area, but also on the whole cost of the operations. Terrain Based Navigation is a sensor-based navigation technique that bounds the error growth of dead-reckoning using a map with terrain information, provided that there is enough terrain variability. An obvious advantage of Terrain Based Navigation is the fact that no external aiding signals or devices are required. Because of this unique feature, terrain navigation has the potential to dramatically improve the autonomy of Autonomous Underwater Vehicles (AUVs). This paper consists on a comprehensive survey on the recent developments for Terrain Based Navigation methods proposed for AUVs. The survey includes a brief introduction to the original Terrain Based Navigation formulations, as well as a description of the algorithms, and a list of the different implementation alternatives found in the literature. Additionally, and due to the relevance, Bathymetric SLAM techniques will also be discussed.},
author = {Melo, Jos{\'{e}} and Matos, An{\'{i}}bal},
doi = {10.1016/j.oceaneng.2017.04.047},
file = {:home/miguel/Dropbox/Mendeley Desktop/Melo, Matos - 2017 - Survey on advances on terrain based navigation for autonomous underwater vehicles.pdf:pdf},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {AUV autonomy,AUV navigation,Terrain based navigation for AUVs},
month = {jul},
pages = {250--264},
pmid = {18039937},
title = {{Survey on advances on terrain based navigation for autonomous underwater vehicles}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S002980181730241X},
volume = {139},
year = {2017}
}
@article{Schiebener1990,
author = {Schiebener, P and Straub, Johannes and {Levelt Sengers}, JMH and Gallagher, JS},
file = {:home/miguel/Dropbox/Mendeley Desktop/Schiebener et al. - 1990 - Refractive index of water and steam as function of wavelength, temperature and density.pdf:pdf},
journal = {Journal of physical and chemical reference data},
number = {3},
pages = {677----717},
title = {{Refractive index of water and steam as function of wavelength, temperature and density}},
url = {https://www.td.mw.tum.de/fileadmin/w00bso/www/Forschung/Publikationen{\_}Straub/59.pdf},
volume = {19},
year = {1990}
}
@article{Perry2004,
author = {Perry, T.S.},
doi = {10.1109/MSPEC.2004.1279192},
file = {:home/miguel/Dropbox/Mendeley Desktop/Perry - 2004 - Tomorrow's TV.pdf:pdf},
issn = {0018-9235},
journal = {IEEE Spectrum},
month = {apr},
number = {4},
pages = {38--41},
title = {{Tomorrow's TV}},
url = {http://ieeexplore.ieee.org/document/1279192/},
volume = {41},
year = {2004}
}
@article{Møller2013,
abstract = {Various 3-D sensors with highly varying properties exist. Comparing these sensors has traditionally been a cumber- some task, involving scanners to be set up and tested at the same place. In this article, a portable test plate, which can be used to test 3-D scanners, is described. Using this portable test plate, eight different scanners have been tested. These are compared both qualitatively and quantitatively.},
author = {M{\o}ller, Bent and Balslev, Ivar and Kr{\"{u}}ger, Norbert},
doi = {10.1109/JSEN.2012.2228638},
file = {:home/miguel/Dropbox/Mendeley Desktop/M{\o}ller, Balslev, Kr{\"{u}}ger - 2013 - An automatic evaluation procedure for 3-D scanners in robotics applications.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sensors Journal},
keywords = {3-D measurement devices,Benchmark testing,Bin picking,Robot vision systems},
month = {feb},
number = {2},
pages = {870--878},
title = {{An automatic evaluation procedure for 3-D scanners in robotics applications}},
url = {http://ieeexplore.ieee.org/document/6359738/},
volume = {13},
year = {2013}
}
@article{Yamafune2016,
abstract = {{\textcopyright} 2016 Springer Science+Business Media New York Methods to record shipwreck sites have evolved considerably in the past two decades. Digital technology and marine robotics regularly present faster and more precise ways to excavate, clean, tag, and record ship remains, while computers simplify many of the steps involved in the reconstruction of ships from their archaeological remains. At the same time, the internet is creating opportunities to share primary data in real time and on a wide scale. This paper presents a methodology used by the authors to record and reconstruct the wooden structures of a 19th-century shipwreck in southern Brazil (Lagoa do Peixe site) and of a 16th-century shipwreck in Croatia (the Gnali{\'{c}} shipwreck).},
author = {Yamafune, K and Torres, R. and Castro, F},
doi = {10.1007/s10816-016-9283-1},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yamafune, Torres, Castro - 2017 - Multi-Image Photogrammetry to Record and Reconstruct Underwater Shipwreck Sites.pdf:pdf},
isbn = {1081601692831},
issn = {15737764},
journal = {Journal of Archaeological Method and Theory},
keywords = {Maritime archaeology,Ship reconstruction,Shipwrecks,Single-image photogrammetry,Underwater recording},
number = {3},
pages = {703--725},
title = {{Multi-Image Photogrammetry to Record and Reconstruct Underwater Shipwreck Sites}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10816-016-9283-1.pdf},
volume = {24},
year = {2017}
}
@inproceedings{Yamamoto2007,
abstract = {The multi-pixel photon counter (MPPC) is a newly developed photodetector with an excellent photon counting capability. It also has many attractive features such as small size, high gain, low operation voltage and power consumption, and capability of operating in magnetic fields and in room temperature. The basic performance of samples has been measured. A gain of {\~{}}10{\^{}}6 is achieved with a noise rate less than 1 MHz with 1 p.e. threshold, and cross-talk probability of less than 30{\%} at room temperature. The photon detection efficiency for green light is twice or more that of the photomultiplier tubes. It is found that the basic performance of the MPPC is satisfactory for use in real experiments.},
author = {Yamamoto, K. and Yamamura, K. and Sato, K. and Kamakura, S. and Ota, T. and Suzuki, H. and Ohsuka, S.},
booktitle = {IEEE Nuclear Science Symposium Conference Record},
doi = {10.1109/NSSMIC.2007.4437286},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yamamoto et al. - 2007 - Development of Multi-Pixel Photon Counter (MPPC).pdf:pdf},
isbn = {1424409233},
issn = {10957863},
pages = {1511--1515},
publisher = {IEEE},
title = {{Development of Multi-Pixel Photon Counter (MPPC)}},
url = {http://ieeexplore.ieee.org/document/4179192/},
volume = {2},
year = {2007}
}
@inproceedings{Davis2011,
abstract = {An emergent electro-optic technology platform, liquid crystal (LC) waveguides, will be presented with a focus on performance attributes that may be relevant to coded aperture approaches. As a low cost and low SWaP alternative to more traditional approaches (e.g. galvos, MEMs, traditional EO techniques, etc.), LC-Waveguides provide a new technique for switching, phase shifting, steering, focusing, and generally controlling light. LC-waveguides provide tremendous continuous voltage control over optical phase delays ({\textgreater} 2mm demonstrated), with very low loss ({\textless} 0.5 dB/cm) and rapid response time. The electro-evanescent architecture exploits the tremendous electro-optic response of liquid crystals (can be {\textgreater} one million pm/Volts) while circumventing their historic limitations; speeds can be in the microseconds and LC scattering losses can be reduced by orders of magnitude from conventional LC optics. This enables a new class of photonic devices: very wide analog non-mechanical beamsteerers (270° demonstrated), chip-scale widely tunable lasers (50 nm demonstrated), chip-scale Fourier transform spectrometers ({\textless} 5 nm resolution demonstrated), widely tunable micro-ring resonators, tunable lenses (fl tuning from 5 mm to infinity demonstrated), ultra-low power ({\textless} 5 microWatts) optical switches, true optical time delay devices (12 nsecs demonstrated) for phased array antennas, and many more. Both the limitations and the opportunity provided by this technology for use in coded aperture schemes will be discussed.},
author = {Davis, Scott R. and Rommel, Scott D. and Farca, George and Luey, Benjamin and Rebolledo, Neil and Selwyn, Stephanie and Anderson, Michael H.},
booktitle = {Unconventional Imaging, Wavefront Sensing, and Adaptive Coded Aperture Imaging and Non-Imaging Sensor Systems},
doi = {10.1117/12.895958},
editor = {Dolne, Jean J. and Karr, Thomas J. and Gamiz, Victor L. and Rogers, Stanley and Casasent, David P.},
keywords = {EO Beamsteering,EO replacement for mechanics,EO scanner,Electro-evanescent,LC waveguide,Laser Scanner,Liquid Crystal Waveguides,liquid crystal},
month = {sep},
pages = {81651E},
publisher = {International Society for Optics and Photonics},
title = {{A new photonics technology platform and its applicability for coded aperture techniques}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.895958},
volume = {8165},
year = {2011}
}
@article{Johnson-Roberson2010,
abstract = {Robust, scalable simultaneous localization and mapping (SLAM) algorithms support the successful deployment of robots in real-world applications. In many cases these platforms deliver vast amounts of sensor data from large-scale, unstructured environments. These data may be difficult to interpret by end users without further processing and suitable visualization tools. We present a robust, automated system for large-scale three-dimensional (3D) reconstruction and visualization that takes stereo imagery from an autonomous underwater vehicle (AUV) and SLAM-based vehicle poses to deliver detailed 3D models of the seafloor in the form of textured polygonal meshes. Our system must cope with thousands of images, lighting conditions that create visual seams when texturing, and possible inconsistencies between stereo meshes arising from errors in calibration, triangulation, and navigation. Our approach breaks down the problem into manageable stages by first estimating local structure and then combining these estimates to recover a composite georeferenced structure using SLAM-based vehicle pose estimates. A texture-mapped surface at multiple scales is then generated tha t is interactively presented to the user through a visualization engine. We adapt established solutions when possible, with an emphasis on quickly delivering approximate yet visually consistent reconstructions on standard computing hardware. This allows scientists on a research cruise to use our system to design follow-up deployments of the AUV and complementary instruments. To date, this system has been tested on several research cruises in Australian waters and has been used to reliably generate and visualize reconstructions for more than 60 dives covering diverse habitats and representing hundreds of linear kilometers of survey. {\textcopyright} 2009 Wiley Periodicals, Inc.},
author = {Johnson-Roberson, Matthew and Pizarro, Oscar and Williams, Stefan B. and Mahon, Ian},
doi = {10.1002/rob.20324},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {1},
pages = {21--51},
title = {{Generation and visualization of large-scale three-dimensional reconstructions from underwater robotic surveys}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20324},
volume = {27},
year = {2010}
}
@article{Widder2007,
abstract = {It is an oft-stated statistic that 95{\%} of the ocean is unexplored, but this number does not expose the full extent of our ignorance about the largest eco- system on the planet—first because it refers to the ocean floor, not the almost unimaginably vast volume of water above it, and second because limited tools have been used to probe what little of the ocean has been explored. For cen- turies, nets were the primary means of exploration, but these move slowly com- pared to the swimming speeds of large, agile predators, and they destroy delicate fauna such as gelatinous zooplankton. Exploration with submersibles and remotely operated vehicles (ROVs) has opened new vistas, revealing the remark- able abundance and incredible adapta- tions of previously unknown fragile fauna, as well as equally diverse and frag- ile benthic communities associated with hydrothermal vents, hydrocarbon seeps (Fisher et al., this issue), and deep-water corals (Baco et al., this issue; Ross et al., this issue). However, such platforms use loud thrusters and bright white lights that are disruptive to organisms adapted to life in the dim, peaceful depths.},
author = {Widder, Edith},
doi = {10.5670/oceanog.2007.04},
isbn = {1042-8275},
issn = {1042-8275},
journal = {Oceanography},
number = {4},
pages = {46--51},
publisher = {Oceanography Society},
title = {{Sly Eye for the Shy Guy: Peering into the Depths with New Sensors}},
url = {https://www.jstor.org/stable/24860139 http://tos.org/oceanography/article/sly-eye-for-the-shy-guy-peering-into-the-depths-with-new-sensors},
volume = {20},
year = {2007}
}
@article{Luczynski2017,
abstract = {The calibration and refraction correction process for underwater cameras with flat-pane interfaces is presented that is very easy and convenient to use in real world applications while yielding very accurate results. The correction is derived from an analysis of the axial camera model for underwater cameras, which is among others computationally hard to tackle. It is shown how realistic constraints on the distance of the camera to the window can be exploited, which leads to an approach dubbed Pinax Model as it combines aspects of a virtual pinhole model with the projection function from the axial camera model. It allows the pre-computation of a lookup-table for very fast refraction correction of the flat-pane with high accuracy. The model takes the refraction indices of water into account, especially with respect to salinity, and it is therefore sufficient to calibrate the underwater camera only once in air. It is demonstrated by real world experiments with several underwater cameras in different salt and sweet water conditions that the proposed process outperforms standard methods. Among others, it is shown how the presented method leads to accurate results with single in-air calibration and even with just estimated salinity values.},
author = {{\L}uczy{\'{n}}ski, Tomasz and Pfingsthorn, Max and Birk, Andreas},
doi = {10.1016/j.oceaneng.2017.01.029},
file = {:home/miguel/Dropbox/Mendeley Desktop/{\L}uczy{\'{n}}ski, Pfingsthorn, Birk - 2017 - The Pinax-model for accurate and efficient refraction correction of underwater cameras in flat-p.pdf:pdf},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {Axial camera,Calibration,Camera model,Rectification,Refraction correction,Underwater vision},
month = {mar},
pages = {9--22},
title = {{The Pinax-model for accurate and efficient refraction correction of underwater cameras in flat-pane housings}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0029801817300434},
volume = {133},
year = {2017}
}
@article{Kwon1999,
abstract = {The purpose of this study was twofold: (a) to investigate the effects of selected experimental factors on the magnitude of the object plane deformation due to refraction, and (b) to discuss their practical implications in an effort to improve the applicability of the 2-D DLT method in the underwater motion analysis. The RMS and maximum object plans reconstruction errors of various experimental conditions were computed systematically. To isolate the error due to refraction from the experimental errors, the comparator coordinates (image plane coordinates) of the control points were computed based on a theoretical refraction model rather than actual digitizing. It was concluded from a series of object plane reconstruction that among the distance and angle factors of the experimental setting in the 2-D underwater motion analysis, the camera-to- interface distance and the interface-to-control-object distance are the two major factors affecting the magnitude of the abject plane deformation. The other factors revealed only minor effects. The advantages of the 2-D DLT method over the traditional multiplier method in underwater motion analysis, such as oblique projection and multiple camera setup, were further discussed. Possible ways to reduce the maximum reconstruction error were also explored.},
author = {Kwon, Young Hoo},
doi = {10.1123/jab.15.4.396},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kwon - 1999 - Object plane deformation due to refraction in two-dimensional underwater motion analysis.pdf:pdf},
issn = {10658483},
journal = {Journal of Applied Biomechanics},
keywords = {Light refraction,Object plane reconstruction,Two-dimensional DLT,Underwater motion analysis},
month = {nov},
number = {4},
pages = {396--403},
title = {{Object plane deformation due to refraction in two-dimensional underwater motion analysis}},
url = {http://journals.humankinetics.com/doi/10.1123/jab.15.4.396},
volume = {15},
year = {1999}
}
@inproceedings{Nuchtera,
abstract = {This paper provides a new solution to the simultaneous localization and mapping (SLAM) problem with six degrees of freedom. A fast variant of the Iterative Closest Points (ICP) algorithm registers 3D scans taken by a mobile robot into a common coordinate system and thus provides relocalization. Hereby, data association is reduced to the problem of searching for closest points. Approximation algorithms for this searching, namely, approximate kd-trees and box decomposition trees, are presented and evaluated in this paper. A solution to 6D SLAM that considers all free parameters in the robot pose is built based on 3D scan matching. {\textcopyright} 2005 IEEE.},
author = {N{\"{u}}chter, Andreas and Lingemann, Kai and Hertzberg, Joachim and Surmann, Hartmut},
booktitle = {2005 International Conference on Advanced Robotics, ICAR '05, Proceedings},
doi = {10.1109/ICAR.2005.1507419},
file = {:home/miguel/Dropbox/Mendeley Desktop/N{\"{u}}chter et al. - 2005 - 6D SLAM with approximate data association.pdf:pdf},
isbn = {0780391772},
pages = {242--249},
title = {{6D SLAM with approximate data association}},
url = {http://www.ais.fraunhofer.de/ARC/kurt3D/index.html},
volume = {2005},
year = {2005}
}
@article{Zhu2016,
abstract = {srep , (2016). doi:10.1038/srep33143},
author = {Zhu, Wenbin and Chao, Ju-hung and Chen, Chang Jiang and Yin, Shizhuo and Hoffman, Robert C},
doi = {10.1038/srep33143},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zhu et al. - 2016 - Three order increase in scanning speed of space charge-controlled KTN deflector by eliminating electric field induce.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
title = {{Three order increase in scanning speed of space charge-controlled KTN deflector by eliminating electric field induced phase transition in nanodisordered KTN}},
url = {www.nature.com/scientificreports},
volume = {6},
year = {2016}
}
@article{Yoo2018,
abstract = {Lidar, the acronym of light detection and ranging, has received much attention for the automotive industry as a key component for high level automated driving systems due to their high resolution and highly accurate 3D imaging of the surroundings under various weather conditions. However, the price and resolution of lidar sensors still do not meet the target values for the automotive market to be accepted as a basic sensor for ensuring safe autonomous driving. Recent work has focused on MEMS scanning mirrors as a potential solution for affordable long range lidar systems. This paper discusses current developments and research on MEMS-based lidars. The LiDcAR project is introduced for bringing precise and reliable MEMS-based lidars to enable safe and reliable autonomous driving. As a part of development in this project, a test bench for the characterization and performance evaluation of MEMS mirror is introduced. A recently developed MEMS-based lidar will be evaluated by various levels of tests including field tests based on realistic scenarios, aiming for safe and reliable autonomous driving in future automotive industry.},
author = {Yoo, Han Woong and Druml, Norbert and Brunner, David and Schwarzl, Christian and Thurner, Thomas and Hennecke, Marcus and Schitter, Georg},
doi = {10.1007/s00502-018-0635-2},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yoo et al. - 2018 - MEMS-based lidar for autonomous driving.pdf:pdf},
isbn = {0050201806352},
issn = {0932383X},
journal = {Elektrotechnik und Informationstechnik},
keywords = {MEMS scanning mirror,autonomous driving,lidar,metrology platform},
number = {6},
pages = {408--415},
title = {{MEMS-based lidar for autonomous driving}},
url = {https://doi.org/10.1007/s00502-018-0635-2},
volume = {135},
year = {2018}
}
@article{Donati2014,
abstract = {Solid-state photomultipliers (SSPM) made by multielement SPAD (single-photon avalanche detector) are nowadays jousted to replace the traditional, vacuum-tube photomultipliers (PMT) based on the photocathode and dynode-chain technology. We revisit the milestones and the conceptual steps leading to single-photon detectors, from the PMT to the SSPM. Then, we discuss state-of-the-art performances of the two detectors and point out that SSPMs are equalling or even surpassing PMTs for response time and sensitivity, while are still lagging for acceptance area, linearity and dark current. We finally compare the detectors in applications to pulse spectrometry, fast waveform analysis, and photon counting.},
author = {Donati, Silvano and Tambosso, Tiziana},
doi = {10.1109/JSTQE.2014.2350836},
file = {:home/miguel/Dropbox/Mendeley Desktop/Donati, Tambosso - 2014 - Single-Photon Detectors From Traditional PMT to Solid-State SPAD-Based Technology.pdf:pdf},
issn = {1077260X},
journal = {IEEE Journal on Selected Topics in Quantum Electronics},
keywords = {Photodetection,avalanche photodiodes,photomultipliers (PMT),semiconductors,single photon detectors},
month = {nov},
number = {6},
pages = {204--211},
title = {{Single-Photon Detectors: From Traditional PMT to Solid-State SPAD-Based Technology}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6882126},
volume = {20},
year = {2014}
}
@inproceedings{Murez2015,
abstract = {Dysregulation of a genomic imprinting gene can contribute to carcinogenesis. Here, delta-like 1 homolog (Drosophila) (DLK1), a paternally expressed gene, was found to be significantly up-regulated in 60 (73.2{\%}) of a total of 82 hepatocellular carcinoma (HCC) specimens using reverse transcription-polymerase chain reaction. In addition, immunohistochemistry staining was performed in another 88 HCC specimens, of which 50 (56.8{\%}) cancerous tissues were considered as positive. The expression of DLK1 was obviously induced in HCC cells, Bel-7402 and MHCC-H, by a demethylation agent, 5-aza-2'-deoxycytidine. Furthermore, both demethylation of the DLK1 promoter (-565 to -362) and hypermethylation of the imprinting control domain in the region upstream of maternally expressed gene 3 were identified in a few HCC specimens. This implies that deregulation of genomic DNA methylation of the imprinted domain could be attributed to the up-regulation of DLK1 in HCC, although the undoubtedly complex mechanisms involved in the epigenetic event should be further investigated in HCC. Surprisingly, the expression of DLK1 in HCC was confirmed to be monoallelic specific, not biallelic, in three HCC specimens with a single nucleotide polymorphism as at T852C (rs2295660). Importantly, the exogenous DLK1 can significantly promote the cell proliferation of SMMC-7721 cells, a HCC cell line, whereas the suppression of endogenetic DLK1 through RNA interference can markedly inhibit cell growth, colony formation and tumorigenicity of HepG2, Hep3B and HuH-7 cells. These data suggest that DLK1 as an imprinted gene could be significantly up-regulated in HCC due to certain epigenetic events and contribute to the oncogenesis of this tumor.},
author = {Murez, Zak and Treibitz, Tali and Ramamoorthi, Ravi and Kriegman, David J.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/TPAMI.2016.2613862},
file = {:home/miguel/Dropbox/Mendeley Desktop/Murez et al. - 2015 - Photometric Stereo in a Scattering Medium.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {01628828},
keywords = {Photometric stereo,fluorescence,scattering medium},
pages = {3415----3423},
title = {{Photometric Stereo in a Scattering Medium}},
url = {https://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/papers/Murez{\_}Photometric{\_}Stereo{\_}in{\_}ICCV{\_}2015{\_}paper.pdf},
year = {2015}
}
@article{Ji1989,
abstract = {This paper presents the derivation of the precise relation between the displacement of a light spot on an object's surface and the displacement of its image on the detector in an optical triangulation device, along with applications of the design of triangulation devices. Based on this relation, improved designs of optical triangulation devices, including devices of adjustable configurations, are proposed and discussed. {\textcopyright} 1989.},
author = {Ji, Z. and Leu, M. C.},
doi = {10.1016/0030-3992(89)90068-6},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ji, Leu - 1989 - Design of optical triangulation devices.pdf:pdf},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {displacement measurement,optical devices,optical triangulation,optics},
month = {oct},
number = {5},
pages = {339--341},
publisher = {Elsevier},
title = {{Design of optical triangulation devices}},
url = {https://www.sciencedirect.com/science/article/pii/0030399289900686},
volume = {21},
year = {1989}
}
@book{Zanuttigh2016,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2016. This book provides a comprehensive overview of the key technologies and applications related to new cameras that have brought 3D data acquisition to the mass market. It covers both the theoretical principles behind the acquisition devices and the practical implementation aspects of the computer vision algorithms needed for the various applications. Real data examples are used in order to show the performances of the various algorithms. The performance and limitations of the depth camera technology are explored, along with an extensive review of the most effective methods for addressing challenges in common applications. Applications covered in specific detail include scene segmentation, 3D scene reconstruction, human pose estimation and tracking and gesture recognition. This book offers students, practitioners and researchers the tools necessary to explore the potential uses of depth data in light of the expanding number of devices available for sale. It explores the impact of these devices on the rapidly growing field of depth-based computer vision.},
address = {Cham},
author = {Zanuttigh, Pietro and Mutto, Carlo Dal and Minto, Ludovico and Marin, Giulio and Dominio, Fabio and Cortelazzo, Guido Maria},
booktitle = {Time-of-Flight and Structured Light Depth Cameras: Technology and Applications},
doi = {10.1007/978-3-319-30973-6},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zanuttigh et al. - 2016 - Time-of-flight and structured light depth cameras Technology and applications.pdf:pdf},
isbn = {9783319309736},
pages = {1--355},
publisher = {Springer International Publishing},
title = {{Time-of-flight and structured light depth cameras: Technology and applications}},
url = {http://link.springer.com/10.1007/978-3-319-30973-6},
year = {2016}
}
@inproceedings{Hou2007a,
abstract = {Electrowetting beam steering using prism refraction and mirror reflection is presented. Electrowetting at the sidewalls of a square liquid channel is able to tilt prism or mirror apex angles over +/- 55°. {\textcopyright}2007 IEEE.},
author = {Hou, Linlin and Smith, Neil R. and Heikenfeld, Jason},
booktitle = {Conference Proceedings - Lasers and Electro-Optics Society Annual Meeting-LEOS},
doi = {10.1109/LEOS.2007.4382476},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hou, Smith, Heikenfeld - 2007 - Electrowetting micro-prisms and micro-mirrors.pdf:pdf},
isbn = {142440925X},
issn = {10928081},
month = {oct},
pages = {457--458},
publisher = {IEEE},
title = {{Electrowetting micro-prisms and micro-mirrors}},
url = {http://ieeexplore.ieee.org/document/4382476/},
year = {2007}
}
@incollection{Smith1990,
abstract = {In this paper, we describe a representation for spatial information, called the stochastic map, and associated procedures for building it, reading information from it, and revising it incrementally as new information is obtained. The map contains the estimates of relationships among objects in the map, and their un- certainties, given all the available information. The procedures provide a general solution to the problem of estimating uncertain relative spatial relationships. The estimates are probabilistic in nature, an advance over the previous, very conservative, worst-case approaches to the problem. Finally, the procedures are developed in the context of state-estimation and filtering theory, which provides a solid basis for numerous extensions.},
author = {Smith, Randall and Self, Matthew and Cheeseman, Peter},
booktitle = {Autonomous robot vehicles},
doi = {10.1109/robot.1987.1087846},
file = {:home/miguel/Dropbox/Mendeley Desktop/Smith, Self, Cheeseman - 1990 - Estimating uncertain spatial relationships in robotics.pdf:pdf},
pages = {167----193},
publisher = {Springer},
title = {{Estimating uncertain spatial relationships in robotics}},
url = {https://www.researchgate.net/publication/221405213},
year = {1990}
}
@article{Pierzchaa2018,
abstract = {Enabling automated 3D mapping in forests is an important component of the future development of forest technology, and has been garnering interest in the scientific community, as can be seen from the many recent publications. Accordingly, the authors of the present paper propose the use of a Simultaneous Localisation and Mapping algorithm, called graph-SLAM, to generate local maps of forests. In their study, the 3D data required for the mapping process were collected using a custom-made, mobile platform equipped with a number of sensors, including Velodyne VLP-16 LiDAR, a stereo camera, an IMU, and a GPS. The 3D map was generated solely from laser scans, first by relying on laser odometry and then by improving it with robust graph optimisation after loop closures, which is the core of the graph-SLAM algorithm. The resulting map, in the form of a 3D point cloud, was then evaluated in terms of its accuracy and precision. Specifically, the accuracy of the fitted diameter at breast height (DBH) and the relative distance between the trees were evaluated. The results show that the DBH estimates using the Pratt circle fit method could enable a mean estimation error of approximately 2 cm (7–12{\%}) and an RMSE of 2.38 cm (9{\%}), whereas for tree positioning accuracy, the mean error was 0.0476 m. The authors conclude that robust SLAM algorithms can support the development of forestry by providing cost-effective and acceptable quality methods for forest mapping. Moreover, such maps open up the possibility for precision localisation for forestry vehicles.},
author = {Pierzcha{\l}a, Marek and Gigu{\`{e}}re, Philippe and Astrup, Rasmus},
doi = {10.1016/j.compag.2017.12.034},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pierzcha{\l}a, Gigu{\`{e}}re, Astrup - 2018 - Mapping forests using an unmanned ground vehicle with 3D LiDAR and graph-SLAM.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {DBH,Forest operations,LiDAR,Precision forestry,SLAM},
month = {feb},
pages = {217--225},
publisher = {Elsevier B.V.},
title = {{Mapping forests using an unmanned ground vehicle with 3D LiDAR and graph-SLAM}},
volume = {145},
year = {2018}
}
@article{Moore2000,
abstract = {The design, construction, and performance of a new high-resolution underwater bathymetric prototype system (L-Bath) with extended imaging capability is presented. The design offers simultaneous reflectance and depth information on a pixel-by-pixel basis so that high-resolution reflectance and bathymetric maps of underwater targets can be provided with exact registration. The design supports operation in shallow coastal waters under daylight conditions where high turbidity and the influence of ambient backscatter are particularly limiting for underwater imaging systems. Its configuration is similar to existing laser line scanning systems but uses a pulsed laser for the source and a fixed field-of-view high-resolution linear charge-coupled device (CCD) as receiver. The pulsed laser allows short camera integration times, thereby reducing the influence of the ambient daylight signal, and the fixed field of view of the detector provides a precision nonmoving multielement receiver with imaging capability. As the laser sweeps across the field of view of the CCD, the position and signal strength of each laser target spot is imaged, permitting a measure of bathymetry and reflectance. Using the CCD, a highresolution slice through the reflected target spot radiance distribution is imaged so that system resolution can exceed the target spot size. The image of the target spot radiance distribution, modified by in-water scattering and target reflectance, provides new opportunities for image manipulation compared to typical underwater laser line scanning based systems. The simultaneous acquisition of reflectance and bathymetric maps permits discrimination capability between real objects of relief from scene reflectance variations.},
author = {Moore, Karl D and Jaffe, Jules S. and Ochoa, Benjamin L},
doi = {10.1175/1520-0426(2000)017<1106:DOANUB>2.0.CO;2},
file = {:home/miguel/Dropbox/Mendeley Desktop/Moore, Jaffe, Ochoa - 2000 - Development of a new underwater bathymetric laser imaging system L-Bath.pdf:pdf},
isbn = {9780230624993},
issn = {07390572},
journal = {Journal of Atmospheric and Oceanic Technology},
pages = {1106--1117},
title = {{Development of a new underwater bathymetric laser imaging system: L-Bath}},
url = {https://journals.ametsoc.org/doi/pdf/10.1175/1520-0426(2000)017{\%}3C1106{\%}3ADOANUB{\%}3E2.0.CO{\%}3B2},
volume = {17},
year = {2000}
}
@inproceedings{Bodenmann2012,
abstract = {This paper introduces a mapping device capable of recording seafloor photos from altitudes of up to 8m, which can be used to build colour 3D reconstructions. Data recorded at sea are shown and the results after processing with two different methods for 3D reconstruction are introduced and compared.},
author = {Bodenmann, Adrian and Thornton, Blair and Hara, Seiichi and Hioki, Kazuyuki and Kojima, Mitsuhiro and Ura, Tamaki and Kawato, Masaru and Fujiwara, Yoshihiro},
booktitle = {OCEANS 2012 MTS/IEEE: Harnessing the Power of the Ocean},
doi = {10.1109/OCEANS.2012.6405002},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bodenmann et al. - 2012 - Development of 8m long range imaging technology for generation of wide area colour 3D seafloor reconstructions.pdf:pdf},
isbn = {9781467308298},
month = {oct},
pages = {1----4},
publisher = {IEEE},
title = {{Development of 8m long range imaging technology for generation of wide area colour 3D seafloor reconstructions}},
url = {http://ieeexplore.ieee.org/document/6405002/},
year = {2012}
}
@inproceedings{Hou2007,
abstract = {The presented effort is aimed at establishing a framework in order to restore underwater imagery to the best possible level, working with both simulated and field measured data. Under this framework, the traditional image restoration approach is extended by incorporating underwater optical properties into the system response function, specifically the point spread function (PSF) in spatial domain and modulation transfer function (MTF) in frequency domain. Due to the intensity variations involved in underwater sensing, denoising is carefully carried out by wavelet decompositions. This is necessary to explore different effects of restoration constrains, and especially their response to underwater environment where the effects of scattering can be easily treated as either signal or noise. The images are then restored using measured or modeled PSFs. An objective image quality metric, tuned with environmental optical properties, is designed to gauge the effectiveness of the restoration, and serves to check the optimization approach. This metric utilizes previous wavelet decompositions to constrain the sharpness metric based on grayscale slopes at the edge, weighted by the ratio of the power of high frequency components of the image to the total power of the image. Modeled PSFs, based on Wells' small angle approximations, are compared to those derived from Monte Carlo simulation using measured scattering properties. Initial results are presented, including estimation of water optical properties from the imagery-derived MTFs, and optimization outputs applying automated restoration framework.},
author = {Hou, Weilin and Gray, Deric J. and Weidemann, Alan D and Fournier, Georges R. and Forand, J. Luc},
booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
doi = {10.1109/IGARSS.2007.4423193},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hou et al. - 2007 - Automated underwater image restoration and retrieval of related optical properties.pdf:pdf},
isbn = {1424412129},
keywords = {Image restoration,Modulation transfer function,NIRDD,Ocean optics,Point spread function,Scattering},
pages = {1889--1892},
title = {{Automated underwater image restoration and retrieval of related optical properties}},
url = {https://apps.dtic.mil/dtic/tr/fulltext/u2/a474300.pdf},
year = {2007}
}
@inproceedings{Milanovic2011,
abstract = {We demonstrate a compact, low-power device which combines a laser source, a MEMS mirror, and photo-sensors to enable fast-motion tracking of an object in a 3D volume while obtaining its precise XYZ coordinates, as well as high resolution laser-based imaging. Any object can be tracked which is marked by retro-reflective tape, or a corner-cube retroreflector (CCR). Two separate subsystems which we termed “MEMSEyes” track the position of the object within a 40° field-of-view cone, allowing triangulation of the object's distance. A demonstration system running in a 40 kHz control loop is capable of human hand motion tracking and provides position information at up to 5m distance with 13-bit precision and repeatability. In another demonstration, a longer object is marked by two CCRs at its ends and the system measures its orientation in space with 0.1° precision by locating both ends (CCRs) in a time-multiplexed manner. When used in rastering mode, a single MEMSEye device can efficiently scan objects and 2D barcodes.},
author = {Milanovi{\'{c}}, V. and Kasturi, A. and Siu, N. and Radoji{\v{c}}i{\'{c}}, M. and Su, Y.},
booktitle = {2011 16th International Solid-State Sensors, Actuators and Microsystems Conference, TRANSDUCERS'11},
doi = {10.1109/TRANSDUCERS.2011.5969770},
file = {:home/miguel/Dropbox/Mendeley Desktop/Milanovi{\'{c}} et al. - 2011 - MEMSEye for optical 3D tracking and imaging applications.pdf:pdf},
isbn = {9781457701573},
keywords = {3D tracking,MEMS mirror,barcode scanner,corner-cube retroreflector,laser imaging,laser tracking,micromirror,orientation sensor},
month = {jun},
pages = {1895--1898},
publisher = {IEEE},
title = {{MEMSEye for optical 3D tracking and imaging applications}},
url = {http://ieeexplore.ieee.org/document/5969770/},
year = {2011}
}
@article{Hofmann2013,
abstract = {Low-cost automotive laser scanners for environmental perception are needed to enable the integration of advanced driver assistant systems into all automotive vehicle segments, which is a key to reduce the number of traffic accidents on roads. Within the scope of the European-funded project MiniFaros, partners from five different countries have been cooperating in developing a small-sized low-cost time-of-flight-based range sensor. An omnidirectional 360-deg laser scanning concept has been developed based on the combination of an omnidirectional lens and a biaxial large aperture MEMS mirror. The concept, design, fabrication, and first measurement results of a resonant biaxial 7-mm gimbal-less MEMS mirror that is electrostatically actuated by stacked vertical comb drives is described. Identical resonant frequencies of the two orthogonal axes are necessary to enable the required circle scanning capability. A tripod suspension was chosen, since it minimizes the frequency splitting of the two resonant axes. Low-mirror curvature is achieved by a thickness of the mirror of more than 500 pm. Hermetic wafer-level vacuum packaging of such large mirrors based on multiple wafer bonding has been developed to enable a large mechanical tilt angle of +/- 6.5 deg in each axis. Due to the large targeted tilt angle of +/- 15 deg and because of the MEMS mirror actuator having a diameter of 10 mm, a cavity depth of about 1.6 mm has been realized. (C) 2014 Society of Photo-Optical Instrumentation Engineers (SPIE)},
author = {Hofmann, Ulrich and Senger, Frank and Soerensen, Frerk and Stenchly, Vanessa and Jensen, Bjoern and Janes, Joachim},
doi = {10.1117/1.JMM.13.1.011103},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hofmann et al. - 2012 - Resonant biaxial 7-mm MEMS mirror for Automotive LIDAR application.pdf:pdf},
isbn = {9780819493859},
issn = {1932-5150},
journal = {2012 International Conference on Optical MEMS and Nanophotonics},
keywords = {lidar,microelectromechanical systems,mirrors,optical devices,scanners,scanning},
month = {dec},
number = {1},
pages = {150----151},
publisher = {International Society for Optics and Photonics},
title = {{Resonant biaxial 7-mm MEMS mirror for Automotive LIDAR application}},
url = {http://nanolithography.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMM.13.1.011103},
volume = {13},
year = {2012}
}
@inproceedings{Kim2016,
abstract = {We propose a method which can perform real-time 3D recon- struction from a single hand-held event camera with no additional sens- ing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene in- verse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity im- age, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.},
author = {Kim, Hanme and Leutenegger, Stefan and Davison, Andrew J},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46466-4_21},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kim, Leutenegger, Davison - 2016 - Real-time 3D reconstruction and 6-DoF tracking with an event camera.pdf:pdf},
isbn = {9783319464657},
issn = {16113349},
keywords = {3D reconstruction,6-DoF tracking,Event-based camera,Intensity reconstruction,SLAM,Visual odometry},
pages = {349--364},
title = {{Real-time 3D reconstruction and 6-DoF tracking with an event camera}},
url = {https://youtu.be/LauQ6LWTkxM?t=35s},
volume = {9910 LNCS},
year = {2016}
}
@article{Fang2018,
abstract = {This paper presents a real-time and low-cost 3D perception and reconstruction system which is suitable for autonomous navigation and large-scale environment reconstruction. The 3D mapping system is based on a rotating 2D planar laser scanner driven by a step motor, which is suitable for continuous mapping. However, for such a continuous mapping system, the challenge is that the range measurements are received at different times when the 3D LiDAR is moving, which will result in big distortion of the local 3D point cloud. As a result, the errors in motion estimation can cause misregistration of the resulting point cloud. In order to continuously estimate the trajectory of the sensor, we first extract feature points from the local point cloud and then estimate the transformation between current frame to local map to get the LiDAR odometry. After that, we use the estimated motion to remove the distortion of the local point cloud and then register the undistorted local point cloud to the global point cloud to get accurate global map. Finally, we propose a coarse-to-fine graph optimization method to minimize the global drift. The proposed 3D sensor system is advantageous due to its mechanical simplicity, mobility, low weight, low cost, and real-time estimation. To validate the performance of the proposed system, we carried out several experiments to verify its accuracy, robustness, and efficiency. The experimental results show that our system can accurately estimate the trajectory of the sensor and build a quality 3D point cloud map simultaneously.},
author = {Fang, Zheng and Zhao, Shibo and Wen, Shiguang and Zhang, Yu},
doi = {10.1155/2018/2937694},
file = {:home/miguel/Dropbox/Mendeley Desktop/Fang et al. - 2018 - A Real-Time 3D Perception and Reconstruction System Based on a 2D Laser Scanner.pdf:pdf},
isbn = {9781538604892},
issn = {1687-725X},
journal = {Journal of Sensors},
month = {may},
pages = {1--14},
publisher = {Hindawi},
title = {{A Real-Time 3D Perception and Reconstruction System Based on a 2D Laser Scanner}},
url = {https://www.hindawi.com/journals/js/2018/2937694/},
volume = {2018},
year = {2018}
}
@incollection{Brown2015,
address = {Berlin, Heidelberg},
author = {Brown, Margaret and Urey, Hakan},
booktitle = {Handbook of Visual Display Technology},
doi = {10.1007/978-3-642-35947-7_128-2},
file = {:home/miguel/Dropbox/Mendeley Desktop/Brown, Urey - 2015 - MEMS Microdisplays.pdf:pdf},
pages = {1--15},
publisher = {Springer Berlin Heidelberg},
title = {{MEMS Microdisplays}},
url = {http://link.springer.com/10.1007/978-3-642-35947-7{\_}128-2},
year = {2015}
}
@article{Pizarro2003,
abstract = {Severe attenuation and backscatter of light fundamentally limits our ability to image extended underwater scenes. Generating a composite view or mosaic from multiple overlapping images is usually the most practical and flexible way around this limitation. In this paper, we look at the general constraints associated with imaging from underwater vehicles for scientific applications - low overlap, nonuniform lighting, and unstructured motion {\$}and present a methodology for dealing with these constraints toward a solution of the problem of large-area global mosaicing. Our approach assumes that the extended scene is planar and determines the homographies for each image by estimating and compensating for radial distortion, topology estimation through feature-based pairwise image registration using a multiscale Harris interest point detector coupled with a feature descriptor based on Zernike moments, and global registration across all images based on the initial registration derived from the pairwise estimates. This approach is purely image based and does not assume that navigation data is available. We demonstrate the utility of our techniques using real data obtained using the Jason remotely operated vehicle (ROV) at an archaeological site covering hundreds of square meters.},
author = {Pizarro, Oscar and Singh, Hanumant},
doi = {10.1109/JOE.2003.819154},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pizarro, Singh - 2003 - Toward large-area mosaicing for underwater scientific applications.pdf:pdf},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Feature-based registration,Global alignment,Global registration,Multiscale Harris interest-point detector,Radial distortion compensation,Underwater mosaicing,Zernike moments},
month = {oct},
number = {4},
pages = {651--672},
title = {{Toward large-area mosaicing for underwater scientific applications}},
url = {http://ieeexplore.ieee.org/document/1255512/},
volume = {28},
year = {2003}
}
@article{Palomer2019a,
annote = {From Duplicate 1 (Inspection of an Underwater Structure using Point Cloud SLAM with an AUV and a Laser Scanner - Palomer, Albert; Ridao, Pere; Ribas, David)

Under review

From Duplicate 2 (Inspection of an Underwater Structure using Point Cloud SLAM with an AUV and a Laser Scanner - Palomer, Albert; Ridao, Pere; Ribas, David)

Under review
Submitted to Journal of Field Robotics on July 2018},
author = {Palomer, Albert and Ridao, Pere and Ribas, David},
doi = {10.1002/rob.21907},
file = {:home/miguel/Dropbox/Mendeley Desktop/Palomer, Ridao, Ribas - 2019 - Inspection of an Underwater Structure using Point Cloud SLAM with an AUV and a Laser Scanner.pdf:pdf},
journal = {Journal of Field Robotics},
number = {8},
pages = {1333----1344},
title = {{Inspection of an Underwater Structure using Point Cloud SLAM with an AUV and a Laser Scanner}},
volume = {36},
year = {2019}
}
@inproceedings{Tsui2014,
abstract = {Underwater depth measurements and point cloud images can be used for robotic navigation and haptic feedback. Structured Light and Time of Flight (ToF) depth camera systems were tested to demonstrate depth measurements and point cloud imaging underwater. A commercial ToF depth camera was modified to include a movable external light source. Images from depth measurements and constructed point cloud images from underwater tests are shown. A comparison of depth images captured while objects were positioned {\~{}}0.10-1.80 m from the camera is presented. Calibration of ToF cameras augmented with movable external light sources for underwater use is discussed.},
author = {Tsui, Chi Leung and Schipf, David and Lin, Keng Ren and Leang, Jonathan and Hsieh, Feng Ju and Wang, Wei Chih},
booktitle = {OCEANS 2014 - TAIPEI},
doi = {10.1109/OCEANS-TAIPEI.2014.6964471},
file = {:home/miguel/Dropbox/Mendeley Desktop/Tsui et al. - 2014 - Using a Time of Flight method for underwater 3-dimensional depth measurements and point cloud imaging.pdf:pdf},
isbn = {9781479936465},
keywords = {Depth Camera,Haptics,Kinect,Point Cloud,Time-of-Flight,Underwater Optics},
month = {apr},
pages = {1--6},
publisher = {IEEE},
title = {{Using a Time of Flight method for underwater 3-dimensional depth measurements and point cloud imaging}},
url = {http://ieeexplore.ieee.org/document/6964471/},
year = {2014}
}
@article{Busck2004,
abstract = {We have developed a fast and high-accuracy three-dimensional (3-D) imaging laser radar that can achieve better than 1-mm range accuracy for half a million pixels in less than 1 s. Our technique is based on range-gating segmentation. We combine the advantages of gated viewing with our new fast technique of 3-D imaging. The system uses a picosecond Q-switched Nd:Yag laser at 532 nm with a 32-kHz pulse repetition frequency (PRF), which triggers an ultrafast camera with a highly sensitive CCD with 582 x 752 pixels. The high range accuracy is achieved with narrow laser pulse widths of approximately 200 ps, a high PRF of 32 kHz, and a high-speed camera with gate times down to 200 ps and delay steps down to 100 ps. The electronics and the software also allow for gated viewing with automatic gain control versus range, whereby foreground backscatter can be suppressed. We describe our technique for the rapid production of high-accuracy 3-D images, derive performance characteristics, and outline future improvements.},
author = {Busck, Jens and Heiselberg, Henning},
doi = {10.1364/AO.43.004705},
file = {:home/miguel/Dropbox/Mendeley Desktop/Busck, Heiselberg - 2004 - Gated viewing and high-accuracy three-dimensional laser radar.pdf:pdf},
issn = {0003-6935},
journal = {Applied Optics},
keywords = {Atmospheric turbulence,Laser radar,Q switched lasers,Remote sensing,Three dimensional imaging,Ultrafast lasers},
month = {aug},
number = {24},
pages = {4705},
pmid = {15352395},
publisher = {Optical Society of America},
title = {{Gated viewing and high-accuracy three-dimensional laser radar}},
url = {https://www.osapublishing.org/abstract.cfm?URI=ao-43-24-4705},
volume = {43},
year = {2004}
}
@article{Luey2017,
abstract = {Phaeodactylum tricornutum Bohlin is an ideal model diatom; its complete genome is known, and it is an important economic microalgae. Although silicon is not required in laboratory and factory culture of this species, previous studies have shown that silicon starvation can lead to differential expression of miRNAs. The role that silicon plays in P. tricornutum growth in nature is poorly understood. In this study, we compared the growth rate of silicon starved P. tricornutum with that of normal cultured cells under different culture conditions. Pigment analysis, photosynthesis measurement, lipid analysis, and proteomic analysis showed that silicon plays an important role in P. tricornutum growth and that its presence allows the organism to grow well under green light and low temperature.},
author = {Luey, Ben and Davis, Scott R and Rommel, Scott D and Gann, Derek and Gamble, Joseph and Ziemkiewicz, Michael and Anderson, Mike and Paine, Roxie},
doi = {http://dx.doi.org/10.1038/srep03958},
file = {:home/miguel/Dropbox/Mendeley Desktop/Luey et al. - 2017 - A Lightweight, Cost-Efficient, Solid-State LiDAR System Utilizing Liquid Crystal Technology for Laser Beam Steering.pdf:pdf},
institution = {Analog Devices, Inc.},
journal = {25th International Technical Conference on the Enhanced Safety of Vehicles (ESV) National Highway Traffic Safety Administration},
pages = {1--9},
title = {{A Lightweight, Cost-Efficient, Solid-State LiDAR System Utilizing Liquid Crystal Technology for Laser Beam Steering for Advanced Driver Assistance}},
url = {http://indexsmart.mirasmart.com/25esv/PDFfiles/25ESV-000323.pdf},
year = {2017}
}
@inproceedings{Ekkel2015,
abstract = {This paper deals with the development of a measuring procedure and an experimental set-up (stereo camera system in combination with a projecting line laser and a positioning unit) which are intended to detect the surface topography, particularly of welds, with high accuracy in underwater environments. The system concept makes provision for the fact that the device can be positioned in space and manipulated by hand. The development, optimization and testing of the system components for surface measurements as well as calibration and accuracy evaluations are the main objectives within this research project. Testing procedures and probes are constructed and evaluated to verify the results. First results will be shown, where the test objects are underwater. The development considers conditions for a future adaption to underwater use.},
author = {Ekkel, T and Schmik, J and Luhmann, T and Hastedt, H},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprsarchives-XL-5-W5-117-2015},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ekkel et al. - 2015 - Precise laser-based optical 3D measurement of welding seams under water.pdf:pdf},
issn = {16821750},
keywords = {Calibration,Camera,Multi-media,Optical,Photogrammetry,Tracking,Underwater},
number = {5W5},
pages = {117--122},
title = {{Precise laser-based optical 3D measurement of welding seams under water}},
url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-5-W5/117/2015/isprsarchives-XL-5-W5-117-2015.pdf},
volume = {40},
year = {2015}
}
@inproceedings{Kocak1999,
abstract = {Manned submersibles and remotely operated vehicles make it possible to use many of the techniques of land geology on the seafloor. A fundamental aspect of geological maps is the documentation of the orientation of various planes and lines in space. Strike and dip typically characterize planes, and trend and plunge characterize lines. Sedimentary bedding, lava flow tops, dike margins, igneous layering, metamorphic foliations, joints and faults, etc. Land geologists determine the orientation of outcrop-scale features with various types of hand-held compasses and inclinometers. However, this type of instrument is not appropriate for use on the seafloor and different approaches are required to obtain orientation data. A 3D laser scanning system currently being developed by the Harbor Branch Oceanographic Institution Engineering Division, under National Science Foundation and Duke University sponsorship, affords a method of collecting orientation data. Similar to previously designed HBOI systems, surfaces of interest are rapidly scanned to produce high-resolution digital maps. The 3D map coordinates combined with the measured roll and pitch angles of the instrument are used to accurately determine orientation of the scanned geologically relevant planes and lines on seafloor outcrops. Unlike other techniques currently in use, this instrument does not need to be carefully positioned or placed on the rock surface and is not affected by magnetic fields. Furthermore, due to the high scan rate, the instrument need not be held stationary while scanning. During a single seafloor traverse of several hours, thousands of measurements can potentially be made},
author = {Kocak, Donna M. and Caimi, Frank Michael and Das, P.S. and Karson, J.A.},
booktitle = {Oceans '99. MTS/IEEE. Riding the Crest into the 21st Century. Conference and Exhibition. Conference Proceedings (IEEE Cat. No.99CH37008)},
doi = {10.1109/OCEANS.1999.800144},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kocak et al. - 1999 - A 3-D laser line scanner for outcrop scale studies of seafloor features.pdf:pdf},
isbn = {0-7803-5628-4},
issn = {01977385},
pages = {1105--1114},
publisher = {IEEE {\&} Marine Technol. Soc},
title = {{A 3-D laser line scanner for outcrop scale studies of seafloor features}},
url = {http://ieeexplore.ieee.org/document/800144/},
volume = {3},
year = {1999}
}
@article{Bonin-Font2015,
abstract = {Underwater activities, such as surveying or interventions, carried out by autonomous robots, can benefit greatly from using a vision system. Optics based systems provide information at a spatial and temporal resolution higher than their acoustic counterparts. At present, they are the best option when high precision maneuvering and manipulation is needed, if there is good visibility. This paper presents a new system designed to provide visual information in submarine tasks such as navigation, surveying, mapping and intervention. The main advantages of our system, called Fugu-f (Fugu flexible), are its robustness in both the mechanical structure and the software components, its flexibility, since it is installed as an external module and is adaptable to different vehicles and missions, and its capacity to operate in real-time. Experiments of surveying and object manipulation carried out in real conditions in the context of the TRIDENT project show the suitability of the system and its scientific and industrial potential applications.},
author = {Bonin-Font, Francisco and Oliver, Gabriel and Wirth, Stephan and Massot, Miquel and {Lluis Negre}, Pep and Beltran, Joan Pau},
doi = {10.1016/j.oceaneng.2014.11.005},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bonin-Font et al. - 2015 - Visual sensing for autonomous underwater exploration and intervention tasks.pdf:pdf},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {Underwater intervention and exploration,Vision-based navigation,Visual object detection and tracking},
month = {jan},
pages = {25--44},
title = {{Visual sensing for autonomous underwater exploration and intervention tasks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0029801814004090},
volume = {93},
year = {2015}
}
@techreport{ISOMET2018,
author = {ISOMET},
file = {:home/miguel/Dropbox/Mendeley Desktop/ISOMET - 2018 - Acousto-Optic Scanning and Deflection.pdf:pdf},
title = {{Acousto-Optic Scanning and Deflection}},
year = {2018}
}
@inproceedings{Liu2010,
abstract = {Nowadays, 3D laser scanners are widely used in reverse engineering, industrial design, prototyping, quality control etc. Most of these scanners operate in air. Theoretically, this technology can be extended for the development of 3D laser scanners that work in water. In this paper, we describe the development and practical issues of 3D laser scanners required for applications such as underwater oil and gas inspection. Some experimental results are also shown to demonstrate the feasibility and limit of the 3D laser scanners developed.},
author = {Liu, Jun Jie and Jakas, Anthony and Al-Obaidi, Ala and Liu, Yonghuai},
booktitle = {Proceedings of the 15th IEEE International Conference on Emerging Technologies and Factory Automation, ETFA 2010},
doi = {10.1109/ETFA.2010.5641223},
file = {:home/miguel/Dropbox/Mendeley Desktop/Liu et al. - 2010 - Practical issues and development of underwater 3D laser scanners.pdf:pdf},
isbn = {9781424468508},
month = {sep},
pages = {1--8},
publisher = {IEEE},
title = {{Practical issues and development of underwater 3D laser scanners}},
url = {http://ieeexplore.ieee.org/document/5641223/},
year = {2010}
}
@article{Bechtold2013a,
abstract = {Weintroduce a method to objectively evaluate systems of differing beam deflection technologies that commonly are described by disparate technical specifications. Using our new approach based on resolvable spots we will compare commercially available random-access beam deflection technologies, namely galvanometer scanners, piezo scan- ners, MEMS scanners, acousto-optic deflectors, and electro-optic deflectors.},
author = {Bechtold, Peter and Hohenstein, Ralph and Schmidt, Michael},
doi = {10.1364/ol.38.002934},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bechtold, Hohenstein, Schmidt - 2013 - Evaluation of disparate laser beam deflection technologies by means of number and rate of resolva.pdf:pdf},
issn = {0146-9592},
journal = {Optics Letters},
keywords = {Acoustooptical deflectors,Laser beams,Laser machining,Laser materials processing,Pulsed operation,Scanners},
month = {aug},
number = {16},
pages = {2934},
publisher = {Optical Society of America},
title = {{Evaluation of disparate laser beam deflection technologies by means of number and rate of resolvable spots}},
url = {https://www.osapublishing.org/abstract.cfm?URI=ol-38-16-2934},
volume = {38},
year = {2013}
}
@inproceedings{Cole,
abstract = {Traditional Simultaneous Localization and Mapping (SLAM) algorithms have been used to great effect in flat, indoor environments such as corridors and offices. We demonstrate that with a few augmentations, existing 2D SLAM technology can be extended to perform full 3D SLAM in less benign, outdoor, undulating environments. In particular, we will use data acquired with a 3D laser range finder. We use a simple segmentation algorithm to separate the data stream into distinct point clouds, each referenced to a vehicle position. The SLAM technique we then adopt inherits much from 2D Delayed State (or scan-matching) SLAM in that the state vector is an ever growing stack of past vehicle positions and inter-scan registrations are used to form measurements between them. The registration algorithm used is a novel combination of previous techniques carefully balancing the need for maximally wide convergence basins, robustness and speed. In addition, we introduce a novel post-registration classification technique to detect matches which have converged to incorrect local minima. {\textcopyright} 2006 IEEE.},
author = {Cole, David M and Newman, Paul M},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2006.1641929},
file = {:home/miguel/Dropbox/Mendeley Desktop/Cole, Newman - 2006 - Using laser range data for 3D SLAM in outdoor environments.pdf:pdf},
isbn = {0780395069},
issn = {10504729},
keywords = {3D laser data,Delayed state EKF,Mobile robotics,Outdoor 3D SLAM,Point cloud segmentation},
pages = {1556--1563},
title = {{Using laser range data for 3D SLAM in outdoor environments}},
volume = {2006},
year = {2006}
}
@article{Zhang2000,
abstract = {Outdoor camera networks are becoming ubiquitous in critical urban areas of the largest cities around the world. Although current applications of camera networks are mostly tailored to video surveillance, recent research projects are exploiting their use to aid robotic systems in people-assisting tasks. Such systems require precise calibration of the internal and external parameters of the distributed camera network. Despite the fact that camera calibration has been an extensively studied topic, the development of practical methods for user-assisted calibration that minimize user intervention time and maximize precision still pose significant challenges. These camera systems have non-overlapping fields of view, are subject to environmental stress, and are likely to suffer frequent recalibration. In this paper, we propose the use of a 3D map covering the area to support the calibration process and develop an automated method that allows quick and precise calibration of a large camera network. We present two cases of study of the proposed calibration method: one is the calibration of the Barcelona Robot Lab camera network, which also includes direct mappings (homographies) between image coordinates and world points in the ground plane (walking areas) to support person and robot detection and localization algorithms. The second case consist of improving the GPS positioning of geo-tagged images taken with a mobile device in the Facultat de Matem{\`{a}}tiques i Estad{\'{i}}stica (FME) patio at the Universitat Polit{\`{e}}cnica de Catalunya (UPC).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zhang, Zhengyou},
doi = {10.1109/34.888718},
eprint = {arXiv:1011.1669v3},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zhang - 2000 - A Flexible New Technique for Camera Calibration.pdf:pdf},
isbn = {MSR-TR-98-71},
issn = {01628828},
journal = {IEEE Transactions on pattern analysis and machine intelligence},
keywords = {Camera network calibration},
pmid = {131},
title = {{A Flexible New Technique for Camera Calibration}},
url = {http://research.microsoft.com/˜zhanghttp://research.microsoft.com/˜zhang},
volume = {22},
year = {2000}
}
@article{Kang2017,
author = {Kang, Lai and Wu, Lingda and Wei, Yingmei and Lao, Songyang and Yang, Yee-Hong},
doi = {10.1016/j.patcog.2017.04.006},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kang et al. - 2017 - Two-view underwater 3D reconstruction for cameras with unknown poses under flat refractive interfaces.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
month = {sep},
pages = {251--269},
title = {{Two-view underwater 3D reconstruction for cameras with unknown poses under flat refractive interfaces}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320317301516},
volume = {69},
year = {2017}
}
@article{Lu2018,
abstract = {Underwater optical images are usually influenced by low lighting, high turbidity scattering and wavelength absorption. To solve these issues, a great deal of work has been performed to improve the quality of underwater images. Most of them use the high-intensity LEDs for lighting to obtain the high contrast images. However, in high turbidity water, high-intensity LEDs cause strong scattering and absorption. In this paper, we propose a light field imaging approach for solving underwater imaging problems in a low-intensity light environment. As a solution, we tackle the problem of de-scattering from light field images by using deep convolutional neural networks with depth estimation. Furthermore, a spectral characteristic-based color correction method is used for recovering the color reduction. Experimental results show the effectiveness of the proposed method by challenging real-world underwater imaging.},
author = {Lu, Huimin and Li, Yujie and Uemura, Tomoki and Kim, Hyoungseop and Serikawa, Seiichi},
doi = {10.1016/j.future.2018.01.001},
file = {:home/miguel/Dropbox/Mendeley Desktop/Lu et al. - 2018 - Low illumination underwater light field images reconstruction using deep convolutional neural networks.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {De-scattering,Deep convolutional neural networks,Light field camera,Review,Underwater imaging},
month = {may},
pages = {142--148},
title = {{Low illumination underwater light field images reconstruction using deep convolutional neural networks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X17326845},
volume = {82},
year = {2018}
}
@book{Marshall2004,
abstract = {From its initial publication titled Laser Beam Scanning in 1985 to Handbook of Optical and Laser Scanning, now in its second edition, this reference has kept professionals and students at the forefront of optical scanning technology. Carefully and meticulously updated in each iteration, the book continues to be the most comprehensive scanning resource on the market. It examines the breadth and depth of subtopics in the field from a variety of perspectives. The Second Edition covers: Technologies such as piezoelectric devices Applications of laser scanning such as Ladar (laser radar) Underwater scanning and laser scanning in CTP As laser costs come down, and power and availability increase, the potential applications for laser scanning continue to increase. Bringing together the knowledge and experience of 26 authors from England, Japan and the United States, the book provides an excellent resource for understanding the principles of laser scanning. It illustrates the significance of scanning in society today and would help the user get started in developing system concepts using scanning. It can be used as an introduction to the field and as a reference for persons involved in any aspect of optical and laser beam scanning.},
author = {Marshall, Gerald F. and Stutz, Glenn E.},
booktitle = {Handbook of Optical and Laser Scanning},
doi = {10.1201/9780824759896},
editor = {Marshall, Gerald},
file = {:home/miguel/Dropbox/Mendeley Desktop/Marshall, Stutz - 2004 - Handbook of Optical and Laser Scanning.pdf:pdf},
isbn = {9780824759896},
month = {jul},
publisher = {CRC Press},
series = {Optical Science and Engineering},
title = {{Handbook of Optical and Laser Scanning}},
url = {https://www.taylorfrancis.com/books/9780824759896},
year = {2004}
}
@book{Wilde1998,
author = {Wilde, FD and Radtke, DB and Gibs, Jacob and Iwatsubo, RT},
edition = {9},
publisher = {US Geological Survey Techniques in Water-Resources Investigations},
title = {{National field manual for the collection of water-quality data}},
url = {http://pubs.water.usgs.gov/twri9A},
year = {1998}
}
@inproceedings{Jaffe2013,
abstract = {Underwater optical imaging is an important area for sensing in aquatic environments. In this 2nd article of a two part proceedings series, a contemporary perspective on underwater imaging is presented. As a point of reference, the advent of digital cameras and processing is taken as a transition point that ushered in the modern era. This increase in capability facilitated both the use of more sophisticated technology for image formation and also for processing that led to the use of laser line scan systems, range gated images, and the mosaicking of large areas for survey of the sea floor. Digital technology in both recording and processing underwater images now predominates because of the continuing electronic revolution. The fundamental limitations of underwater imaging have been known for nearly half a decade now, however, increased performance in illumination, recording, and the use of more sophisticated processing methods continues to present new opportunities for increased performance at decreased costs.},
author = {Jaffe, Jules S.},
booktitle = {OCEANS 2013 MTS/IEEE Bergen: The Challenges of the Northern Dimension},
doi = {10.1109/OCEANS-Bergen.2013.6608120},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jaffe - 2013 - A contemporary perspective on underwater optical imaging.pdf:pdf},
isbn = {9781479900015},
keywords = {imaging through haze,imaging through turbid environments,underwater optical imaging},
month = {jun},
pages = {1--3},
publisher = {IEEE},
title = {{A contemporary perspective on underwater optical imaging}},
url = {http://ieeexplore.ieee.org/document/6608120/},
year = {2013}
}
@inproceedings{Furukawa2008,
author = {Furukawa, Ryo and Viet, Huynh Quang Huy and Kawasaki, Hiroshi and Sagawa, Ryusuke and Yagi, Yasushi},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2008.4712057},
file = {:home/miguel/Dropbox/Mendeley Desktop/Furukawa et al. - 2008 - One-shot range scanner using coplanarity constraints.pdf:pdf},
isbn = {1424417643},
issn = {15224880},
keywords = {Machine vision,Shape measurement},
pages = {1524--1527},
publisher = {IEEE},
title = {{One-shot range scanner using coplanarity constraints}},
url = {http://ieeexplore.ieee.org/document/4712057/},
year = {2008}
}
@inproceedings{Kondo2004a,
abstract = {Autonomous Underwater Vehicles (AUVs) are suitable for condition survey of artificial structures such as pillars and caissons in harbors. This paper describes a method to trace the structure's surface using a light-section profiling system. This profiling system determines the continuous shape of the target objects over a wide area by the light sectioning method. The vehicle navigates referencing the principal shape of the structure, tracing its surface while taking video images. This method also enables three-dimensional mapping of traced structures and the seabed. The method is implemented using testbed AUV "Tri-Dog 1" and verified by tank tests. The AUV navigates robustly against trivial objects such as small obstacles and floating particles. Precise depth mapping of the tank bottom is carried out.},
author = {Kondo, Hayato and Maki, Toshihiro and Ura, Tamaki and Nose, Yoshiaki and Sakamaki, Takashi and Inaishi, Masaaki},
booktitle = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2004.1389544},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kondo et al. - 2004 - Relative navigation of an Autonomous Underwater Vehicle using a light-section profiling system.pdf:pdf},
isbn = {0780384636},
pages = {1103--1108},
publisher = {IEEE},
title = {{Relative navigation of an Autonomous Underwater Vehicle using a light-section profiling system}},
url = {http://ieeexplore.ieee.org/document/1389544/},
volume = {2},
year = {2004}
}
@inproceedings{Schenk2014,
abstract = {This paper focuses on high-speed optical MEMS Scanners and Micro Mirror Arrays. Devices supporting spot/pixel rates higher than 10 Mpixel / s are considered and discussed regarding limits and possibilities to further improve speed and optical properties. Several variants of both types, developed by our group, are presented. Scanning Micro Mirrors with frequencies up to 100 kHz enable spot rates of up to 130 Mpixels / s at 650 nm. Bragg-coatings enable high power applications up to 20 W (beam {\o} 2 mm). Challenges like static and dynamic mirror planariy are discussed. A 29-kHz-scanner for laser projection serves as application example. Highly parallel operated Micro Mirror Arrays extend pattern speed to 10 Gpixel / s including analog grey scaling. Irradiation tests prove stable operation of the mirrors at DUV. Prospects regarding optical planarity and high reflective coatings are discussed. By means of two examples, laser patterning of semiconductor masks and laser patterning of Printed Circuit Boards, properties of the spatial light modulators are presented. The two device classes are compared regarding spot/pixel rate and frequency. The comparison includes representative MEMS device examples from literature.},
author = {Schenk, Harald and Grahmann, Jan and Sandner, Thilo and Wagner, Michael and Dauderst{\"{a}}dt, Ulrike and Schmidt, Jan Uwe},
booktitle = {Physics Procedia},
doi = {10.1016/j.phpro.2014.08.090},
file = {:home/miguel/Dropbox/Mendeley Desktop/Schenk et al. - 2014 - Micro mirrors for high-speed laser deflection and patterning.pdf:pdf},
issn = {18753892},
keywords = {Laser patterning,MEMS mirror,Micro Mirror Array,Micro mirror,Mirror device,Scanner,Spatial light modulator},
number = {C},
pages = {7--18},
title = {{Micro mirrors for high-speed laser deflection and patterning}},
url = {www.sciencedirect.com},
volume = {56},
year = {2014}
}
@article{Neumann2018a,
abstract = {Underwater image processing has to face the problem of loss of color and con- trast that occurs when images are acquired at a certain depth and range. The longer wavelengths of sunlight such as red or orange are rapidly absorbed by the water body, while the shorter ones have a higher scattering. Thereby, at larger distance, the scene colors appear bluish-greenish, as well as blurry. The loss of color increases not only vertically through the water column, but also horizontally, so that the subjects further away from the camera appear colorless and indistinguishable, suffering from lack of visible details. This paper presents a fast enhancement method for color correction of under- water images. The method is based on the gray-world assumption applied in the Ruderman-opponent color space and is able to cope with non-uniformly illuminated scenes. Integral images are exploited by the proposed method to perform fast color correction, taking into account locally changing luminance and chrominance. Due to the low-complexity cost this method is suitable for real-time applications ensuring realistic colors of the objects, more visible de- tails and enhanced visual quality. Keywords},
author = {Neumann, L{\'{a}}szl{\'{o}} and Garcia, Rafael and J{\'{a}}nosik, J{\'{o}}zsef and Gracias, Nuno},
file = {:home/miguel/Dropbox/Mendeley Desktop/Neumann et al. - 2018 - Fast underwater color correction using integral images.pdf:pdf},
journal = {Instrumentation viewpoint},
keywords = {- underwater imaging,and scattering,and these phenomena affect,as light propagates through,color correction,color correction in underwater,every wavelength of white,i,image enhancement,imaging,in a different way,it suffers from absorption,light,water},
number = {20},
pages = {53--54},
title = {{Fast underwater color correction using integral images}},
url = {https://upcommons.upc.edu/bitstream/handle/2117/126282/ID37.pdf},
year = {2018}
}
@article{Sarafraz2016,
abstract = {A new structured-light method for 3D imaging has been developed which can simultaneously estimate both the geometric shape of the water surface and the geometric shape of underwater objects. The method requires only a single image and thus can be applied to dynamic as well as static scenes. Experimental results show the utility of this method in non-invasive underwater 3D reconstruction applications. The performance of the new method is studied through a sensitivity analysis for different parameters of the suggested method.},
author = {Sarafraz, Amin and Haus, Brian K.},
doi = {10.1016/j.isprsjprs.2016.01.014},
file = {:home/miguel/Dropbox/Mendeley Desktop/Sarafraz, Haus - 2016 - A structured light method for underwater surface reconstruction.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {3D reconstruction,Structured light,Water surface estimation},
month = {apr},
pages = {40--52},
publisher = {Elsevier},
title = {{A structured light method for underwater surface reconstruction}},
url = {https://www.sciencedirect.com/science/article/pii/S0924271616000290},
volume = {114},
year = {2016}
}
@inproceedings{Bleier2017,
abstract = {In-situ calibration of structured light scanners in underwater environments is time-consuming and complicated. This paper presents a self-calibrating line laser scanning system, which enables the creation of dense 3D models with a single fixed camera and a freely moving hand-held cross line laser projector. The proposed approach exploits geometric constraints, such as coplanarities, to recover the depth information and is applicable without any prior knowledge of the position and orientation of the laser projector. By employing an off-the-shelf underwater camera and a waterproof housing with high power line lasers an affordable 3D scanning solution can be built. In experiments the performance of the proposed technique is studied and compared with 3D reconstruction using explicit calibration. We demonstrate that the scanning system can be applied to above-the-water as well as underwater scenes.},
author = {Bleier, Michael and N{\"{u}}chter, Andreas},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W3-105-2017},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bleier, N{\"{u}}chter - 2017 - Low-Cost 3D laser scanning in air or water using self-calibrating structured light.pdf:pdf},
issn = {16821750},
keywords = {3D reconstruction,Self-calibration,Structured light,Underwater laser scanning},
number = {2W3},
pages = {105--112},
title = {{Low-Cost 3D laser scanning in air or water using self-calibrating structured light}},
url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W3/105/2017/isprs-archives-XLII-2-W3-105-2017.pdf},
volume = {42},
year = {2017}
}
@article{Johnson-Roberson2017,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Johnson-Roberson, Matthew and Bryson, Mitch and Friedman, Ariell and Pizarro, Oscar and Troni, Giancarlo and Ozog, Paul and Henderson, Jon C},
doi = {10.1002/rob.21658},
eprint = {10.1.1.91.5767},
file = {:home/miguel/Dropbox/Mendeley Desktop/Johnson-Roberson et al. - 2017 - High-Resolution Underwater Robotic Vision-Based Mapping and Three-Dimensional Reconstruction for Archae.pdf:pdf},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {4},
pages = {625--643},
pmid = {22164016},
title = {{High-Resolution Underwater Robotic Vision-Based Mapping and Three-Dimensional Reconstruction for Archaeology}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21658},
volume = {34},
year = {2017}
}
@phdthesis{Wang2018,
abstract = {In the past few decades, imaging technology has made great strides. From high resolution sensors for photography to 3D scanners in autonomous driving, imaging has become one of the key drivers of the modern society. However, there are still many scenarios where the traditional methods of imaging are woefully inadequate. Examples include high-resolution non-visible light imaging, 3D scanning in the presence of strong ambient light, and imaging through scattering media. In these scenarios, the two classical solutions of single-shot imaging using 2D sensors and point scanning using photodiodes have severe shortcomings in terms of cost, measurement rate and robustness to non-idealities in the imaging process. The goal of this dissertation is the design of computational imagers that work under traditionally difficult conditions by providing the robustness and economy of point scanning systems along with the speed and resolution of conventional cameras. In order to achieve this goal, we use line sensors or 1D sensors and make three contributions in this dissertation. The first contribution is the design of a line sensor based compressive camera (LiSens) which uses a line sensor and a spatial light modulator for 2D imaging. It can provide a measurement rate that is equal to that of a 2D sensor but with only a fraction of the number of pixels. The second contribution is the design of a dual structured light (DualSL) system which uses a 1D sensor and a 2D projector to achieve 3D scanning with same resolution and performance as traditional structured light system. The third contribution is the design of programmable triangulation light curtains (TriLC) for proximity detection by rotating a 1D sensor and a 1D light source in synchrony. This device detects the presence of objects that intersect a programmable virtual shell around itself. The shape of this virtual shell can be changed during operation and the device can perform under strong sunlight as well as in foggy and smoky environments. We believe that the camera architectures proposed in this dissertation can be used in a wide range of applications, such as autonomous driving cars, field robotics, and underwater exploration.},
author = {Wang, Jian},
booktitle = {ProQuest Dissertations and Theses},
file = {:home/miguel/Dropbox/Mendeley Desktop/Wang - 2018 - High Resolution 2D Imaging and 3D Scanning with Line Sensors.pdf:pdf},
isbn = {9780438338784},
keywords = {0464:Computer Engineering,0544:Electrical engineering,0752:Optics,3D scanning,Applied sciences,Compressive sensing,Computational photography,Computer Engineering,Computer vision,Electrical engineering,Optics,Pure sciences},
pages = {98},
title = {{High Resolution 2D Imaging and 3D Scanning with Line Sensors}},
url = {https://media.proquest.com/media/pq/classic/doc/4327094457/fmt/ai/rep/NPDF?{\_}a=ChgyMDE5MDIwNzExNTIyMDU5Njo1NDI0MzYSBTk0MTE1GgpPTkVfU0VBUkNIIg04NC44OC4xNTQuMjA0KgUxODc1MDIKMjEwMDcwMzAwNDoRT3BlbnZpZXdQYWdlSW1hZ2VCATFSBk9ubGluZVoCRlRiA1BGVGoKMjAxOC8wMS8wMXIKM},
year = {2018}
}
@article{Menna2018,
abstract = {Since remote times, mankind has been bound to water bodies and evidence of human life from the very beginning hides under the water level, off the coasts, under shallow seas or deep oceans, but also inland water bodies of countries all around the world. Recording, documenting and, ultimately, protecting underwater cultural heritage is an obligation of mankind and dictated by international treaties like the Convention on the Protection of the Underwater Cultural Heritage that fosters and encourages the use of “non-destructive techniques and survey methods in preference over the recovery of objects”. 3D digital surveying and mapping techniques represent an invaluable set of effective tools for reconnaissance, documentation, monitoring, but also public diffusion and awareness of underwater cultural heritage (UCH) assets. This paper presents an extensive review over the sensors and the methodologies used in archaeological underwater 3D recording and mapping together with relevant highlights of well renowned projects in 3D recording underwater.},
author = {Menna, Fabio and Agrafiotis, Panagiotis and Georgopoulos, Andreas},
doi = {10.1016/j.culher.2018.02.017},
file = {:home/miguel/Dropbox/Mendeley Desktop/Menna, Agrafiotis, Georgopoulos - 2018 - State of the art and applications in archaeological underwater 3D recording and mapping.pdf:pdf},
isbn = {1545-5017 (Electronic)$\backslash$r1545-5009 (Linking)},
issn = {12962074},
journal = {Journal of Cultural Heritage},
keywords = {3D Recording and Mapping,LiDAR,Photogrammetry,Sonar,Underwater Archaeology},
month = {sep},
pages = {231--248},
pmid = {19101997},
title = {{State of the art and applications in archaeological underwater 3D recording and mapping}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1296207417308555},
volume = {33},
year = {2018}
}
@article{Weiner2000,
abstract = {We review the field of femtosecond pulse shaping, in which Fourier synthesis methods are used to generate nearly arbitrarily shaped ultrafast optical wave forms according to user specification. An emphasis is placed on programmable pulse shaping methods based on the use of spatial light modulators. After outlining the fundamental principles of pulse shaping, we then present a detailed discussion of pulse shaping using several different types of spatial light modulators. Finally, new research directions in pulse shaping, and applications of pulse shaping to optical communications, biomedical optical imaging, high power laser amplifiers, quantum control, and laser-electron beam interactions are reviewed.},
author = {Weiner, A M},
doi = {10.1063/1.1150614},
file = {:home/miguel/Dropbox/Mendeley Desktop/Weiner - 2000 - Femtosecond pulse shaping using spatial light modulators.pdf:pdf},
issn = {00346748},
journal = {Review of Scientific Instruments},
number = {5},
pages = {1929--1951},
title = {{Femtosecond pulse shaping using spatial light modulators}},
url = {http://rsi.aip.org/rsi/copyright.jsp},
volume = {71},
year = {2000}
}
@inproceedings{Lam2007,
abstract = {An underwater surveillance camera has been designed for monitoring fish species activity on an inshore coral reef in a Marine Park in Hong Kong. The system consisted of a high-resolution camera. It was connected to the shore base station via a fibre-optic cable with power conductors. The camera could record video on a DVD recorder with hard disk in real time. The system was designed to be used for long periods of time up to three months continuously without maintainence. It was used to monitor changes in coral fish abundance with respect to time of day and among months. The present results thus showed two major behavioural patterns of coral fish in Hoi Ha Wan. The first is the diurnal activities as most fish species are active during the daylight rather than at night. Both mean number of species and fish density showed an increase in daylight and a decrease at night. The second was the seasonal migratory pattern. A seasonal variation of fish abundance on a subtropical reef was recorded with water temperature changes from 29degC in summer to 17degC in winter. Mean maximum number of species during daylight in warmer months was {\~{}}10-12 while that in colder months decreased to 2-3.},
author = {Lam, Katherine and Bradbeer, Robin S. and Shin, Paul K. S. and Ku, Kenneth K. K. and Hodgson, Paul},
booktitle = {Oceans Conference Record (IEEE)},
doi = {10.1109/OCEANS.2007.4449240},
isbn = {0933957351},
issn = {01977385},
month = {sep},
pages = {1--7},
publisher = {IEEE},
title = {{Application of a real-time underwater surveillance camera in monitoring of fish assemblages on a shallow coral communities in a marine park}},
url = {http://ieeexplore.ieee.org/document/4449240/},
year = {2007}
}
@article{Kocak2005,
abstract = {This State of Technology Report on Underwater Imaging provides a historical synopsis of underwater imaging, discusses current state of the art, and suggests future possibilities for continued advancement of the field. The history presented herein provides information assembled in a manner not found in previous reviews. Present work is grouped according to imaging methodology wherein foremost research and technical innovations of the field are highlighted, with a focus on the past five years. Trends in research and development are also discussed as they relate to emerging underwater imaging techniques and technologies.},
author = {Kocak, Donna M. and Caimi, Frank Michael},
doi = {10.4031/002533205787442576},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kocak, Caimi - 2005 - The Current Art of Underwater Imaging- With a Glimpse of the Past and Vision of the Future.pdf:pdf},
isbn = {0025-3324},
issn = {00253324},
journal = {Marine Technology Society Journal},
month = {sep},
number = {3},
pages = {5--26},
title = {{The Current Art of Underwater Imaging- With a Glimpse of the Past and Vision of the Future}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=0025-3324{\&}volume=39{\&}issue=3{\&}spage=5},
volume = {39},
year = {2005}
}
@article{Hafez2003,
abstract = {Beam distortion profiles are studied for scanning devices that have a single mirror with two rotational degrees of freedom (DOF), also named tip/tilt scanners. The case of a fast steering scanner used for high power material processing applications is studied. The scanner has a bandwidth of 700 Hz, a range of motion of ±52 mrad (±3 deg), and a resolution {\textless}5 $\mu$rad. The main dominant parameters that affect the distortion profile are identified. Furthermore, a vector analysis is derived to represent these distortions, and equations of the correction factors used to compensate for the systematic errors are proposed. A final accuracy better than 0.05{\%} is obtained when these compensation factors are taken into account. The derivation proposed here can be extended to any scanning device with a single mirror with two-rotation DOF. {\textcopyright} 2003 Society of Photo-Optical Instrumentation Engineers.},
author = {Hafez, Moustapha and Sidler, Thomas and Salath{\'{e}}, Ren{\'{e}}-Paul},
doi = {10.1117/1.1557694},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hafez, Sidler, Salath{\'{e}} - 2003 - Study of the beam path distortion profiles generated by a two-axis tilt single-mirror laser scanner.pdf:pdf},
issn = {0091-3286},
journal = {Optical Engineering},
keywords = {[SLAB]},
month = {apr},
number = {4},
pages = {1048 ---- 1057},
title = {{Study of the beam path distortion profiles generated by a two-axis tilt single-mirror laser scanner}},
url = {http://opticalengineering.spiedigitallibrary.org/article.aspx?doi=10.1117/1.1557694},
volume = {42},
year = {2003}
}
@article{Jaffe2015,
abstract = {This paper discusses the current state of underwater optical imaging in the context of physics, technology, biology, and history. The paper encompasses not only the history of human's ability to see underwater, but also the adaptations that various organisms living in oceans or lakes have developed. The continued development of underwater imaging systems at military, commercial, and consumer levels portends well for both increased visibility and accessibility by these various segments. However, the fundamental limits imposed by the environment, as currently understood, set the ultimate constraints. Physics, biology, computermodeling, processing, and the development of technology that ranges from simple cameras and lights to more advanced gated and modulated illumination are described. The future prospects for continuing advancements are also discussed.},
author = {Jaffe, Jules S.},
doi = {10.1109/JOE.2014.2350751},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jaffe - 2015 - Underwater Optical Imaging The Past, the Present, and the Prospects.pdf:pdf},
isbn = {1471-2474 (Electronic)$\backslash$r1471-2474 (Linking)},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Underwater optical imaging},
month = {jul},
number = {3},
pages = {683--700},
pmid = {19134184},
title = {{Underwater Optical Imaging: The Past, the Present, and the Prospects}},
url = {http://ieeexplore.ieee.org/document/6930829/},
volume = {40},
year = {2015}
}
@incollection{DeDominicis2013a,
abstract = {An impressive number of laser-based sensors for 3D vision for terrestrial applications have been developed so far, making it a mature and expanding market worth billions of euros. When it comes to the subsea environment the presence of water, combined with the demand to operate at depth, acts as game-changing factors and considerable scientific and technological challenges arise. This chapter concentrates on selected techniques for subsea 3D vision and ranging using laser-based sensors, with priority given to projects where the efforts have been focused on developing devices ready to be deployed in real scenarios. {\textcopyright} 2013 Woodhead Publishing Limited. All rights reserved.},
author = {de Dominicis, Luigi},
booktitle = {Subsea Optics and Imaging},
doi = {10.1533/9780857093523.3.379},
file = {:home/miguel/Dropbox/Mendeley Desktop/de Dominicis - 2013 - Underwater 3D vision, ranging and range gating.pdf:pdf},
isbn = {9780857093417},
keywords = {3D vision,Laser,Range gating,Ranging,Structural monitoring},
pages = {379--408},
publisher = {Elsevier},
title = {{Underwater 3D vision, ranging and range gating}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780857093417500157},
year = {2013}
}
@inproceedings{Palomer2017,
abstract = {Development of underwater 3D perception is necessary for autonomous manipulation and mapping. Using a mirror-galvanometer system to steer a laser plane and using triangulation, it is possible to produce full 3D perception without the need of moving the sensor. If the sensor does not meet certain hardware requirements, the laser plane is distorted when it passes through the different media (air–viewport–water). However, the deformation of this plane has not been studied. In this work a ray-tracing model is presented to study the deformation of the laser plane. To validate it, two types of datasets have been used, one synthetically generated using the model presented below, and another one using real data gathered underwater with an actual laser scanner. For both datasets an elliptic cone is fitted on the data and compared to a plane fit (the surface commonly used for triangulation). In the two experiments, the elliptic cone proved to be a better fit than the plane.},
author = {Palomer, Albert and Ridao, Pere and Ribas, David and Forest, Josep},
booktitle = {Lecture Notes in Control and Information Sciences},
doi = {10.1007/978-3-319-55372-6_4},
file = {:home/miguel/Dropbox/Mendeley Desktop/Palomer et al. - 2017 - Underwater 3D laser scanners The deformation of the plane.pdf:pdf},
isbn = {9783319553719},
issn = {01708643},
pages = {73--88},
publisher = {Springer, Cham},
title = {{Underwater 3D laser scanners: The deformation of the plane}},
url = {http://link.springer.com/10.1007/978-3-319-55372-6{\_}4},
volume = {474},
year = {2017}
}
@article{Stettner2001,
abstract = {This paper reviews the motivation, design and testing of a 3D imaging, sequential time-integration technology. Most recent test of a 10 by 10 and a 100 by 100 imaging chip are presented.},
author = {Stettner, Roger and Bailey, Howard W},
doi = {10.1117/12.440126},
editor = {Kamerman, Gary W.},
issn = {0277786X},
month = {sep},
pages = {57--64},
publisher = {International Society for Optics and Photonics},
title = {{Staring underwater laser radar (SULAR) 3-D Imaging}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=913624},
volume = {4377},
year = {2001}
}
@article{Jaffe2007,
abstract = {This article considers the potential use of multiple autonomous underwater vehicles (AUVs) in order to increase the performance of underwater optical imaging systems. In this case, placing cameras on one set of vehicles and lights on another set can surmount the traditional limitations to single vehicle imaging. As envisioned, the arbitrary location of these vehicles in 3-dimensional space permits the arbitrary placement of cameras and lights. One advantage over traditional, single vehicle implementations is that the locations of the cameras and the lights can be dynamically configured in order to optimize imaging system performance. Highly scattering environments might by optimized by one set of camera light configurations whereas low scattering and low absorption environments might benefit from alternate configurations. In addition. 3-dimensional measurement of the sea floor can be facilitated via various configurations that will support either structured light imaging or stereo imaging. We conclude that Multiple AUV Optical Imaging presents new options for improving the performance of underwater optical imaging system.},
author = {Jaffe, Jules S.},
doi = {10.1109/OCEANSE.2007.4302223},
isbn = {1424406358},
journal = {Oceans 2007 - Europe, Vols 1-3},
keywords = {autonomous underwater vehicles,auvs,image generation,underwater optical imaging},
month = {jun},
pages = {182--185$\backslash$r1581},
publisher = {IEEE},
title = {{Multi autonomous underwater vehicle optical imaging for extended performance}},
url = {http://ieeexplore.ieee.org/document/4302223/},
year = {2007}
}
@inproceedings{Morinaga2016,
abstract = {Structured Light Systems (SLS) are widely used for various purposes. Recently, a strong demand to apply SLS to underwater applications has emerged. When SLS is used in an air medium, the stereo correspondence problem can be solved efficiently by epipolar geometry due to the co-planarity of the 3D point and its corresponding 2D points on camera/projector planes. However, in underwater environments, the camera and projector are usually set in special housings and refraction occurs at the interfaces between water/glass and glass/air, resulting in invalid conditions for epipolar geometry which strongly affect the correspondence search process. In this paper, we tackle the problem of underwater 3D shape acquisition with SLS. In this paper, we propose a method to perform 3D reconstruction by calibrating the system as if they are in the air at multiple depth. Since refraction cannot be completely described by a polynomial approximation of distortion model, grid based SLS method solve the problem. Finally, we propose a bundle adjustment method to refine the final result. We tested our method with an underwater SLS prototype, consisting of custom-made diffractive optical element (DOE) laser and underwater housings, showing the validity of the proposed approach.},
author = {Morinaga, Hiroki and Baba, Hirohisa and Visentini-Scarzanella, Marco and Kawasaki, Hiroshi and Furukawa, Ryo and Sagawa, Ryusuke},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-29451-3_33},
file = {:home/miguel/Dropbox/Mendeley Desktop/Morinaga et al. - 2016 - Underwater active oneshot scan with static wave pattern and bundle adjustment.pdf:pdf},
isbn = {9783319294506},
issn = {16113349},
keywords = {Camera-projector system,Oneshot scan,Underwater scan},
pages = {404--418},
title = {{Underwater active oneshot scan with static wave pattern and bundle adjustment}},
url = {https://pdfs.semanticscholar.org/e9be/dbfa1df8ffd81636178b9cc41a2de2ca896a.pdf},
volume = {9431},
year = {2016}
}
@inproceedings{Einramhof2011,
abstract = {Human vision is the reference when designing perception systems for cognitive service robots, especially its ability to quickly identify task-relevant regions in a scene and to foveate on these regions. An adaptive 3D camera currently under development aims at mimicking these properties for endowing service robots with a higher level of perception and interaction capabilities with respect to everyday objects and environments. A scene is coarsely scanned and analyzed. Based on the result of analysis and the task, relevant regions within the scene are identified and data acquisition is concentrated on details of interest allowing for higher resolution 3D sampling of these details. To set the stage we first briefly describe the sensor hardware and focus then on the analysis of range images captured by the hardware. Two approaches - one based on saliency maps and the other on range image segmentation - and preliminary results are presented.},
author = {Einramhof, Peter and Schwarz, Robert and Vincze, Markus},
booktitle = {URAI 2011 - 2011 8th International Conference on Ubiquitous Robots and Ambient Intelligence},
doi = {10.1109/URAI.2011.6145869},
file = {:home/miguel/Dropbox/Mendeley Desktop/Einramhof, Schwarz, Vincze - 2011 - Range image analysis for controlling an adaptive 3D camera.pdf:pdf},
isbn = {9781457707223},
keywords = {Adaptive Camera,Foveation,Range Images},
pages = {493--494},
title = {{Range image analysis for controlling an adaptive 3D camera}},
url = {https://www.researchgate.net/publication/251573275},
year = {2011}
}
@article{Risholm2018a,
abstract = {We present a range-gated camera system designed for real-time (10 Hz) 3D estimation underwater. The system uses a fast-shutter CMOS sensor (1280×1024 ) customized to facilitate gating with 1.67 ns (18.8 cm in water) delay steps relative to the triggering of a solid-state actively ? -switched 532 nm laser. A depth estimation algorithm has been carefully designed to handle the effects of light scattering in water, i.e., forward and backward scattering. The raw range-gated signal is carefully filtered to reduce noise while preserving the signal even in the presence of unwanted backscatter. The resulting signal is proportional to the number of photons that are reflected during a small time unit (range), and objects will show up as peaks in the filtered signal. We present a peak-finding algorithm that is robust to unwanted forward scatter peaks and at the same time can pick out distant peaks that are barely higher than peaks caused by sensor and intensity noise. Super-resolution is achieved by fitting a parabola around the peak, which we show can provide depth precision below 1 cm at high signal levels. We show depth estimation results when scanning a range of 8 m (typically 1–9 m) at 10 Hz. The results are dependent on the water quality. We are capable of estimating depth at distances of over 4.5 attenuation lengths when imaging high albedo targets at low attenuation lengths, and we achieve a depth resolution (?) ranging from 0.8 to 9 cm, depending on signal level.},
author = {Risholm, Petter and Thorstensen, Jostein and Thielemann, Jens T. and Kaspersen, Kristin and Tschudi, Jon and Yates, Chris and Softley, Chris and Abrosimov, Igor and Alexander, Jonathan and Haugholt, Karl Henrik},
doi = {10.1364/AO.57.003927},
file = {:home/miguel/Dropbox/Mendeley Desktop/Risholm et al. - 2018 - Real-time super-resolved 3D in turbid water using a fast range-gated CMOS camera.pdf:pdf},
issn = {21553165},
journal = {Appl. Opt.},
keywords = {Imaging through turbid media,Range finding,Three-dimensional image acquisition,Three-dimensional sensing},
number = {14},
pages = {3927--3937},
title = {{Real-time super-resolved 3D in turbid water using a fast range-gated CMOS camera}},
url = {https://doi.org/10.1364/AO.57.003927 http://ao.osa.org/abstract.cfm?URI=ao-57-14-3927},
volume = {57},
year = {2018}
}
@misc{Warrant2004,
abstract = {The deep sea is the largest habitat on earth. Its three great faunal environments - the twilight mesopelagic zone, the dark bathypelagic zone and the vast flat expanses of the benthic habitat- are home to a rich fauna of vertebrates and invertebrates. In the mesopelagic zone (150-1000 in), the down-welling daylight creates an extended scene that becomes increasingly dimmer and bluer with depth. The available daylight also originates increasingly, from vertically above, and bioluminescent point-source flashes, well contrasted against the dim background daylight become increasingly visible. In the bathypelagic zone below 1000 m no daylight remains, and the scene becomes entirel, dominated by point-like biolumincscence. This changing nature of visual scenes with depth - from extended source to point source - has had a profound effect on the designs of deep-sea eyes, both optically and neurally, a fact that until recently was not fully appreciated. Recent measurements of the sensitivity and spatial resolution of deep-sea eyes - particularly from the camera eyes of fishes and cephalopods and the compound eyes of crustaceans - reveal that ocular designs are well matched to the nature of the visual scene at any criven depth. This match between eye design and visual scene is the subject of this review. The greatest variation eye design is found in the mesopelagic zone, where dim down-welling daylight and bioluminescent point Sources may be visible simultaneously. Some ruesopelagic eyes rely on spatial and temporal Summation to increase sensitivity to a dim extended scene, while others sacrifice this sensitivity to localise pinpoints of bright bioluminescence. Yet other eyes have retinal regions separately specialised for each type of light. In the bathypelagic zone, eyes generally get smaller and therefore less sensitive to point sources with increasing depth. In fishes, this insensitivty, combined with surprisingly high spatial resolution, is very well adapted to the detection and locallsation of point-source bioluminescence at ecologically meaningful distances. At all depths, the eyes of animals active on and over the nutrient-rich sea floor are generally larger than the eyes of pelagic species. In fishes, the retinal ganglion bells are also frequently arranged in a horizontal visual streak, an adaptation for., the wide flat horizon of the sea floor, and all animals living there. These and many other aspects of light viewing and vision in the deep sea are renewed in support of the following conclusion: it is not only the intensity of light at different depths, but also its distribution in space, which has been a major force in the evolution of deep-sea vision.},
author = {Warrant, Eric J. and Locket, N. Adam},
booktitle = {Biological Reviews of the Cambridge Philosophical Society},
doi = {10.1017/S1464793103006420},
file = {:home/miguel/Dropbox/Mendeley Desktop/Warrant, Locket - 2004 - Vision in the deep sea.pdf:pdf},
isbn = {1464-7931},
issn = {14647931},
keywords = {Bioluminescence,Cephalopod,Crustacean,Deep sea,Eye design,Fish,Natural scene,Vision,Visual ecology},
month = {aug},
number = {3},
pages = {671--712},
pmid = {15366767},
title = {{Vision in the deep sea}},
url = {http://doi.wiley.com/10.1017/S1464793103006420},
volume = {79},
year = {2004}
}
@article{Zhang2011,
abstract = {In this paper, a practical method using phase tracking and ray tracing algorithms is proposed for measuring the three-dimensional (3D) shape of an underwater object. A 2D projected sinusoidal fringe goes through the water and illuminates the tested object. Firstly, the phase tracking algorithm is employed to identify homologous points in phase distributions of the deformed fringe captured by the camera and these of the fringe pattern projected by the projector. The projector is regarded as a special camera as regards the stereovision principle. In the calibrated system, both ray directions of the homologous points can be easily figured out. Secondly, the ray tracing algorithm is used to trace the propagation path of each ray and to calculate the 3D coordinates of each point on the tested object's surface. Finally, the whole shape of the tested object can be reconstructed. {\textcopyright} 2010 Elsevier Ltd.},
author = {Zhang, Qican and Wang, Qingfeng and Hou, Zhiling and Liu, Yuankun and Su, Xianyu},
doi = {10.1016/j.optlastec.2010.11.007},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zhang et al. - 2011 - Three-dimensional shape measurement for an underwater object based on two-dimensional grating pattern projection.pdf:pdf},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {3D measurement,Fringe projection,Underwater object},
number = {4},
pages = {801--805},
title = {{Three-dimensional shape measurement for an underwater object based on two-dimensional grating pattern projection}},
url = {www.elsevier.com/locate/optlastec},
volume = {43},
year = {2011}
}
@article{Castillon2019,
abstract = {Underwater inspection, maintenance and repair (IMR) operations are being increasingly robotized in order to reduce safety issues and costs. These robotic systems rely on vision sensors to perform fundamental tasks, such as navigation and object recognition and manipulation. Especially, active optical 3D scanners are commonly used due to the domain-specific challenges of underwater imaging. This paper presents an exhaustive survey on the state of the art of optical 3D underwater scanners. A literature review on light projection and light-sensing technologies is presented. Moreover, quantitative performance comparisons of underwater 3D scanners present in the literature and commercial products are carried out.},
author = {Castill{\'{o}}n, Miguel and Palomer, Albert and Forest, Josep and Ridao, Pere},
doi = {10.3390/s19235161},
file = {:home/miguel/Dropbox/Mendeley Desktop/Castill{\'{o}}n et al. - 2019 - State of the Art of Underwater Active Optical 3D Scanners.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {3D reconstruction,Active 3D techniques,Underwater 3D laser scanners,Underwater imaging,Underwater robotics},
number = {23},
pages = {5161},
title = {{State of the Art of Underwater Active Optical 3D Scanners}},
url = {https://www.mdpi.com/1424-8220/19/23/5161},
volume = {19},
year = {2019}
}
@article{Salman2019,
author = {Salman, Ahmad and Siddiqui, Shoaib Ahmad and Shafait, Faisal and Mian, Ajmal and Shortis, Mark R and Khurshid, Khawar and Ulges, Adrian and Schwanecke, Ulrich},
doi = {10.1093/icesjms/fsz025},
file = {:home/miguel/Dropbox/Mendeley Desktop/Salman et al. - 2019 - Automatic fish detection in underwater videos by a deep neural network-based hybrid motion learning system.pdf:pdf},
issn = {1054-3139},
journal = {ICES Journal of Marine Science},
title = {{Automatic fish detection in underwater videos by a deep neural network-based hybrid motion learning system}},
url = {https://academic.oup.com/icesjms/advance-article-abstract/doi/10.1093/icesjms/fsz025/5366225},
year = {2019}
}
@inproceedings{Dalgleish2009,
abstract = {Some of the authors of this publication are also working on these related projects: in situ LiDAR for water column attenuation studies View project Observing Arctic Substrates: Unveiling ice, water column, and benthic physical and biological properties using laser remote sensing from autonomous underwater vehicles and unmanned aerial vehicles View project Fraser Dalgleish L3Harris Technologies ABSTRACT Experimental results from two alternate approaches to underwater imaging based around the well known Laser Line Scan (LLS) serial imaging technique are presented. Traditionally employing Continuous Wave (CW) laser excitation, LLS is known to improve achievable distance and image contrast in scattering-dominant waters by reducing both the backscatter and forward scatter levels reaching the optical receiver. This study involved designing and building prototype benchtop CW-LLS and pulsed-gated LLS imagers to perform a series of experiments in the Harbor Branch Oceanographic Institute (HBOI) full-scale laser imaging tank, under controlled scattering conditions using known particle suspensions. Employing fixed laser-receiver separation (24.3cm) in a bi-static optical geometry, the CW-LLS was capable of producing crisp, high contrast images at beyond 4 beam attenuation lengths at 7 meters stand-off distance. Beyond this stand-off distance or at greater turbidity, the imaging performance began to be limited mainly by multiple backscatter and shot noise generated in the receiver, eventually reaching a complete contrast limit at around 6 beam attenuation lengths. Using identical optical geometry as the CW-LLS, a pulsed-gated laser line scan (PG-LLS) system was configured and tested, demonstrating a significant reduction in the backscatter reaching the receiver. When compared with the CW-LLS at 7 meters stand-off distance, the PG-LLS did not become limited due to multiple backscatter, instead reaching a limit (believed to be primarily due to forward-scattered light overcoming the attenuated direct target signal) beyond 7 beam attenuation lengths. This result demonstrates the potential for a greater operational limit as compared to previous CW-LLS configuration.},
author = {Dalgleish, Fraser R. and Caimi, Frank M and Britton, Walter B and Andren, Carl F},
booktitle = {Ocean Sensing and Monitoring},
doi = {10.1117/12.820836},
file = {:home/miguel/Dropbox/Mendeley Desktop/Dalgleish et al. - 2009 - Improved LLS imaging performance in scattering-dominant waters.pdf:pdf},
keywords = {Lidar,image quality,laser imaging,range-gated,underwater imaging},
pages = {73170E},
title = {{Improved LLS imaging performance in scattering-dominant waters}},
url = {https://www.researchgate.net/publication/252385567},
volume = {7317},
year = {2009}
}
@article{Sansoni1999,
abstract = {A combination of phase-shift with gray-code light projection into a three-dimensional vision system based on the projection of structured light is presented. The gray-code method is exploited to detect without ambiguity even marked surface discontinuities, whereas the phase-shift technique allows the measurement of fine surface details. The system shows excellent linearity. An overall mean value of the measurement error equal to 40 microm, with a variability of approximately +/-35 microm, corresponding to 0.06{\%} of full scale, has been estimated. The implementation of the technique is discussed, the analysis of the systematic errors is presented in detail, and the calibration procedure designed to determine the optimal setting of the measurement parameters is illustrated.},
archivePrefix = {arXiv},
arxivId = {1009.1677},
author = {Sansoni, Giovanna and Carocci, Matteo and Rodella, Roberto},
doi = {10.1364/AO.38.006565},
eprint = {1009.1677},
file = {:home/miguel/Dropbox/Mendeley Desktop/Sansoni, Carocci, Rodella - 1999 - Three-dimensional vision based on a combination of gray-code and phase-shift light projection analysi.pdf:pdf},
issn = {0003-6935},
journal = {Applied Optics},
keywords = {1202830,1203930,1500150,1506910,OCIS codes: 1200120},
number = {31},
pages = {6565},
pmid = {18324191},
title = {{Three-dimensional vision based on a combination of gray-code and phase-shift light projection: analysis and compensation of the systematic errors}},
url = {https://pdfs.semanticscholar.org/3277/685d11b524c135d1625af904253f035d8970.pdf https://www.osapublishing.org/abstract.cfm?URI=ao-38-31-6565},
volume = {38},
year = {1999}
}
@inproceedings{Risholm2018,
author = {Risholm, Petter and Kirkhus, Trine and Thielemann, Jens T.},
booktitle = {OCEANS 2018 MTS/IEEE Charleston},
doi = {10.1109/OCEANS.2018.8604930},
file = {:home/miguel/Dropbox/Mendeley Desktop/Risholm, Kirkhus, Thielemann - 2018 - High-resolution structured light 3D sensor for autonomous underwater inspection.pdf:pdf},
isbn = {978-1-5386-4814-8},
month = {oct},
pages = {1--5},
publisher = {IEEE},
title = {{High-resolution structured light 3D sensor for autonomous underwater inspection}},
url = {https://ieeexplore.ieee.org/document/8604930/},
year = {2018}
}
@inproceedings{Buschinelli2016,
author = {Buschinelli, Pedro D.V. and Matos, Gabriel and Pinto, Tiago and Albertazzi, Armando},
booktitle = {OCEANS 2016 MTS/IEEE Monterey, OCE 2016},
doi = {10.1109/OCEANS.2016.7761231},
file = {:home/miguel/Dropbox/Mendeley Desktop/Buschinelli et al. - 2016 - Underwater 3D shape measurement using inverse triangulation through two flat refractive surfaces.pdf:pdf},
isbn = {9781509015375},
keywords = {3D shape measurement,Photogrammetry,Stereo vision,Structured light,Underwater measurement},
month = {sep},
pages = {1--7},
publisher = {IEEE},
title = {{Underwater 3D shape measurement using inverse triangulation through two flat refractive surfaces}},
url = {http://ieeexplore.ieee.org/document/7761231/},
year = {2016}
}
@inproceedings{Finkelstein2006,
abstract = {We demonstrate a new single-photon avalanche diode (SPAD) device, which utilizes the silicon-dioxide shallow-trench isolation (STI) structure common to all deep-submicron CMOS technologies, both for junction planarization and as an area-efficient guard-ring. This makes it possible to achieve an order-of-magnitude improvement in fill factor and a significant reduction in pixel area compared with existing CMOS SPADs, and results in improved SPAD performance. We present numerical simulations as well preliminary experimental results from a test chip, which was manufactured in an IBM 0.18 m CMOS technology, and which incorporates the devices. With these new and efficient structures, 12 m-pitch pixels with sub-10ns dead times are achievable without requiring active recharge, creating the opportunity to integrate large arrays of these ultra-fast SPADs for use in biological imaging systems.},
author = {Finkelstein, Hod and Hsu, Mark J and Esener, Sadik},
booktitle = {Advanced Photon Counting Techniques},
doi = {10.1117/12.705259},
file = {:home/miguel/Dropbox/Mendeley Desktop/Finkelstein, Hsu, Esener - 2006 - An ultrafast Geiger-mode single-photon avalanche diode in 0.18-$\mu$m CMOS technology.pdf:pdf},
keywords = {Avalanche breakdown,Avalanche photodiodes,Biological imaging,Photodetectors},
pages = {63720W},
title = {{An ultrafast Geiger-mode single-photon avalanche diode in 0.18-$\mu$m CMOS technology}},
url = {https://www.bioee.ee.columbia.edu/courses/upload/Bibliography/finkelstein{\_}0.18.pdf},
volume = {6372},
year = {2006}
}
@article{Brauer-Burchardt2015,
abstract = {In this work we show the principle of optical 3D surface measurements based on the fringe projection technique for underwater applications. The challenges of underwater use of this technique are shown and discussed in comparison with the classical application. We describe an extended camera model which takes refraction effects into account as well as a proposal of an effective, low-effort calibration procedure for underwater optical stereo scanners. This calibration technique combines a classical air calibration based on the pinhole model with ray-based modeling and requires only a few underwater recordings of an object of known length and a planar surface. We demonstrate a new underwater 3D scanning device based on the fringe projection technique. It has a weight of about 10 kg and the maximal water depth for application of the scanner is 40 m. It covers an underwater measurement volume of 250 mm × 200 mm × 120 mm. The surface of the measurement objects is captured with a lateral resolution of 150 $\mu$m in a third of a second. Calibration evaluation results are presented and examples of first underwater measurements are given.},
author = {Br{\"{a}}uer-Burchardt, Christian and Heinze, Matthias and Schmidt, Ingo and K{\"{u}}hmstedt, Peter and Notni, Gunther},
doi = {10.3390/s16010013},
file = {:home/miguel/Dropbox/Mendeley Desktop/Br{\"{a}}uer-Burchardt et al. - 2015 - Underwater 3D surface measurement using fringe projection based scanning devices.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Fringe projection,Underwater 3D-scanner,Underwater stereo camera calibration},
month = {dec},
number = {1},
pages = {13},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Underwater 3D surface measurement using fringe projection based scanning devices}},
url = {http://www.mdpi.com/1424-8220/16/1/13},
volume = {16},
year = {2015}
}
@article{Nassalski2010,
abstract = {The performance of multi pixel photon counters (MPPC) of 3 mm × 3 mm size, with 14400 and 3600 pixels, were studied by means of the signal from a laser light pulser and using the 3 mm × 3 mm × 20 mm LSO pixel scintillator. Special attention was paid to measure number of fired pixels, generated by the light of pulser and that of the LSO crystal, using a direct method of a comparison of the light peak position in the pulse height spectrum with that of the single photoelectron. The tests of the LSO crystal showed 1550 ± 80 fired pixels per MeV in the MPPC with 14400 pixels assuring a good linearity of the response up to about 1 MeV energy of gamma rays absorbed in the LSO crystal. Energy resolution of 14.8{\%} for 662 keV gamma rays from137Cs source and a time resolution of about 850 ps for 511 keV annihilation quanta were limited by a rather low number of the fired pixels compared to the number of photoelectrons in photomultipliers. {\textcopyright} 2010 IEEE.},
author = {Nassalski, A. and Moszy{\'{n}}ski, M. and Syntfeld-Kazuch, A. and Szcze{\'{s}}niak, T. and {\'{S}}widerski and Wolski, D. and Batsch, T. and Baszak, J.},
doi = {10.1109/TNS.2010.2044586},
file = {:home/miguel/Dropbox/Mendeley Desktop/Nassalski et al. - 2010 - Multi pixel photon counters (MPPC) as an alternative to APD in PET applications.pdf:pdf},
issn = {00189499},
journal = {IEEE Transactions on Nuclear Science},
keywords = {Detector linearity,Multi pixel photon counters (MPPC),Scintillation detection},
number = {3 PART 1},
pages = {1008--1014},
title = {{Multi pixel photon counters (MPPC) as an alternative to APD in PET applications}},
volume = {57},
year = {2010}
}
@misc{Vazquez-Arellano2016,
abstract = {Efficiency increase of resources through automation of agriculture requires more information about the production process, as well as process and machinery status. Sensors are necessary for monitoring the status and condition of production by recognizing the surrounding structures such as objects, field structures, natural or artificial markers, and obstacles. Currently, three dimensional (3-D) sensors are economically affordable and technologically advanced to a great extent, so a breakthrough is already possible if enough research projects are commercialized. The aim of this review paper is to investigate the state-of-the-art of 3-D vision systems in agriculture, and the role and value that only 3-D data can have to provide information about environmental structures based on the recent progress in optical 3-D sensors. The structure of this research consists of an overview of the different optical 3-D vision techniques, based on the basic principles. Afterwards, their application in agriculture are reviewed. The main focus lays on vehicle navigation, and crop and animal husbandry. The depth dimension brought by 3-D sensors provides key information that greatly facilitates the implementation of automation and robotics in agriculture.},
author = {V{\'{a}}zquez-Arellano, Manuel and Griepentrog, Hans W and Reiser, David and Paraforos, Dimitris S},
booktitle = {Sensors},
doi = {10.3390/s16050618},
file = {:home/miguel/Dropbox/Mendeley Desktop/V{\'{a}}zquez-Arellano et al. - 2016 - 3-D imaging systems for agricultural applications—a review.pdf:pdf},
isbn = {9783540367116},
issn = {14248220},
keywords = {3-D sensors,Agricultural automation,Agricultural robotics,Interferometry,Optical triangulation,Time-of-flight},
number = {5},
pmid = {27136560},
title = {{3-D imaging systems for agricultural applications—a review}},
url = {www.mdpi.com/journal/sensors},
volume = {16},
year = {2016}
}
@article{Duda2013,
abstract = {Structure from motion and structure from light are usually regarded separately. To combine both methods, using the same images to retrieve three dimensional information about the environment, both methods must work side by side without interfering each other. This research work describes how laser line patterns can be extracted from images while being robust to scattering media and ambient light.},
author = {Duda, Alexander and Albiez, Jan},
file = {:home/miguel/Dropbox/Mendeley Desktop/Duda, Albiez - 2013 - Back Projection Algorithm for Line Structured Light Extraction.pdf:pdf},
journal = {2013 OCEANS-San Diego},
keywords = {3-d structure,ransac,sift tracker,singular value decomposition,svd,tomsi-kanade factorization method},
pages = {1----7},
title = {{Back Projection Algorithm for Line Structured Light Extraction}},
url = {https://www.semanticscholar.org/paper/Back-projection-algorithm-for-line-structured-light-Duda-Albiez/424d79938301fc9b310cdeac0b626631b61a396e http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Simple+Method+to+Recover+3-D+Rigid+Structure+fro},
year = {2013}
}
@article{He2018,
abstract = {In this article, we survey the recent developments of sensing methods in three-dimensional robot vision, centering on the current three-dimensional sensors and core techniques embedded in robotic systems. Over 8000 publications have reported rather wide application areas of three-dimensional robot vision in the last 40 years, such as human–robot interaction, object recognition, three-dimensional modeling, object tracking, searching and surveillance, as well as robot manipulation, localization, navigation, mapping, and path planning. Representative works and future research trends are also addressed in this article.},
author = {He, Yu and Chen, Shengyong},
doi = {10.1177/1729881418760623},
file = {:home/miguel/Dropbox/Mendeley Desktop/He, Chen - 2018 - Advances in sensing and processing methods for three-dimensional robot vision.pdf:pdf},
issn = {17298814},
journal = {International Journal of Advanced Robotic Systems},
keywords = {3D robot vision,3D sensors,Object representation,SLAM,Sensing technique},
number = {2},
title = {{Advances in sensing and processing methods for three-dimensional robot vision}},
url = {https://us.sagepub.com/en-us/nam/},
volume = {15},
year = {2018}
}
@book{Schilling1996,
author = {Schilling, Robert J},
file = {:home/miguel/Dropbox/Mendeley Desktop/Schilling - 1996 - Fundamentals of robotics analysis and control.pdf:pdf},
publisher = {Simon {\&} Schuster Trade},
title = {{Fundamentals of robotics: analysis and control}},
year = {1996}
}
@incollection{Gracias2017,
address = {Chichester, UK},
author = {Gracias, Nuno and Garc{\'{i}}a, Rafael and Campos, Ricard and Hurt{\'{o}}s, Natalia and Prados, Ricard and Shihavuddin, ASM and Nicosevici, Tudor and Elibol, Armagan and Neumann, Laszlo and Escartin, Javier},
booktitle = {Computer Vision in Vehicle Technology},
doi = {10.1002/9781118868065.ch7},
file = {:home/miguel/Dropbox/Mendeley Desktop/Gracias et al. - 2017 - Application Challenges of Underwater Vision.pdf:pdf},
keywords = {2D mosaics,image registration,inspection applications,navigation data,optical mapping data sets,planar transformation,topology estimation,underwater vehicles},
month = {feb},
pages = {133--160},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Application Challenges of Underwater Vision}},
url = {http://doi.wiley.com/10.1002/9781118868065.ch7},
year = {2017}
}
@inproceedings{Pathak2010,
abstract = {Surface-patches based 3D mapping in a real world underwater scenario is presented. It is based on a 6 degrees of freedom registration of sonar data. Planar surfaces are fitted into the sonar data and the subsequent registration method maximizes the overall geometric consistency within a search-space to determine correspondences between the planes. This approach has previously only been used on high quality range data from sensors on land robots like laser range finders. It is shown here that the algorithm is also applicable to very noisy, coarse sonar data. The 3D map presented is of a large underwater structure, namely the Lesumer Sperrwerk, a flood gate north of the city of Bremen, Germany. It is generated from 18 scans collected using a Tritech Eclipse sonar. {\textcopyright}2010 IEEE.},
author = {Pathak, Kaustubh and Birk, Andreas and Vaskevicius, Narunas},
booktitle = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
doi = {10.1109/IROS.2010.5650953},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pathak, Birk, Vaskevicius - 2010 - Plane-based registration of sonar data for underwater 3D mapping.pdf:pdf},
isbn = {9781424466757},
month = {oct},
pages = {4880--4885},
publisher = {IEEE},
title = {{Plane-based registration of sonar data for underwater 3D mapping}},
url = {http://ieeexplore.ieee.org/document/5650953/},
year = {2010}
}
@phdthesis{Frigerio2006,
abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 2006.},
author = {Frigerio, Federico},
file = {:home/miguel/Dropbox/Mendeley Desktop/Frigerio - 2006 - 3-dimensional surface imaging using Active Wavefront Sampling.pdf:pdf},
keywords = {Mechanical Engineering.,Thesis},
publisher = {Massachusetts Institute of Technology},
school = {Massachusetts Institute of Technology},
title = {{3-dimensional surface imaging using Active Wavefront Sampling}},
url = {https://dspace.mit.edu/handle/1721.1/38258},
year = {2006}
}
@misc{Tanabashi2018,
abstract = {The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 3,062 new measurements from 721 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as supersymmetric particles, heavy bosons, axions, dark photons, etc. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as Higgs Boson Physics, Supersymmetry, Grand Unified Theories, Neutrino Mixing, Dark Energy, Dark Matter, Cosmology, Particle Detectors, Colliders, Probability and Statistics. Among the 117 reviews are many that are new or heavily revised, including those on Pentaquarks and Inflation. The complete Review is published online in a journal and on the website of the Particle Data Group ( http://pdg.lbl.gov [http://pdg.lbl.gov] ). The printed PDG Book contains the Summary Tables and all review articles but no longer includes the detailed tables from the Particle Listings. A Booklet with the Summary Tables and abbreviated versions of some of the review articles is also available. ----- -- Contents ----- -- ----- -- Abstract, Contributors, Highlights and Table of Contents -- Acrobat PDF (150 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0001-0010.pdf] ----- -- ----- -- Introduction -- Acrobat PDF (456 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0011-0026.pdf] ----- -- ----- -- Particle Physics Summary Tables ----- -- Gauge and Higgs bosons -- Acrobat PDF (155 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0029-0031.pdf] ----- -- Leptons -- Acrobat PDF (134 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0032-0035.pdf] ----- -- Quarks -- Acrobat PDF (84 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0036.pdf] ----- -- Mesons -- Acrobat PDF (871 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0037-0086.pdf] ----- -- Baryons -- Acrobat PDF (300 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0087-0102.pdf] ----- -- Searches (Supersymmetry, Compositeness, etc.) -- Acrobat PDF (91 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0103-0104.pdf] ----- -- Tests of conservation laws -- Acrobat PDF (330 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0105-0116.pdf] ----- -- ----- -- Reviews, Tables, and Plots ----- -- Detailed contents for this section -- Acrobat PDF (37 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0117-0118.pdf] ----- -- Constants, Units, Atomic and Nuclear Properties -- Acrobat PDF (278 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0119-0131.pdf] ----- -- Standard Model and Related Topics -- Acrobat PDF (7.3 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0132-0348.pdf] ----- -- Astrophysics and Cosmology -- Acrobat PDF (2.7 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0349-0428.pdf] ----- -- Experimental Methods and Colliders -- Acrobat PDF (3.8 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0429-0516.pdf] ----- -- Mathematical Tools or Statistics, Monte Carlo, GroupTheory -- Acrobat PDF (1.3 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0517-0559.pdf] ----- -- Kinematics, Cross-Section Formulae, and Plots -- Acrobat PDF (3.9 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0560-0598.pdf] ----- -- ----- -- Particle Listings ----- -- Illustrative key and abbreviations -- Acrobat PDF (235 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0601-0610.pdf] ----- -- Gauge and Higgs bosons -- Acrobat PDF (2 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0613-0710.pdf] ----- -- Leptons -- Acrobat PDF (1.5 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0711-0790.pdf] ----- -- Quarks -- Acrobat PDF (1.2 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0791-0846.pdf] ----- -- Mesons: Light unflavored and strange -- Acrobat PDF (4 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}0847-1043.pdf] ----- -- Mesons: Charmed and bottom -- Acrobat PDF (7.4 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}1044-1363.pdf] ----- -- Mesons: Other -- Acrobat PDF (3.1 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}1364-1500.pdf] ----- -- Baryons -- Acrobat PDF (3.97 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}1501-1672.pdf] ----- -- Miscellaneous searches -- Acrobat PDF (2.4 MB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}1673-1790.pdf] ----- -- ----- -- Index -- Acrobat PDF (160 KB) [http://iopscience.iop.org/1674-1137/40/10/100001/media/rpp2016{\_}1791-1808.pdf] -----},
author = {Tanabashi, M. and Hagiwara, K. and Hikasa, K.},
booktitle = {Physical Review D},
doi = {10.1103/PhysRevD.98.030001},
issn = {24700029},
month = {aug},
number = {3},
pages = {030001},
publisher = {American Physical Society},
title = {{Review of Particle Physics}},
url = {https://link.aps.org/doi/10.1103/PhysRevD.98.030001},
volume = {98},
year = {2018}
}
@inproceedings{Menna2017,
abstract = {Underwater photogrammetry, like its counterpart in 'air', has gained an increasing diffusion thanks to the availability of easy-to-use, fast and often quite inexpensive software applications. Moreover, underwater equipment that allows the use of digital cameras normally designed to work in air also in water are largely available. However, for assuring accurate and reliable 3D modelling results a profound knowledge of the employed devices as well as physical and geometric principle is even more crucial than in air. This study aims to take a step forward in understanding the effect of underwater ports in front of the photographic lens. In particular, the effect of dome or flat ports on image quality in 3D modelling applications is investigated. Experiments conducted in a semi submerged indust rial structure show that the tested flat port performs worse than the dome, providing higher image residuals and lower precision and accuracy in object space. A significant different quality per colour channel is also observed and its influence on achievable processing results is discussed.},
author = {Menna, Fabio and Nocerino, Erica and Remondino, Fabio},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W3-481-2017},
file = {:home/miguel/Dropbox/Mendeley Desktop/Menna, Nocerino, Remondino - 2017 - Flat versus hemispherical dome ports in underwater photogrammetry.pdf:pdf},
issn = {16821750},
keywords = {Camera calibration,Flat port,Hemispherical dome port,MTF,Photogrammetry,Underwater},
number = {2W3},
pages = {481--487},
title = {{Flat versus hemispherical dome ports in underwater photogrammetry}},
url = {https://www.researchgate.net/profile/Fabio{\_}Remondino/publication/313943838{\_}FLAT{\_}VERSUS{\_}HEMISPHERICAL{\_}DOME{\_}PORTS{\_}IN{\_}UNDERWATER{\_}PHOTOGRAMMETRY/links/58cf84a2aca272335517ef42/FLAT-VERSUS-HEMISPHERICAL-DOME-PORTS-IN-UNDERWATER-PHOTOGRAMMETRY.pdf},
volume = {42},
year = {2017}
}
@inproceedings{Caimi1997,
abstract = {Recently developed undersea imaging systems are cable of providing three-dimensional surface maps of the image space using a scanning laser configuration. Triangulation methods, whereby the scene is viewed from a separate location, provide depth information, while instantaneous position of the laser scanning elements are used to estimate the lateral position in object space. A specially developed detector provides an approximate position for the apparent landing spot of the laser beam for each scan angle, which in turn, is used to compute an estimated range value in real time. Several prototype systems constructed using these techniques are in a testing phase. One system reported here is designed to scan a 10 by 10 degree field-of-view with a 10 milliwatt laser at distances from 20 to 40 centimeters with a resolution of less than 1 millimeter.},
annote = {Albert: "only three published works use a triangulation method with a steered laser"},
author = {Caimi, Frank Michael and Kocak, Donna M.},
doi = {10.1117/12.277754},
editor = {Beiser, Leo and Sagan, Stephen F.},
month = {jul},
pages = {241--252},
publisher = {International Society for Optics and Photonics},
title = {{Real-time 3D underwater imaging and mapping using a laser line scan technique}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=929417},
volume = {3131},
year = {1997}
}
@article{Zohrabi2019,
abstract = {A light detection and ranging (lidar) system with ±90 • of steering based on an adaptive electrowetting-based prism for nonmechanical beam steering has been demonstrated. Electrowetting-based prisms provide a transmissive, low power, and compact alternative to conventional adaptive optics as a nonmechanical beam scanner. The electrowetting prism has a steering range of ±7.8 •. We demonstrate a method to amplify the scan angle to ±90 • and perform a one-dimensional scan in a lidar system.},
author = {Zohrabi, Mo and Cormack, Robert H and Supekar, Omkar D and Lim, Wei Yang and Gopinath, Juliet T. and Bright, Victor M},
doi = {10.1364/oe.27.004404},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zohrabi et al. - 2019 - Lidar system with nonmechanical electrowetting-based wide-angle beam steering.pdf:pdf},
issn = {10944087},
journal = {Optics Express},
number = {4},
pages = {4404},
title = {{Lidar system with nonmechanical electrowetting-based wide-angle beam steering}},
url = {https://doi.org/10.1364/OE.27.004404},
volume = {27},
year = {2019}
}
@phdthesis{Lange2000,
author = {Lange, Robert},
file = {:home/miguel/Dropbox/Mendeley Desktop/Lange - 2000 - 3D Time-of-flight distance measurement with custom solid-state image sensors in CMOSCCD-technology.pdf:pdf},
school = {University of Siegen},
title = {{3D Time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-technology}},
url = {https://dokumentix.ub.uni-siegen.de/opus/volltexte/2006/178/pdf/lange.pdf},
year = {2000}
}
@inproceedings{Yang2013,
abstract = {Laser line scan (LLS) is one of the underwater optical imaging methods, which can reduce the detrimental effects of backscatter and forward scatter. This paper put forward a 3D reconstruction method based on the LLS system, for the purpose of underwater surveying. By direct camera calibration, a coordinate mapping relationship between 2D pix coordinate and 3D world coordinate is established, which then can be used to reconstruct 3D objects from 2D scan data. Experiments had been carried out in a pool, and underwater 3D scene can be reconstructed at millimeter scale, which demonstrated that this method can be used to underwater surveying.},
author = {Yang, Yu and Zheng, Bing and Zheng, Hai Yong and Wang, Zi Tao and Wu, Guo Shuai and Wang, Jin Cheng},
booktitle = {OCEANS 2013 MTS/IEEE Bergen: The Challenges of the Northern Dimension},
doi = {10.1109/OCEANS-Bergen.2013.6607973},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yang et al. - 2013 - 3D reconstruction for underwater laser line scanning.pdf:pdf},
isbn = {9781479900015},
keywords = {3D reconstruction,laser line scan,underwater imaging},
month = {jun},
pages = {1--3},
publisher = {IEEE},
title = {{3D reconstruction for underwater laser line scanning}},
url = {http://ieeexplore.ieee.org/document/6607973/},
year = {2013}
}
@article{Pope1997,
abstract = {Definitive data on the absorption spectrum of pure water from 380 to 700 nm have been obtained with an integrating cavity technique. The results are in good agreement with those recently obtained by our group with a completely independent photothermal technique. As before, we find that the absorption in the blue is significantly lower than had previously been generally believed and that the absorption minimum is at a significantly shorter wavelength, i.e., 0.0044 ? 0.0006 m(-1) at 418 nm. Several spectroscopic features have been identified in the visible spectrum to our knowledge for the first time.},
author = {Pope, Robin M and Fry, Edward S},
doi = {10.1364/ao.36.008710},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pope, Fry - 1997 - Absorption spectrum (380–700 nm) of pure water II Integrating cavity measurements.pdf:pdf},
isbn = {0003-6935},
issn = {0003-6935},
journal = {Applied Optics},
keywords = {Absorption,integrating cavity,ocean water,spectroscopy,water},
number = {33},
pages = {8710----8723},
pmid = {18264420},
title = {{Absorption spectrum (380–700 nm) of pure water II Integrating cavity measurements}},
url = {https://www.osapublishing.org/DirectPDFAccess/B395E194-E678-D8CB-E2E553B8BB0AB502{\_}63107/ao-36-33-8710.pdf?da=1{\&}id=63107{\&}seq=0{\&}mobile=no},
volume = {36},
year = {1997}
}
@inproceedings{Kim2014,
abstract = {An event camera is a silicon retina which outputs not a sequence of video frames like a standard camera, but a stream of asynchronous spikes, each with pixel location, sign and precise timing, indicating when individual pixels record a threshold log intensity change. By encoding only image change, it offers the potential to transmit the informa- tion in a standard video but at vastly reduced bitrate, and with huge added advantages of very high dynamic range and temporal resolution. However, event data calls for new algorithms, and in particular we believe that algorithms which incrementally estimate global scene models are best placed to take full advantages of its properties. Here, we show for the first time that an event stream, with no additional sensing, can be used to track accurate camera rotation while building a persistent and high quality mosaic of a scene which is super-resolution accurate and has high dynamic range. Our method involves parallel camera rotation tracking and template reconstruction from estimated gradients, both operating on an event-by-event basis and based on probabilistic filtering.},
author = {Kim, Hanme and Handa, Ankur and Benosman, Ryad and Ieng, Sio-Hoi and Davison, Andrew},
booktitle = {Proceedings of the British Machine Vision Conference},
doi = {10.5244/c.28.26},
editor = {Valstar, Michel and French, Andrew and Pridmore, Tony},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kim et al. - 2014 - Simultaneous Mosaicing and Tracking with an Event Camera.pdf:pdf},
publisher = {BMVA Press},
title = {{Simultaneous Mosaicing and Tracking with an Event Camera}},
url = {https://www.doc.ic.ac.uk/{~}ajd/Publications/kim{\_}etal{\_}bmvc2014.pdf},
year = {2014}
}
@inproceedings{Jokinen1999,
abstract = {A novel method is proposed for refining the calibration of a light striping system including a projective transformation between the image plane of the camera and the plane of the laser sheet, and also the direction of the scanning with respect to the plane of the laser sheet. The refinement is obtained through weighted least squares matching of multiple profile maps acquired from different viewpoints and registered previously using an approximate calibration. Testing with synthetically generated profile maps shows that if the geometry of the object is appropriate and the registration parameters and the intrinsic parameters of the system are known exactly, then a calibration accuracy of 0.003...0.00003{\%} relative to the scene dimensions can be achieved as the average noise level in the maps used for the calibration decreases from 0.3 down to zero pixels. It is also possible to adjust several calibrations at the same time. The registration and calibration parameters can be refined simultaneously, but a close initial estimate and rather complex object geometry are needed for an accuracy of 0.03{\%} when the average noise level is 0.03 pixels. Determining the corresponding points by interpolation on the parametric domains of the maps yields higher accuracy than perpendicular projection to the tangent planes at the closest points in 3D in both registration and calibration tasks. The highest accuracy is achieved when the interpolation errors are as equal as possible within the overlapping areas},
author = {Jokinen, Olli},
booktitle = {Proceedings - 2nd International Conference on 3-D Digital Imaging and Modeling, 3DIM 1999},
doi = {10.1109/IM.1999.805348},
file = {:home/miguel/Dropbox/Mendeley Desktop/Jokinen - 1999 - Self-calibration of a light striping system by matching multiple 3-D profile maps.pdf:pdf},
isbn = {0769500625},
pages = {180--190},
title = {{Self-calibration of a light striping system by matching multiple 3-D profile maps}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.1272{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@inproceedings{Kohlbrecher2011,
abstract = {For many applications in Urban Search and Rescue (USAR) scenarios robots need to learn a map of unknown environments. We present a system for fast online learning of occupancy grid maps requiring low computational resources. It combines a robust scan matching approach using a LIDAR system with a 3D attitude estimation system based on inertial sensing. By using a fast approximation of map gradients and a multi-resolution grid, reliable localization and mapping capabilities in a variety of challenging environments are realized. Multiple datasets showing the applicability in an embedded hand-held mapping system are provided. We show that the system is sufficiently accurate as to not require explicit loop closing techniques in the considered scenarios. The software is available as an open source package for ROS. {\textcopyright} 2011 IEEE.},
author = {Kohlbrecher, Stefan and {Von Stryk}, Oskar and Meyer, Johannes and Klingauf, Uwe},
booktitle = {9th IEEE International Symposium on Safety, Security, and Rescue Robotics, SSRR 2011},
doi = {10.1109/SSRR.2011.6106777},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kohlbrecher et al. - 2011 - A flexible and scalable SLAM system with full 3D motion estimation.pdf:pdf},
isbn = {9781612847696},
keywords = {Inertial Navigation,Robust and Fast Localization,Simultaneous Localization and Mapping},
pages = {155--160},
title = {{A flexible and scalable SLAM system with full 3D motion estimation}},
url = {https://www.researchgate.net/publication/228852006},
year = {2011}
}
@article{Falanga2019,
abstract = {In this letter, we study the effects that perception latency has on the maximum speed a robot can reach to safely navigate through an unknown cluttered environment. We provide a general analysis that can serve as a baseline for future quantitative reasoning for design tradeoffs in autonomous robot navigation. We consider the case where the robot is modeled as a linear second-order system with bounded input and navigates through static obstacles. Also, we focus on a scenario where the robot wants to reach a target destination in as little time as possible, and therefore cannot change its longitudinal velocity to avoid obstacles. We show how the maximum latency that the robot can tolerate to guarantee safety is related to the desired speed, the range of its sensing pipeline, and the actuation limitations of the platform (i.e., the maximum acceleration it can produce). As a particular case study, we compare monocular and stereo frame-based cameras against novel, low-latency sensors, such as event cameras, in the case of quadrotor flight. To validate our analysis, we conduct experiments on a quadrotor platform equipped with an event camera to detect and avoid obstacles thrown towards the robot. To the best of our knowledge, this is the first theoretical work in which perception and actuation limitations are jointly considered to study the performance of a robotic platform in high-speed navigation.},
author = {Falanga, Davide and Kim, Suseong and Scaramuzza, Davide},
doi = {10.1109/LRA.2019.2898117},
file = {:home/miguel/Dropbox/Mendeley Desktop/Falanga, Kim, Scaramuzza - 2019 - How Fast Is Too Fast The Role of Perception Latency in High-Speed Sense and Avoid.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Collision avoidance,aerial systems: perception and autonomy,visual-based navigation},
month = {apr},
number = {2},
pages = {1884--1891},
title = {{How Fast Is Too Fast? The Role of Perception Latency in High-Speed Sense and Avoid}},
url = {https://ieeexplore.ieee.org/document/8636976/},
volume = {4},
year = {2019}
}
@inproceedings{Liljeback2017,
abstract = {Eelume signifies a leap in subsea robotics capabilities. It is a modular, flexible robot capable of swimming like a snake, or being propelled by conventional thrusters. Eelume is designed to reside subsea to provide immediate response to unpredicted inspection, maintenance and repair (IMR) requirements. {\textcopyright} 2017 IEEE.},
author = {Liljeb{\"{a}}ck, Pal and Mills, Richard},
booktitle = {OCEANS 2017 - Aberdeen},
doi = {10.1109/OCEANSE.2017.8084826},
file = {:home/miguel/Dropbox/Mendeley Desktop/Liljeb{\"{a}}ck, Mills - 2017 - Eelume A flexible and subsea resident IMR vehicle.pdf:pdf},
isbn = {9781509052783},
month = {jun},
pages = {1--4},
publisher = {IEEE},
title = {{Eelume: A flexible and subsea resident IMR vehicle}},
url = {http://ieeexplore.ieee.org/document/8084826/},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{Sandner2010,
abstract = {Traditional laser scanners for 3D distance measurement involve expensive, heavy, (potentially) slow rotating mirrors for light deflection of the scanning TOF (time of flight) distance measurement, not suitable for compact, robust and highly portable LIDAR system. On the other hand MEMS scanners are limited to small apertures not suitable for a precise TOF measurement. To overcome this problem Fraunhofer IPMS presents a large aperture 1D-MEMS scanner array especially designed for LIDAR applications. It is composed of 2 × 7 silicon mirror elements each having an identical design with comparatively large aperture of 2.51 × 9.51mm2 and ±30° optical scan range. All mirrors are driven electrostatically resonant with identical frequency close to design frequency of 250 Hz. By driving control all single scanner elements are synchronized to identical phase and amplitude in respect to a master scanner. This results in a large effective scanner aperture of 334 mm2 for the receiver optics and a filling factor of 80 {\%}. To guarantee the synchronized operation the paper discusses in detail the scanner design to enable a sufficiently large frequency bandwidth of all scanner elements to the compensate frequency tolerances caused by fabrication and packaging. In comparison to LIDAR systems with conventional scanner components, the large aperture 1D-MEMS scanner array enables 3D-LIDAR systems to become significantly smaller, more robust and (potentially) less expensive, also higher scan rates can be realized without additional efforts (e.g. no air bearings are needed).},
author = {Sandner, Thilo and Grasshoff, Thomas and Wildenhain, Michael and Schenk, Harald},
doi = {10.1117/12.844923},
editor = {Schenk, Harald and Piyawattanametha, Wibool},
isbn = {9780819479907},
issn = {0277786X},
keywords = {3D distance measurement,LIDAR,MOEMS,large aperture,micro mirror array,scanner synchronization,scanning micro mirror},
month = {feb},
pages = {75940C},
publisher = {International Society for Optics and Photonics},
title = {{Synchronized microscanner array for large aperture receiver optics of LIDAR systems}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.844923},
volume = {7594},
year = {2010}
}
@inproceedings{Abdelsalam2017,
abstract = {{\textcopyright} 2017 SPIE. In this work, we present a novel and simple optical solution for MEMS LiDARs. The idea is based on increasing the collection optics throughput by removing the MEMS mirror from the path of the collected light, while inserting a multi-segment tapered structure to collect the light from a wide angle. The tapered also converts the large size optical spot captured to a small area compatible with the requirement of low detector noise dimensions. The expected improvement in the collected power is analyzed versus the tapering angle of a single tapered structure. A multi-segment optical system, or multiple tapered structure arranged in parallel, is also introduced allowing for the optimization of the acceptance angle and the power improvement ratio. Using a 3-segment mirror, the expected improvement is about 15x with an acceptance angle of ±30 degrees. The design of a single element taper section is fabricated using aluminum-coated acrylic and tested experimentally showing an improvement of about 7x in the coupled power through an angle of ±10 degrees in good agreement with the theoretical expectations.},
author = {Abdelsalam, Mostafa and Sabry, Yasser and Erfan, Mazen and Khalil, Diaa},
doi = {10.1117/12.2251330},
editor = {Hemmati, Hamid and Boroson, Don M.},
isbn = {9781510606333},
issn = {1996756X},
keywords = {Autonomous vehicles,LiDAR System,MEMS Technology,Multi-segment optical system,Spot size converter,Tapered optical mirror,Wide angle optical receiver},
month = {feb},
pages = {100960O},
publisher = {International Society for Optics and Photonics},
title = {{Multi-segment tapered optical mirror for MEMS LiDAR application}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2251330},
volume = {10096},
year = {2017}
}
@article{Wang2018a,
abstract = {A handheld 3D laser scanning system is proposed for measuring large-sized objects on site. This system is mainly composed of two CCD cameras and a line laser projector, in which the two CCD cameras constitute a binocular stereo vision system to locate the scanner's position in the fixed workpiece coordinate system online, meanwhile the left CCD camera and the laser line projector constitute a structured light system to get the laser lines modulated by the workpiece features. The marked points and laser line are both obtained in the coordinate system of the left camera in each moment. To get the workpiece outline, the handheld scanner's position is evaluated online by matching up the marked points got by the binocular stereo vision system and those in the workpiece coordinate system measured by a TRITOP system beforehand; then the laser line with workpiece's features got at this moment is transformed into the fixed workpiece coordinate system. Finally, the 3D information composed by the laser lines can be reconstructed in the workpiece coordinate system. A ball arm with two standard balls, which is placed on a glass plate with many marked points randomly stuck on, is measured to test the system accuracy. The distance errors between the two balls are within ±0.05 mm, the radius errors of the two balls are all within ±0.04 mm, the distance errors from the scatter points to the fitted sphere are distributed evenly, within ±0.25 mm, without accumulated errors. Measurement results of two typical workpieces show that the system can measure large-sized objects completely with acceptable accuracy and have the advantage of avoiding some deficiencies, such as sheltering and limited measuring range.},
author = {Wang, Xiaomin and Xie, Zexiao and Wang, Kun and Zhou, Liqin},
doi = {10.3390/s18103567},
file = {:home/miguel/Dropbox/Mendeley Desktop/Wang et al. - 2018 - Research on a handheld 3D laser scanning system for measuring large-sized objects.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Binocular stereo vision,Handheld 3D measurement system,Large-sized object,Measurement on site,Structured light},
month = {oct},
number = {10},
pages = {3567},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Research on a handheld 3D laser scanning system for measuring large-sized objects}},
url = {http://www.mdpi.com/1424-8220/18/10/3567},
volume = {18},
year = {2018}
}
@article{Tan2005,
abstract = {Range-gated imaging system improves signal-to-backscattering noise ratio (SBR) by rejecting backscattered light from the target irradiance. This is achieved by synchronizing the arrival of pulsed target irradiance with the gating of an intensified camera. Witherspoon and Holloway (Ocean Optics X 1302 (1990) 414-420) indicated that the image quality of range-gated imaging system might be further improved by a delay in the camera gating towards the tail of reflected image temporal profile (RITP). This phenomena has, however, not been further elaborated. This paper extends on Witherspoon's observation. The MCRITP (Monte Carlo for RITP) algorithm is validated with theoretical and experimental results for medium attenuation coefficients of 0.26-5.90/m. By temporal convolution of the simulated results with outgoing Gaussian light pulse, Tail RITP region shows more reduction in the unwanted backscatter effects than the target intensity. This can be observed as an improvement in image contrast and a modified fidelity index (MF). {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
author = {Tan, Ching Seong and Seet, Gerald G.L. and Sluzek, Andrzej and He, Duo Min},
doi = {10.1016/j.optlaseng.2004.10.005},
file = {:home/miguel/Dropbox/Mendeley Desktop/Tan et al. - 2005 - A novel application of range-gated underwater laser imaging system (ULIS) in near-target turbid medium.pdf:pdf},
issn = {01438166},
journal = {Optics and Lasers in Engineering},
keywords = {Backscattering,Monte Carlo,Range-gated,Reflected image temporal profile,ULIS},
number = {9},
pages = {995--1009},
title = {{A novel application of range-gated underwater laser imaging system (ULIS) in near-target turbid medium}},
url = {https://pdf.sciencedirectassets.com/271471/1-s2.0-S0143816605X01828/1-s2.0-S0143816604002386/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEHcaCXVzLWVhc3QtMSJIMEYCIQCa7fV5O{\%}2FYwsR4jQBsSTGJTXpCXX4eyBg{\%}2B6wN4SAbHKUAIhAIeK9lMqapBOyhYMhKXytKKU841mi8HaL1lJ9{\%}2F},
volume = {43},
year = {2005}
}
@article{Chourasiya2017,
abstract = {This paper reports a novel approach for underwater measurement of mobile bottom surface by using an inexpensive 3-D depth sensor – Kinect. The sensor is tested in controlled conditions for surface profiling of rigid objects of different colors placed under varying water depths, turbidity, illuminosity and sensor height. Measurements indicate random errors are present when the object is in air only and these errors can be eliminated by smoothing. However, systematic errors were observed in the presence of water and are attributed to refraction. A refractive correction equation is developed to remove those systematic errors. Validation of the sensor's performance, quantification of its limitations and a systematic procedure for its use in underwater profiling is presented. Finally, capabilities of the sensor as an underwater measurement device for laboratory applications are demonstrated by measuring erosion of cross-stream sand bar due to overtopping and evolution of mining-pit.},
author = {Chourasiya, Shikha and Mohapatra, P. K. and Tripathi, Shivam},
doi = {10.1016/j.advwatres.2017.03.009},
file = {:home/miguel/Dropbox/Mendeley Desktop/Chourasiya, Mohapatra, Tripathi - 2017 - Non-intrusive underwater measurement of mobile bottom surface.pdf:pdf},
issn = {03091708},
journal = {Advances in Water Resources},
keywords = {3-D depth sensor,Cross-stream sand bar erosion,Kinect,Mining-pit evolution,Non-intrusive measurement,Underwater bed profiling},
month = {jun},
pages = {76--88},
title = {{Non-intrusive underwater measurement of mobile bottom surface}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0309170817302385},
volume = {104},
year = {2017}
}
@book{Kirchner2020,
address = {Cham},
author = {Kirchner, Frank and Straube, Sirko and K{\"{u}}hn, Daniel and Hoyer, Nina},
doi = {10.1007/978-3-030-30683-0},
editor = {Kirchner, Frank and Straube, Sirko and K{\"{u}}hn, Daniel and Hoyer, Nina},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kirchner et al. - 2020 - AI Technology for Underwater Robots.pdf:pdf},
isbn = {978-3-030-30682-3},
publisher = {Springer International Publishing},
series = {Intelligent Systems, Control and Automation: Science and Engineering},
title = {{AI Technology for Underwater Robots}},
url = {http://link.springer.com/10.1007/978-3-030-30683-0},
volume = {96},
year = {2020}
}
@article{Cochenour2011,
abstract = {Optical detection, ranging, and imaging of targets in turbid water is complicated by absorption and scattering. It has been shown that using a pulsed laser source with a range-gated receiver or an intensity modulated source with a coherent RF receiver can improve target contrast in turbid water. A blended approach using a modulated-pulse waveform has been previously suggested as a way to further improve target contrast. However only recently has a rugged and reliable laser source been developed that is capable of synthesizing such a waveform so that the effect of the underwater environment on the propagation of a modulated pulse can be studied. In this paper, we outline the motivation for the modulated-pulse (MP) concept, and experimentally evaluate different MP waveforms: single-tone MP and pseudorandom coded MP sequences.},
author = {Cochenour, Brandon and Mullen, Linda and Muth, John},
doi = {10.1364/ao.50.006168},
file = {:home/miguel/Dropbox/Mendeley Desktop/Cochenour, Mullen, Muth - 2011 - Modulated pulse laser with pseudorandom coding capabilities for underwater ranging, detection, and imag.pdf:pdf},
issn = {0003-6935},
journal = {Applied Optics},
number = {33},
pages = {6168},
title = {{Modulated pulse laser with pseudorandom coding capabilities for underwater ranging, detection, and imaging}},
url = {https://www.researchgate.net/publication/51822461},
volume = {50},
year = {2011}
}
@book{Hansard2012,
abstract = {Severalmethods that combine range and color data have been investigated and successfully used in various applications.Most of these systems suffer from the problems of noise in the range data and resolutionmismatch between the range sen- sor and the color cameras. High-resolution depth maps can be obtained using stereo matching, but this often fails to construct accurate depthmaps of weakly/repetitively textured scenes. Range sensors provide coarse depth information regardless of pres- ence/absence of texture.We propose a novel tof-stereo fusion method based on an efficient seed-growing algorithm which uses the tof data projected onto the stereo image pair as an initial set of correspondences. These initial “seeds” are then prop- agated to nearby pixels using a matching score that combines an image similarity criterion with rough depth priors computed from the low-resolution range data. The overall result is a dense and accurate depthmap at the resolution of the color cameras at hand. We show that the proposed algorithm outperforms 2D image-based stereo algorithms and that the results are of higher resolution than off-the-shelf RGB-D sensors, e.g., Kinect.},
address = {London},
author = {Hansard, Miles and Lee, Seungkyu and Choi, Ouk and Horaud, Radu},
booktitle = {Springer Briefs in Computer Science},
doi = {10.1007/978-1-4471-4658-2},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hansard et al. - 2012 - Time of Flight Cameras Principles , Methods , and Applications.pdf:pdf},
isbn = {9781447146582},
pages = {95},
publisher = {Springer London},
series = {SpringerBriefs in Computer Science},
title = {{Time of Flight Cameras : Principles , Methods , and Applications}},
url = {http://link.springer.com/10.1007/978-1-4471-4658-2 https://hal.inria.fr/hal-00725654/PDF/TOF.pdf},
year = {2012}
}
@techreport{Siegwart2004,
author = {Siegwart, Roland and Nourbakhsh, Illah R.},
file = {:home/miguel/Dropbox/Mendeley Desktop/Siegwart, Nourbakhsh - 2004 - Introduction to Autonomous Mobile Robots.pdf:pdf},
isbn = {0-262-19502-X},
pages = {321},
title = {{Introduction to Autonomous Mobile Robots}},
url = {http://home.deib.polimi.it/gini/robot/docs/siegwart.pdf},
year = {2004}
}
@inproceedings{Mandal2018,
author = {Mandal, Ranju and Connolly, Rod M. and Schlacher, Thomas A. and Stantic, Bela},
booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2018.8489482},
file = {:home/miguel/Dropbox/Mendeley Desktop/Mandal et al. - 2018 - Assessing fish abundance from underwater video using deep neural networks.pdf:pdf},
isbn = {978-1-5090-6014-6},
month = {jul},
pages = {1--6},
publisher = {IEEE},
title = {{Assessing fish abundance from underwater video using deep neural networks}},
url = {https://ieeexplore.ieee.org/document/8489482/},
year = {2018}
}
@inproceedings{Yao2019,
abstract = {In order to adopt the conventional visual measurement technology to underwater application, this paper proposes an underwater image conversion method based on a multi-layer plane refraction model, which converts the underwater image into an air image. First, under the multi-layer plane refraction model, the imaging process of the underwater camera is modelled in the form of a four-dimensional light field parameterization, and the direction vector of the air image is calculated using the light field direction information. Then, the perspective projection transformation is used to obtain the corresponding pixel coordinates of the direction vector and the image plane. Finally, the transformed air image is obtained by interpolation method, using the mapping relation of the corresponding image point coordinates before and after the transformation. Simulation and experimental results show that the average error of the corrected image is 0.56 pixel, and the high-precision air image provides strong support for subsequent underwater 3D reconstruction.},
author = {Yao, Qinzhou and Zhuang, Sufeng and Tu, Dawei and Zhang, Xu and Xie, Liangliang},
booktitle = {2nd International Conference on Air Pollution and Environmental Engineering},
doi = {10.1088/1755-1315/450/1/012079},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yao et al. - 2019 - Underwater image conversion method based on multi-layer plane refraction model with light field processing method Un.pdf:pdf},
title = {{Underwater image conversion method based on multi-layer plane refraction model with light field processing method Underwater image conversion method based on multi-layer plane refraction model with light field processing method}},
year = {2019}
}
@article{Campos2015,
abstract = {Underwater range scanning techniques are starting to gain interest in underwater exploration, providing new tools to represent the seafloor. These scans (often) acquired by underwater robots usually result in an unstructured point cloud, but given the common downward-looking or forward-looking configuration of these sensors with respect to the scene, the problem of recovering a piecewise linear approximation representing the scene is normally solved by approximating these 3D points using a heightmap (2.5D). Nevertheless, this representation is not able to correctly represent complex structures, especially those presenting arbitrary concavities normally exhibited in underwater objects. We present a method devoted to full 3D surface reconstruction that does not assume any specific sensor configuration. The method presented is robust to common defects in raw scanned data such as outliers and noise often present in extreme environments such as underwater, both for sonar and optical surveys. Moreover, the proposed method does not need a manual preprocessing step. It is also generic as it does not need any information other than the points themselves to work. This property leads to its wide application to any kind of range scanning technologies and we demonstrate its versatility by using it on synthetic data, controlled laser scans, and multibeam sonar surveys. Finally, and given the unbeatable level of detail that optical methods can provide, we analyze the application of this method on optical datasets related to biology, geology and archeology.},
author = {Campos, Ricard and Garc{\'{i}}a, Rafael and Alliez, Pierre and Yvinec, Mariette},
doi = {10.1177/0278364914544531},
file = {:home/miguel/Dropbox/Mendeley Desktop/Campos et al. - 2015 - A surface reconstruction method for in-detail underwater 3D optical mapping.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Surface reconstruction,noise attenuation,outlier-rejection,underwater robotics},
number = {1},
pages = {64--89},
publisher = {SAGE Publications},
title = {{A surface reconstruction method for in-detail underwater 3D optical mapping}},
url = {https://hal.inria.fr/file/index/docid/1030845/filename/underwater.pdf},
volume = {34},
year = {2015}
}
@inproceedings{Ahlberg2017,
abstract = {Hyperspectral remote sensing based on unmanned airborne vehicles is a field increasing in importance. The combined functionality of simultaneous hyperspectral and geometric modeling is less developed. A configuration has been developed that enables the reconstruction of the hyperspectral three-dimensional (3D) environment. The hyperspectral camera is based on a linear variable filter and a high frame rate, high resolution camera enabling point-to-point matching and 3D reconstruction. This allows the information to be combined into a single and complete 3D hyperspectral model. In this paper, we describe the camera and illustrate capabilities and difficulties through real-world experiments.},
author = {Ahlberg, J{\"{o}}rgen and Renhorn, Ingmar G. and Chevalier, Tomas R. and Rydell, Joakim and Bergstr{\"{o}}m, David},
booktitle = {Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIII},
doi = {10.1117/12.2262456},
editor = {Velez-Reyes, Miguel and Messinger, David W.},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ahlberg et al. - 2017 - Three-dimensional hyperspectral imaging technique.pdf:pdf},
isbn = {9781510608979},
issn = {1996756X},
keywords = {3d,hyperspectral,remote sensing},
pages = {1019805},
publisher = {International Society for Optics and Photonics},
title = {{Three-dimensional hyperspectral imaging technique}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2262456},
year = {2017}
}
@article{Busck2005,
abstract = {New 3-D optical underwater images are presented. The 3-D images are recorded in fresh water and brackish sea water (salinity 1.5{\%}), at 4- to 5-m range. For the first time, underwater 3-D images are computed by our new algorithm, which applies the method of weighted averages on a sequence of 2-D images. It is proposed that 3-D gated viewing images can be recorded at any contrast level between 0 and 100{\%}. A novel and dynamic way of measuring the depth of gating is presented. A novel correction for gated viewing 3-D imaging is presented. For the first time, an exact solution of the depth of gating is proposed for a rectangular laser pulse.},
author = {Busck, Jens},
doi = {10.1117/1.2127895},
file = {:home/miguel/Dropbox/Mendeley Desktop/Busck - 2005 - Underwater 3-D optical imaging with a gated viewing laser radar.pdf:pdf},
isbn = {0091-3286},
issn = {0091-3286},
journal = {Optical Engineering},
month = {nov},
number = {11},
pages = {116001},
publisher = {International Society for Optics and Photonics},
title = {{Underwater 3-D optical imaging with a gated viewing laser radar}},
url = {http://opticalengineering.spiedigitallibrary.org/article.aspx?doi=10.1117/1.2127895},
volume = {44},
year = {2005}
}
@article{Chao2017,
abstract = {In this paper, a high-speed non-mechanical two-dimensional KTN beam deflector is reported. The scanning mechanism is based on the combination of space charge controlled beam deflection and temperature gradient enabled beam deflection in a nanodisordered KTN crystal. Both theoretical analyses and experimental investigations are provided, which agree relatively well with each other. This work provides an effective way for realizing multi-dimensional high-speed non-mechanical beam deflection, which can be very useful for a variety of applications, including high-speed 3D laser printing, high resolution high speed scanning imaging, and free space reconfigurable laser communications.},
author = {Chao, Ju-hung and Zhu, Wenbin and Chen, Chang Jiang and Hoffman, Robert C and Campbell, Adrian L and Henry, Michael G and Yin, Shizhuo},
doi = {10.1364/oe.25.015481},
file = {:home/miguel/Dropbox/Mendeley Desktop/Chao et al. - 2017 - High speed non-mechanical two-dimensional KTN beam deflector enabled by space charge and temperature gradient defle.pdf:pdf},
journal = {Optics Express},
number = {13},
pages = {15481},
title = {{High speed non-mechanical two-dimensional KTN beam deflector enabled by space charge and temperature gradient deflection}},
url = {https://doi.org/10.1364/OE.25.015481},
volume = {25},
year = {2017}
}
@article{Kawahara2016,
abstract = {This paper presents an underwater active stereo system that realizes 3D capture of dynamic objects in water such as swimming fish. The key idea on realizing a practical underwater 3D sensing is to model the refraction process by our pixel-wise varifocal camera model that provides efficient forward (3D to 2D) projections as well as an underwater projector–camera calibration. Evaluations demonstrate that our method achieves reasonable calibration accuracy using off-the-shelf cameras and projectors, and provides a 3D capture of real swimming fish in water.},
author = {Kawahara, Ryo and Nobuhara, Shohei and Matsuyama, Takashi},
doi = {10.1016/j.mio.2016.08.002},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kawahara, Nobuhara, Matsuyama - 2016 - Dynamic 3D capture of swimming fish by underwater active stereo.pdf:pdf},
issn = {22111220},
journal = {Methods in Oceanography},
keywords = {3D shape reconstruction,Calibration,Refraction,Underwater active stereo},
month = {dec},
pages = {118--137},
title = {{Dynamic 3D capture of swimming fish by underwater active stereo}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2211122015300074},
volume = {17},
year = {2016}
}
@article{Cai2016,
abstract = {In this paper, we propose a method by means of light field imaging under structured illumination to deal with high dynamic range 3D imaging. Fringe patterns are projected onto a scene and modulated by the scene depth then a structured light field is detected using light field recording devices. The structured light field contains information about ray direction and phase-encoded depth, via which the scene depth can be estimated from different directions. The multidirectional depth estimation can achieve high dynamic 3D imaging effectively. We analyzed and derived the phase-depth mapping in the structured light field and then proposed a flexible ray-based calibration approach to determine the independent mapping coefficients for each ray. Experimental results demonstrated the validity of the proposed method to perform high-quality 3D imaging for highly and lowly reflective surfaces.},
author = {Cai, Zewei and Liu, Xiaoli and Peng, Xiang and Yin, Yongkai and Li, Ameng and Wu, Jiachen and Gao, Bruce Z.},
doi = {10.1364/OE.24.020324},
file = {:home/miguel/Dropbox/Mendeley Desktop/Cai et al. - 2016 - Structured light field 3D imaging.pdf:pdf},
isbn = {1094-4087},
issn = {1094-4087},
journal = {Optics Express},
keywords = {Light fields,Phase unwrapping,Plenoptic imaging,Spatial resolution,Structured light,Three dimensional imaging},
month = {sep},
number = {18},
pages = {20324},
publisher = {Optical Society of America},
title = {{Structured light field 3D imaging}},
url = {https://www.osapublishing.org/abstract.cfm?URI=oe-24-18-20324},
volume = {24},
year = {2016}
}
@inproceedings{Gracias2002,
abstract = {This paper deals with the problem of constructing high quality mosaics of the sea bed. It focuses on the use of long image sequences with time-distant superpositions, such as the ones arising from loop trajectories or zig-zag scanning patterns. An algorithm is presented for the simultaneous creation of mosaics and the estimation of the camera trajectory. The method comprises three major stages. The first stage consists of the sequential estimation of the image motion, using a reduced image motion model. The set of resulting consecutive homographies is cascaded, in order to infer the approximate topology of the camera movement. The topology information is then used to predict the areas where there is image overlap resulting from non-consecutive images. Secondly, a motion refinement is performed, by iteratively executing the following two main steps. (1) Point correspondences are established between non-adjacent pairs of images that present enough overlap. (2) The topology is refined, by searching for the set of homographies that minimizes the overall sum of distances in the point matches. The final stage of the algorithm consists of estimating the set of homographies and a world plane description that best fit the observation data. As the main concern is attaining high registration accuracy, a general parameterization of the homographies with 6 DOF for the pose is used, which is capable of modelling the effects of wave-induced general rotation and translation. The overall method is fully automatic in the sense it does not require human intervention at any of the stages, apart from the beforehand specification of the most adequate motion model for the first stage. We present results obtained from shallow water image sequences acquired by a ROV. The images present some of the common difficulties of underwater mosaicing, such as non planar sea-bottom, moving objects and severe illumination changes. This sequence also serves to illustrate the robustness and good performance of the presented algorithm.},
author = {Gracias, N. and Santos-Victor, J.},
booktitle = {MTS/IEEE Oceans 2001. An Ocean Odyssey. Conference Proceedings (IEEE Cat. No.01CH37295)},
doi = {10.1109/oceans.2001.968403},
file = {:home/miguel/Dropbox/Mendeley Desktop/Gracias, Santos-Victor - 2002 - Underwater mosaicing and trajectory reconstruction using global alignment.pdf:pdf},
isbn = {0-933957-28-9},
pages = {2557--2563},
publisher = {Marine Technol. Soc},
title = {{Underwater mosaicing and trajectory reconstruction using global alignment}},
url = {http://ieeexplore.ieee.org/document/968403/},
volume = {4},
year = {2002}
}
@misc{Holmstrom2014,
abstract = {Laser scanners have been an integral part of MEMS research for more than three decades. During the last decade, miniaturized projection displays and various medical-imaging applications became the main driver for progress in MEMS laser scanners. Portable and truly miniaturized projectors became possible with the availability of red, green, and blue diode lasers during the past few years. Inherent traits of the laser scanning technology, such as the very large color gamut, scalability to higher resolutions within the same footprint, and capability of producing an always-in-focus image render it a very viable competitor in mobile projection. Here, we review the requirements on MEMS laser scanners for the demanding display applications, performance levels of the best scanners in the published literature, and the advantages and disadvantages of electrostatic, electromagnetic, piezoelectric, and mechanically coupled actuation principles. Resonant high-frequency scanners, low-frequency linear scanners, and 2-D scanners are included in this review.},
author = {Holmstr{\"{o}}m, Sven T.S. and Baran, Utku and Urey, Hakan},
booktitle = {Journal of Microelectromechanical Systems},
doi = {10.1109/JMEMS.2013.2295470},
file = {:home/miguel/Dropbox/Mendeley Desktop/Holmstr{\"{o}}m, Baran, Urey - 2014 - MEMS laser scanners A review.pdf:pdf},
isbn = {10577157},
issn = {10577157},
keywords = {Laser displays,MEMS scanners,portable projectors},
month = {apr},
number = {2},
pages = {259--275},
pmid = {2764},
title = {{MEMS laser scanners: A review}},
url = {http://ieeexplore.ieee.org/document/6714402/},
volume = {23},
year = {2014}
}
@techreport{TexasInstruments2008a,
abstract = {The DLP {\textregistered} Digital Micromirror Device [DMD] has long been used in video projection systems for commercial and consumer applications. The DMD is also an attractive Spatial Light Modulator for applications that may use monochromatic and/or coherent light sources both inside and outside of the visible spectrum. The DLP The DMD presents some unique advantages and considerations that must be understood for these applications particularly when using coherent or semi-coherent sources with the DMD. When used with such sources the fact that the DMD is a 2D array of periodically spaced mirrors cannot be ignored, rather the diffractive effects can be understood and even exploited. The DMD presents some unique advantages and considerations that must be understood for these applications particularly when using coherent or semi-coherent sources with the DMD. When used with such sources the fact that the DMD is a 2D array of periodically spaced mirrors cannot be ignored, rather the diffractive effects can be understood and even exploited. {\textregistered} Digital Micromirror Device [DMD] has long been used in video projection systems for commercial and consumer applications. The DMD is also an attractive Spatial Light Modulator for applications that may use monochromatic and/or coherent light sources both inside and outside of the visible spectrum. The purpose of this paper is to present an intuitive understanding of the 2D diffraction properties of the DMD and the advantages and challenges that result. The purpose of this paper is to present an intuitive understanding of the 2D diffraction properties of the DMD and the advantages and challenges that result.},
author = {{Texas Instruments}},
file = {:home/miguel/Dropbox/Mendeley Desktop/Texas Instruments - 2008 - Using Lasers with DLP {\textregistered} DMD technology.pdf:pdf},
title = {{Using Lasers with DLP {\textregistered} DMD technology}},
url = {http://www.ti.com/lit/wp/dlpa037/dlpa037.pdf},
year = {2008}
}
@inproceedings{Nuchter,
abstract = {This paper shows how to use the result of Google's SLAM solution, called Cartographer, to bootstrap our continuous-time SLAM algorithm. The presented approach optimizes the consistency of the global point cloud, and thus improves on Google's results. We use the algorithms and data from Google as input for our continuous-time SLAM software. We also successfully applied our software to a similar backpack system which delivers consistent 3D point clouds even in absence of an IMU.},
author = {N{\"{u}}chter, Andreas and Bleier, Michael and Schauer, Johannes and Janotta, Peter},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W3-543-2017},
file = {:home/miguel/Dropbox/Mendeley Desktop/N{\"{u}}chter et al. - 2017 - Improving google's cartographer 3D mapping by continuous-time slam.pdf:pdf},
issn = {16821750},
keywords = {3D point clouds,Backpack,Personal laser scanner,SLAM,Trajectory optimization},
number = {2W3},
pages = {543--549},
title = {{Improving google's cartographer 3D mapping by continuous-time slam}},
url = {http://threedtk.de},
volume = {42},
year = {2017}
}
@article{Singh2004,
abstract = {The propagation of visible light underwater suffers rapid attenuation and extreme scattering. This, in combination with the limited camera-to-light separation available on most imaging platforms, places severe limitations on our ability to optically image large areas of the sea floor at high resolution. In this paper, we present a general framework for mosaicking large areas underwater with a specific emphasis on the issues that are unique to the underwater environment. At the individual image level, we examine the role of attenuation, scattering, and camera to light separation and present the tradeoffs involved in optimizing a particular imaging geometry. We also examine the arbitrary image-registration problem in the face of conditions prevalent underwater, namely a moving nonuniform lighting source and the effects of a featureless unstructured terrain. Our analysis is based on photomosaics encompassing several hundred images on archaeological, forensic, and geological expeditions from a diverse set of imaging platforms, including the NR-1 nuclear submarine, the manned submersible Alvin, the Argo towed vehicle, the Jason remotely operated vehicle, and the ABE autonomous underwater vehicle.},
author = {Singh, Hanumant and Howland, Jonathan and Pizarro, Oscar},
doi = {10.1109/JOE.2004.831619},
file = {:home/miguel/Dropbox/Mendeley Desktop/Singh, Howland, Pizarro - 2004 - Advances in large-area photomosaicking underwater.pdf:pdf},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Mosaicking,Optical imaging,Underwater vehicles},
month = {jul},
number = {3},
pages = {872--886},
title = {{Advances in large-area photomosaicking underwater}},
url = {http://ieeexplore.ieee.org/document/1353438/},
volume = {29},
year = {2004}
}
@inproceedings{Grisetti2010,
abstract = {In this paper, we present a new hierarchical optimization solution to the graph-based simultaneous localization and mapping (SLAM) problem. During online mapping, the approach corrects only the coarse structure of the scene and not the overall map. In this way, only updates for the parts of the map that need to be considered for making data associations are carried out. The hierarchical approach provides accurate non-linear map estimates while being highly efficient. Our error minimization approach exploits the manifold structure of the underlying space. In this way, it avoids singularities in the state space parameterization. The overall approach is accurate, efficient, designed for online operation, overcomes singularities, provides a hierarchical representation, and outperforms a series of state-of-the-art methods. {\textcopyright}2010 IEEE.},
author = {Grisetti, Giorgio and K{\"{u}}mmerle, Rainer and Stachniss, Cyrill and Frese, Udo and Hertzberg, Christoph},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509407},
file = {:home/miguel/Dropbox/Mendeley Desktop/Grisetti et al. - 2010 - Hierarchical optimization on manifolds for online 2D and 3D mapping.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
pages = {273--278},
title = {{Hierarchical optimization on manifolds for online 2D and 3D mapping}},
year = {2010}
}
@article{Schjolberg2016,
abstract = {This paper presents the latest developments in the project Next Generation Subsea Inspection, Maintenance and Repair (IMR). Subsea IMR operations are frequently carried out in the offshore oil and gas business. Increased efficiency in such operations will reduce time of operations and costs. Autonomous functionalities constitute an enabler for such reduction. To this end, the project is focusing on technologies, algorithms and methods required for enabling the right level of autonomy and human-machine interaction in IMR operations. This includes new perception, localization, path-planning, and shared control technologies, as well as new methods for risk management. Although NextGenIMR has a particular focus on subsea operations in the oil and gas industry, the technologies developed will also be highly relevant for IMR operations in fish farms and in deep sea mining.},
author = {Schj{\o}lberg, Ingrid and Gjersvik, Tor B and Transeth, Aksel A and Utne, Ingrid B},
doi = {10.1016/j.ifacol.2016.10.443},
file = {:home/miguel/Dropbox/Mendeley Desktop/Schj{\o}lberg et al. - 2016 - Next Generation Subsea Inspection, Maintenance and Repair Operations.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Autonomous operations,Localization,Risk management},
number = {23},
pages = {434--439},
title = {{Next Generation Subsea Inspection, Maintenance and Repair Operations}},
url = {http://srv.uib.es/public/CAMS2016/media/files/0071.pdf},
volume = {49},
year = {2016}
}
@book{Vosselman2010,
abstract = {Written by a team of international experts, this book provides a comprehensive overview of the major applications ofairborne and terrestrial laser scanning. The book focuses on principles and methods and presents an integratedtreatment of airborne and terrestrial laser scanning technology.Laser scanning is a relatively young 3D measurement technique offering much potential in the acquisition of preciseand reliable 3D geodata and object geometries. However, there are many terrestrial and airborne scanners on themarket, accompanied by numerous software packages that handle data acquisition, processing and visualization, yetexisting knowledge is fragmented over a wide variety of publications, whether printed or electronic.This book brings together the various facets of the subject in a coherent text that will be relevant for advancedstudents, academics and practitioners.},
author = {Vosselman, George and Maas, Hans-Gerd},
file = {:home/miguel/Dropbox/Mendeley Desktop/Vosselman, Maas - 2010 - Airborne and terrestrial laser scanning.pdf:pdf},
isbn = {9781439827987},
publisher = {Whittles Publishing},
title = {{Airborne and terrestrial laser scanning}},
url = {www.whittlespublishing.com http://www.tandfonline.com/doi/abs/10.1080/17538947.2011.553487},
year = {2010}
}
@inproceedings{Kondo2004,
abstract = {Autonomous Underwater Vehicles (AUVs) are suitable for condition survey of artificial structures such as pillars and caissons in harbors. The authors have developed a method to investigate structures using AUVs. The method has been successfully demonstrated in tank tests and sea trials as reported in previous papers. Since the ranging system in this method used an array of pinpoint lasers, it is oversensitive to noise, for example small obstacles and floating particles. This paper describes a method to trace the structure's surface using a sheet laser beam that overcomes the difficulties previously encountered. This ranging system determines the continuous shape of the target objects over a wide area by light-sectioning. The vehicle navigates referencing the principal shape of the structure, tracing its surface while taking video images. The method is implemented using testbed AUV "Tri-Dog 1" and verified by tank tests. It is proved that the new method is more robust against noise and trivial objects than the previous method that used a pinpoint laser ranging system. This ranging system can also be used for 3D mapping of the structure surface and bottom. {\textcopyright} 2004 IEEE.},
author = {Kondo, H. and Maki, T. and Ura, Tamaki and Nose, Y. and Sakamaki, Takashi and Inaishi, M.},
booktitle = {Proceedings of the 2004 International Symposium on Underwater Technology (IEEE Cat. No.04EX869)},
doi = {10.1109/ut.2004.1405483},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kondo et al. - 2004 - Structure tracing with a ranging system using a sheet laser beam.pdf:pdf},
isbn = {0-7803-8541-1},
pages = {83--88},
publisher = {IEEE},
title = {{Structure tracing with a ranging system using a sheet laser beam}},
url = {http://ieeexplore.ieee.org/document/1405483/},
year = {2004}
}
@article{Ribas2008,
abstract = {This paper describes a navigation system for autonomous underwater vehicles (AUVs) in partially structured environments, such as dams, harbors, marinas, and marine plat- forms. A mechanically scanned imaging sonar is used to obtain information about the location of vertical planar structures present in such environments. A robust voting algo- rithm has been developed to extract line features, together with their uncertainty, fromthe continuous sonar data flow. The obtained information is incorporated into a feature-based simultaneous localization and mapping (SLAM) algorithm running an extended Kalman filter. Simultaneously, the AUV's position estimate is provided to the feature extraction al- gorithm to correct the distortions that the vehiclemotion produces in the acoustic images. Moreover, a procedure to build andmaintain a sequence of local maps and to posteriorly recover the full global map has been adapted for the application presented. Experiments carried out in a marina located in the Costa Brava (Spain) with the Ictineu AUV show the viability of the proposed approach.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Ribas, David and Ridao, Pere and Tard{\'{o}}s, Juan Domingo and Neira, Jos{\'{e}}},
doi = {10.1002/rob.20249},
eprint = {10.1.1.91.5767},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ribas et al. - 2008 - Underwater SLAM in man-made structured environments.pdf:pdf},
isbn = {15564959},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {nov},
number = {11-12},
pages = {898--921},
pmid = {22164016},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Underwater SLAM in man-made structured environments}},
url = {http://doi.wiley.com/10.1002/rob.20249},
volume = {25},
year = {2008}
}
@inproceedings{Inglis2012,
abstract = {This paper details a methodology for using struc- tured light laser imaging to create high resolution bathymetric maps of the sea floor. The system includes a pair of stereo cameras and an inclined 532nm sheet laser mounted to a remotely operated vehicle (ROV). While a structured light system generally requires a single camera, a stereo vision set up is used here for in-situ calibration of the laser system geometry by triangulating points on the laser line. This allows for quick calibration at the survey site and does not require precise jigs or a controlled environment. A batch procedure to extract the laser line from the images to sub-pixel accuracy is also presented. The method is robust to variations in image quality and moderate amounts of water column turbidity. The final maps are constructed using a reformulation of a previous bathymetric Simultaneous Localization and Mapping (SLAM) algorithm called incremental Smoothing and Mapping (iSAM). The iSAM framework is adapted from previous applications to perform sub-mapping, where segments of previously visited terrain are registered to create relative pose constraints. The resulting maps can be gridded at one centimeter and have significantly higher sample density than similar surveys using high frequency multibeam sonar or stereo vision. Results are presented for sample surveys at a submerged archaeological site and sea floor rock outcrop.},
author = {Inglis, Gabrielle and Smart, Clara and Vaughn, Ian and Roman, Chris},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386038},
file = {:home/miguel/Dropbox/Mendeley Desktop/Inglis et al. - 2012 - A pipeline for structured light bathymetric mapping.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {4425--4432},
publisher = {IEEE},
title = {{A pipeline for structured light bathymetric mapping}},
url = {http://ieeexplore.ieee.org/document/6386038/},
year = {2012}
}
@inproceedings{McLeod2013,
abstract = {Advances in autonomous inspection of subsea facilities using a 3 Dimensional(3D) LIght Detection And Ranging (LiDAR) sensor are examined to illustrate the favorable enhancement of safety, reliability, reduction in risks, economic benefits and superior data products compared to conventional means. These benefits provide operators with significant improvements over general visual inspection by the addition of sensors that produce 3D models of the structure being inspected. Examples are provided illustrating test data from operations conducted from 2012-2013. Supported by funding from the Research Partnership to Secure Energy for America (RPSEA) Lockheed Martin MST teamed with the innovative company 3D at Depth to incorporate their new DP2TM 3D LiDAR onto the proven Marlin{\textregistered} Autonomous Underwater Vehicle. The objectives of the project include: Survey using combination of high resolution 3D Sonar and 3D LiDAR Generation of high fidelity 3D models Detection and localization of structural changes vs. reference model Lockheed Martin has successfully integrated the laser into the Marlin shore based laboratory addressing the challenges associated with coordinating a scanning laser from a moving platform and producing high resolution georegistered images. Testing will be conducted in early 2014 to validate the system in the Atlantic Ocean waters offshore Palm Beach, Florida. Three dimensional georegistered models of an entire scene can be rapidly collected providing a clear vision of the underwater scene along high resolution 3D models of the imaged item. Data collected at sea and in the laboratory will be presented demonstrating the performance of the system. Application to deepwater life of field inspection will be presented with evidence gained from offshore trials. This emergent technology supports Subsea Facility Inspection Repair and Maintenance, Integrity Management Inspections of Marine Risers, Moorings and anchors, Subsea Pipelines, Flowlines, Umbilicals, and supporting subsea infrastructure.},
author = {McLeod, D and Jacobson, J and Hardy, M and Embry, C},
booktitle = {2013 OCEANS - San Diego},
doi = {10.23919/OCEANS.2013.6741175},
file = {:home/miguel/Dropbox/Mendeley Desktop/McLeod et al. - 2013 - Autonomous inspection using an underwater 3D LiDAR.pdf:pdf},
issn = {0197-7385},
keywords = {autonomous underwater vehicles,image resolution,in},
pages = {1--8},
title = {{Autonomous inspection using an underwater 3D LiDAR}},
year = {2013}
}
@article{Bruno2011,
abstract = {Current research on underwater 3D imaging methods is mainly addressing long range applications like seafloor mapping or surveys of archeological sites and shipwrecks. Recently, there is an increasing need for more accessible and precise close-range 3D acquisition technologies in some application fields like, for example, monitoring the growth of coral reefs or reconstructing underwater archaeological pieces that in most cases cannot be recovered from the seabed. This paper presents the first results of a research project that aims to investigate the possibility of using active optical techniques for the whole-field 3D reconstructions in an underwater environment. In this work we have tested an optical technique, frequently used for in air acquisition, based on the projection of structured lighting patterns acquired by a stereo vision system. We describe the experimental setup used for the underwater tests, which were conducted in a water tank with different turbidity conditions. The tests have evidenced that the quality of 3D reconstruction is acceptable even with high turbidity values, despite the heavy presence of scattering and absorption effects. {\textcopyright} 2011 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Bruno, Fabio and Bianco, Gianfranco and Muzzupappa, Maurizio and Barone, S and Razionale, A V},
doi = {10.1016/j.isprsjprs.2011.02.009},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bruno et al. - 2011 - Experimentation of structured light and stereo vision for underwater 3D reconstruction.pdf:pdf},
isbn = {978-1-4799-8736-8},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {3D reconstruction,Imaging in turbid medium,Photogrammetry,Structured light,Underwater imaging},
number = {4},
pages = {508--518},
pmid = {20642495},
title = {{Experimentation of structured light and stereo vision for underwater 3D reconstruction}},
url = {www.elsevier.com/locate/isprsjprs},
volume = {66},
year = {2011}
}
@inproceedings{Ishibashi2011,
abstract = {The visual information is one of most important for underwater platforms like underwater vehicles to perform various missions. So most of them are equipped with underwater cameras, and a lot of information can be obtained from images taken by the cameras. However, for example, it is very difficult to understand the accurate size or accurate length and area of shooting subjects. So we propose the stereo camera system in order to measure such parameters accurately because Most of the underwater platforms are already equipped with some underwater cameras. And it is more compact than other sensor devices, and also its measurement accuracy is much better than them. However the design of the properly camera model for the deep-sea is very difficult because of the complex optical refraction. So, new camera model for underwater must be designed. In this paper, the outline and the effect of the underwater camera model, which can compensate the influence cased by the complex optical refraction, are shown.},
author = {Ishibashi, Shojiro},
booktitle = {OCEANS 2011 IEEE - Spain},
doi = {10.1109/Oceans-Spain.2011.6003436},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ishibashi - 2011 - The study of the underwater camera model.pdf:pdf},
isbn = {9781457700866},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{The study of the underwater camera model}},
url = {http://ieeexplore.ieee.org/document/6003436/},
year = {2011}
}
@inproceedings{Moss2012,
abstract = {Future robots and autonomous vehicles require compact low-cost Laser Detection and Ranging (LADAR) systems for autonomous navigation. Army Research Laboratory (ARL) had recently demonstrated a brass-board short-range eye-safe MEMS scanning LADAR system for robotic applications. Boeing Spectrolab is doing a tech-transfer (CRADA) of this system and has built a compact MEMS scanning LADAR system with additional improvements in receiver sensitivity, laser system, and data processing system. Improved system sensitivity, low-cost, miniaturization, and low power consumption are the main goals for the commercialization of this LADAR system. The receiver sensitivity has been improved by 2x using large-area InGaAs PIN detectors with low-noise amplifiers. The FPGA code has been updated to extend the range to 50 meters and detect up to 3 targets per pixel. Range accuracy has been improved through the implementation of an optical T-Zero input line. A compact commercially available erbium fiber laser operating at 1550 nm wavelength is used as a transmitter, thus reducing the size of the LADAR system considerably from the ARL brassboard system. The computer interface has been consolidated to allow image data and configuration data (configuration settings and system status) to pass through a single Ethernet port. In this presentation we will discuss the system architecture and future improvements to receiver sensitivity using avalanche photodiodes.},
author = {Moss, Robert and Yuan, Ping and Bai, Xiaogang and Quesada, Emilio and Sudharsanan, Rengarajan and Stann, Barry L and Dammann, John F and Giza, Mark M and Lawler, William B},
doi = {10.1117/12.919804},
file = {:home/miguel/Dropbox/Mendeley Desktop/Moss et al. - 2012 - Low-cost compact MEMS scanning ladar system for robotic applications.pdf:pdf},
isbn = {9780819490575},
issn = {0277786X},
keywords = {ground robots,ladar,laser radar,lidar,three-dimensional imaging},
pages = {837903},
title = {{Low-cost compact MEMS scanning ladar system for robotic applications}},
url = {http://spiedl.org/terms http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.919804},
year = {2012}
}
@article{Ito2013,
abstract = {This paper reports on a light detection and ranging (LIDAR) system that incorporates a microelectromechanical-system (MEMS) mirror scanner and a single-photon imager. The proposed architecture enables a high signal-to-background ratio due to pixel-level synchronization of the single-photon imager and the MEMS mirror. It also allows the receiving optics to feature a large aperture, yet utilizing a small MEMS device. The MEMS actuator achieves a mechanical scanning amplitude of ±4° horizontally and ±3° vertically, while the field of view of the overall sensor is 45 by 110. Distance images were acquired outdoors in order to qualitatively evaluate our sensor imaging capabilities. Quantitative ranging performance characterization carried out under 10 klx of ambient light revealed a precision of 14.5 cm throughout the distance range to 25 m, thus leading to a relative precision of 0.58{\%}.},
author = {Ito, K. and Niclass, C. and Aoyagi, I. and Matsubara, H. and Soga, M. and Kato, S. and Maeda, M. and Kagami, M.},
doi = {10.1109/JPHOT.2013.2247586},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ito et al. - 2013 - System Design and Performance Characterization of a MEMS-Based Laser Scanning Time-of-Flight Sensor Based on a 256 x.pdf:pdf},
isbn = {1943-0655},
issn = {1943-0655},
journal = {IEEE Photonics Journal},
month = {apr},
number = {2},
pages = {6800114--6800114},
title = {{System Design and Performance Characterization of a MEMS-Based Laser Scanning Time-of-Flight Sensor Based on a 256 x 64-pixel Single-Photon Imager}},
url = {http://ieeexplore.ieee.org/document/6471165/},
volume = {5},
year = {2013}
}
@inproceedings{Penalver2015,
abstract = {Autonomous manipulation in unestructured underwater scenarios is a high challenging skill that has been poorly studied and is becoming more and more important in the last years. One of the main problems regarding the autonomous manipulation, is to find out the characteristics of the object which is going to be manipulated. This paper presents a new approach to obtain an accurate 3D reconstruction of this object. This approach consists in attaching a laser stripe emitter and a camera in the forearm of a robotic arm. Moving the arm, the laser scans the scene where the object is and, at the same time, the camera records the scan. Thanks to the arm and the position of the camera, the scene can be reconstructed from different views and from a position close to the object. The recorded images are processed to obtain the 3D position of the part of the scene projected by the laser. Before the intervention, a process of calibration is needed to calculate the relationship between each part of the system. Furthemore, in order to reduce the time of processing of the images recorded during the scan, an optimization algorithm is presented which consists in discarding, before the processing, the pixels of the image which do not contain relevant information. The approach herein presented and the optimization algorithm are tested using an underwater simulator.},
author = {{Pe{\~{n}}alver Monfort}, Antonio and Fernandez, J. Javier and Sales, Jorge and Sanz, Pedro J.},
booktitle = {MTS/IEEE OCEANS 2015 - Genova: Discovering Sustainable Ocean Energy for a New World},
doi = {10.1109/OCEANS-Genova.2015.7271497},
file = {:home/miguel/Dropbox/Mendeley Desktop/Pe{\~{n}}alver Monfort et al. - 2015 - Multi-view underwater 3D reconstruction using a stripe laser light and an eye-in-hand camera.pdf:pdf},
isbn = {9781479987368},
keywords = {3D Reconstruction,Autonomous Grasping,Laser-Camera Calibration,Underwater Laser Scanning,optimization algorithm},
month = {may},
pages = {1--6},
publisher = {IEEE},
title = {{Multi-view underwater 3D reconstruction using a stripe laser light and an eye-in-hand camera}},
url = {http://ieeexplore.ieee.org/document/7271497/},
year = {2015}
}
@phdthesis{Rossi2018,
author = {Rossi, Matija},
file = {:home/miguel/Dropbox/Mendeley Desktop/Rossi - 2018 - Real-time vision systems for underwater robotics Towards the automation of underwater inspection and intervention operati.pdf:pdf},
school = {University of Limerick},
title = {{Real-time vision systems for underwater robotics Towards the automation of underwater inspection and intervention operations}},
url = {https://ulir.ul.ie/bitstream/handle/10344/7591/Rossi{\_}2018{\_}Real{\_}Time.pdf?sequence=6},
year = {2018}
}
@article{Bryson2016,
abstract = {This paper presents an automated approach to recovering the true color of objects on the seafloor in images collected from multiple perspectives by an autonomous underwater vehicle (AUV) during the construction of three-dimensional (3D) seafloor models and image mosaics. When capturing images underwater, the water col- umn induces several effects on light that are typically negligible in air, such as color-dependent attenuation and backscatter. AUVsmust typically carry artificial lightingwhen operating at depths below 20-30 m; the lighting pattern generated is usually not spatially consistent. These effects cause problems for human interpretation of images, limit the ability of using color to identify benthic biota or quantify changes over multiple dives, and confound computer-based techniques for clustering and classification. Our approach exploits the 3D structure of the scene generated using structure-from-motion and photogrammetry techniques to provide basic spatial data to an underwater image formation model. Parameters that are dependent on the properties of the water column are estimated fromthe image data itself, rather than using fixed in situ infrastructure, such as reflectance panels or detailed data onwater constitutes. The model accounts for distance-based attenuation and backscatter, camera vignetting and the artificial lighting pattern, recovering measurements of the true color (reflectance) and thus allows us to approximate the appearance of the scene as if imaged in air and illuminated from above. Our method is validated against known color targets using imagery collected in different underwater environments by two AUVs that are routinely used as part of a benthic habitat monitoring program.},
author = {Bryson, Mitch and Johnson-Roberson, Matthew and Pizarro, Oscar and Williams, Stefan B},
doi = {10.1002/rob.21638},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bryson et al. - 2016 - True Color Correction of Autonomous Underwater Vehicle Imagery.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {6},
pages = {853--874},
title = {{True Color Correction of Autonomous Underwater Vehicle Imagery}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21638},
volume = {33},
year = {2016}
}
@inproceedings{Maruyama2018,
author = {Maruyama, Michika and Tabata, Satoshi and Watanabe, Yoshihiro and Ishikawa, Masatoshi},
booktitle = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
doi = {10.1109/WACV.2018.00106},
file = {:home/miguel/Dropbox/Mendeley Desktop/Maruyama et al. - 2018 - Multi-pattern Embedded Phase Shifting Using a High-Speed Projector for Fast and Accurate Dynamic 3D Measurement.pdf:pdf},
isbn = {9781538648865},
issn = {00221112},
month = {mar},
pages = {921--929},
publisher = {IEEE},
title = {{Multi-pattern Embedded Phase Shifting Using a High-Speed Projector for Fast and Accurate Dynamic 3D Measurement}},
url = {https://ieeexplore.ieee.org/document/8354210/},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Poulton2018a,
abstract = {{\textcopyright} OSA 2018. We present high-performance integrated optical phased arrays and LiDAR on diffusive targets for the first time at 25m with velocity extraction. This work shows the promise for optical phased arrays in solid-state LiDARs.},
author = {Poulton, Christopher V and Russo, Peter and Timurdogan, Erman and Whitson, Michael and Byrd, Matthew J and Hosseini, Ehsan and Moss, Benjamin and Su, Zhan and Vermeulen, Diedrik and Watts, Michael R},
booktitle = {Conference on Lasers and Electro-Optics},
doi = {10.1364/CLEO_AT.2018.ATu3R.2},
file = {:home/miguel/Dropbox/Mendeley Desktop/Poulton et al. - 2018 - High-Performance Integrated Optical Phased Arrays for Chip-Scale Beam Steering and LiDAR.pdf:pdf},
isbn = {978-1-943580-42-2},
issn = {19492081},
keywords = {(2505300) Photonic integrated circuits,(2803640) LIDAR,OCIS codes: (1303120) Integrated optics devices},
pages = {ATu3R.2},
title = {{High-Performance Integrated Optical Phased Arrays for Chip-Scale Beam Steering and LiDAR}},
url = {https://www.researchgate.net/publication/325001311 https://www.osapublishing.org/abstract.cfm?URI=CLEO{\_}AT-2018-ATu3R.2},
year = {2018}
}
@inproceedings{Haner,
abstract = {This paper studies the problem of determining the absolute pose of a perspective camera observing a scene through a known refractive plane, the flat boundary between transparent media with different refractive indices. Efficient minimal solvers are developed for the 2D, known orientation and known rotation axis cases, and near-minimal solvers for the general calibrated and unknown focal length cases. We show that ambiguities in the equations of Snell's law give rise to a large number of false solutions, increasing the complexity of the problem. Evaluation of the solvers on both synthetic and real data show excellent numerical performance, and the necessity of explicitly modelling refraction to obtain accurate pose estimates.},
author = {Haner, Sebastian and {\AA}str{\"{o}}m, Kalle},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298749},
file = {:home/miguel/Dropbox/Mendeley Desktop/Haner, {\AA}str{\"{o}}m - 2015 - Absolute pose for cameras under flat refractive interfaces.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {1428--1436},
title = {{Absolute pose for cameras under flat refractive interfaces}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2015/papers/Haner{\_}Absolute{\_}Pose{\_}for{\_}2015{\_}CVPR{\_}paper.pdf},
volume = {07-12-June},
year = {2015}
}
@article{Shortis2015,
abstract = {Calibration of a camera system is essential to ensure that image measurements result in accurate estimates of locations and dimensions within the object space. In the underwater environment, the calibration must implicitly or explicitly model and compensate for the refractive effects of waterproof housings and the water medium. This paper reviews the different approaches to the calibration of underwater camera systems in theoretical and practical terms. The accuracy, reliability, validation and stability of underwater camera system calibration are also discussed. Samples of results from published reports are provided to demonstrate the range of possible accuracies for the measurements produced by underwater camera systems.},
author = {Shortis, Mark R},
doi = {10.3390/s151229831},
file = {:home/miguel/Dropbox/Mendeley Desktop/Shortis - 2015 - Calibration techniques for accurate measurements by underwater camera systems.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Accuracy,Calibration,Camera,Refraction,Stability,Underwater,Validation},
month = {dec},
number = {12},
pages = {30810--30827},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Calibration techniques for accurate measurements by underwater camera systems}},
url = {http://www.mdpi.com/1424-8220/15/12/29831},
volume = {15},
year = {2015}
}
@article{Wang2012,
abstract = {Two features required for modeling high-frequency acoustic scattering from seafloor are roughness and discrete objects distributed on the seafloor. A laser scanner was deployed to acquire fine-scale bottom roughness off the coast of New Jersey in 2006. On the bathymetry map, it was found that the sites were covered by shell fragments. Cross-comparison between the laser reflection intensity and the bathymetry suggests that the shell fragments have a stronger reflection intensity than the seafloor. This phenomenon was confirmed by an indoor experiment on a simulated seafloor. An intensity-based algorithm was developed to detect shell fragments on the seafloor. The field data were analyzed by this method. Shell fragments visible on the bathymetry were detected along with smaller pieces which were not obvious on the bathymetry map. Preliminary analysis shows that the seafloor was covered by shell fragments (greater than 4 mm(2)) with an area coverage 6.80{\%} and 9.82{\%} for the two sites studied. The shell size probability density distribution function is well modeled by a power-law which implies the weathering of shells is from numerous processes acting on different scales. This method provides the sediment surface information over a larger area which is difficult to acquire by conventional sediment coring.},
author = {Wang, Chau Chang and Tang, Dajun},
file = {:home/miguel/Dropbox/Mendeley Desktop/Wang, Tang - 2012 - Application of underwater laser scanning for seafloor shell fragments characterization.pdf:pdf},
issn = {10232796},
journal = {Journal of Marine Science and Technology},
keywords = {Laser scanning,Seafloor,Shells,Statistics,Volume scattering},
number = {1},
pages = {95--102},
title = {{Application of underwater laser scanning for seafloor shell fragments characterization}},
url = {http://jmst.ntou.edu.tw/marine/20-1/95-102.pdf},
volume = {20},
year = {2012}
}
@article{He2004,
abstract = {Although there are rapid developments of underwater robotics vehicles in recent years, the underwater visual sensing or/and underwater imaging is still regarded as a major undertaking challenge, particularly in turbid water condition. Currently, a divergent-beam underwater Lidar imaging (UWLI) system has been finished and it precisely captured nano-seconds fast-gated images in highly turbid water. This permits the receiver to collect only the image-containing light pulse returned by the target and thus reduce the water-scattering related noise of underwater image dramatically. The end result is greatly improved intensity and contrast of the detected images. Based on our newly designed series targets, we are the first to successfully demonstrate UWLI in such a short 3m water tank, and show the fast range-gated phenomenon in water much more clearly. The attenuation coefficients in turbid waters are 1.0m and 2.3m-1, respectively. {\textcopyright} 2002 Elsevier Science Ltd. All rights reserved.},
author = {He, Duo Min and Seet, Gerald G.L.},
doi = {10.1016/S0143-8166(02)00138-0},
file = {:home/miguel/Dropbox/Mendeley Desktop/He, Seet - 2004 - Divergent-beam Lidar imaging in turbid water.pdf:pdf},
issn = {01438166},
journal = {Optics and Lasers in Engineering},
keywords = {Imaging in turbid medium,Underwater imaging,Underwater lidar imaging,Underwater robotics vehicle},
number = {1},
pages = {217--231},
title = {{Divergent-beam Lidar imaging in turbid water}},
url = {https://ac.els-cdn.com/S0143816602001380/1-s2.0-S0143816602001380-main.pdf?{\_}tid=e5db8a8e-a751-4f7d-8663-d9b0c633186a{\&}acdnat=1551889773{\_}4fda401cd53fa75ceed6ffd8e49dcfa8},
volume = {41},
year = {2004}
}
@inproceedings{Roman2010,
abstract = {This paper presents results from recent work using structured light laser profile imaging to create high resolution bathymetric maps of underwater archaeological sites. Documenting the texture and structure of submerged sites is a difficult task and many applicable acoustic and photographic mapping techniques have recently emerged. This effort was completed to evaluate laser profile imaging in comparison to stereo imaging and high frequency multibeam mapping. A ROV mounted camera and inclined 532 nm sheet laser were used to create profiles of the bottom that were then merged into maps using platform navigation data. These initial results show very promising resolution in comparison to multibeam and stereo reconstructions, particularly in low contrast scenes. At the test sites shown here there were no significant complications related to scattering or attenuation of the laser sheet by the water. The resulting terrain was gridded at 0.25 cm and shows overall centimeter level definition. The largest source of error was related to the calibration of the laser and camera geometry. Results from three small areas show the highest resolution 3D models of a submerged archaeological site to date and demonstrate that laser imaging will be a viable method for accurate three dimensional site mapping and documentation.},
author = {Roman, Chris and Inglis, Gabrielle and Rutter, James},
booktitle = {OCEANS'10 IEEE Sydney, OCEANSSYD 2010},
doi = {10.1109/OCEANSSYD.2010.5603672},
file = {:home/miguel/Dropbox/Mendeley Desktop/Roman, Inglis, Rutter - 2010 - Application of structured light imaging for high resolution mapping of underwater archaeological sites.pdf:pdf},
isbn = {9781424452217},
keywords = {Archeology,Bathymetry,Mapping,Structured light},
title = {{Application of structured light imaging for high resolution mapping of underwater archaeological sites}},
url = {https://www.semanticscholar.org/paper/Application-of-structured-light-imaging-for-high-of-Roman-Inglis/30e36a62966423b059b75d43c502bc631a3291bf},
year = {2010}
}
@incollection{Munaro2015,
abstract = {In this chapter, we present the final system resulting from the European Project "3DComplete" aimed at creating a low-cost and flexible quality inspection system capable of capturing 2.5D color data for completeness inspection. The system uses a single color camera to capture at the same time 3D data with laser trian-gulation and color texture with a special projector of a narrow line of white light, which are then combined into a color 2.5D model in real-time. Many examples of completeness inspection tasks are reported which are extremely difficult to analyze with state-of-the-art 2D-based methods. Our system has been integrated into a real production environment, showing that completeness inspection incorporating 3D technology can be readily achieved in a short time at low costs.},
author = {Munaro, Matteo and So, Edmond Wai Yan and Tonello, Stefano and Menegatti, Emanuele},
booktitle = {Integrated Imaging and Vision Techniques for Industrial Inspection: Advances and Applications},
doi = {10.1007/978-1-4471-6741-9_7},
file = {:home/miguel/Dropbox/Mendeley Desktop/Munaro et al. - 2015 - Efficient completeness inspection using real-time 3D color reconstruction with a dual-laser triangulation system.pdf:pdf},
isbn = {9781447167419},
pages = {201--225},
title = {{Efficient completeness inspection using real-time 3D color reconstruction with a dual-laser triangulation system}},
url = {http://3dcomplete.eu},
year = {2015}
}
@inproceedings{Bleier2019,
abstract = {This paper presents an underwater laser scanning system and GNSS based trajectory estimation system for scanning from a surface vessle in shallow water. The system has an above-the-water and an underwater component. Above-the-water two low-cost multiband GNSS receivers with an antenna baseline of one meter are used for RTK positioning with heading. The full 6-DOF is estimated by fusing the satellite navigation data with a MEMS-based INS. The 3D data is captured in water using a structured light scanner consisting of a low-light underwater camera and a green cross line laser projector. We describe the development of the system and employed hardware components. We show results of scanning a large test object in a water tank acquired by from a tripod with a motorized yaw axis. Additionally, we demonstrate first results of mobile mapping from a floating platform. We evaluate the performance of the system by measuring the 6-DOF trajectory with an external optical tracking system. Additionally, we assess the quality of the created point cloud using reference objects placed in the scene.},
author = {Bleier, Michael and {Van Der Lucht}, Joschka and N{\"{u}}chter, Andreas},
booktitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
doi = {10.5194/isprs-archives-XLII-2-W18-13-2019},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bleier, Van Der Lucht, N{\"{u}}chter - 2019 - SCOUT3D - An underwater laser scanning system for mobile mapping.pdf:pdf},
issn = {16821750},
keywords = {3D reconstruction,Mobile mapping,Underwater laser scanning,Underwater mapping},
number = {2/W18},
pages = {13--18},
title = {{SCOUT3D - An underwater laser scanning system for mobile mapping}},
url = {https://doi.org/10.5194/isprs-archives-XLII-2-W18-13-2019},
volume = {42},
year = {2019}
}
@article{Lee2015,
abstract = {This paper proposes a method for the optical system design of uniform scanning in a larger scan field of view (FOV) in 3D imaging lidar. The theoretical formulas are derived for the design scheme. By employing the optical design software ZEMAX, a foldaway uniform scanning optical system based on MEMS has been designed, and the scanning uniformity and spot size of the system on the target plane, perpendicular to optical axis, are analyzed and discussed. Results show that the designed system can scan uniformly within the FOV of 40{\&}{\#}xB0;{\&}{\#}xD7;40{\&}{\#}xB0; with small spot size for the target at distance of about 100{\&}{\#}xA0;m.},
author = {Lee, Xiaobao and Wang, Chunhui},
doi = {10.1364/AO.54.002219},
file = {:home/miguel/Dropbox/Mendeley Desktop/Lee, Wang - 2015 - Optical design for uniform scanning in MEMS-based 3D imaging lidar.pdf:pdf},
isbn = {15/09221905{\$}15.0},
issn = {1559-128X},
journal = {Applied Optics},
number = {9},
pages = {2219},
title = {{Optical design for uniform scanning in MEMS-based 3D imaging lidar}},
url = {http://dx.doi.org/10.1364/AO.54.002219 https://www.osapublishing.org/abstract.cfm?URI=ao-54-9-2219},
volume = {54},
year = {2015}
}
@article{Kharraz2013,
abstract = {In this report, a performance comparison of the conventional PIN photodiode with the Avalanche Photodiode (APD) in an optical communication system is presented. The effects of bandwidth, gain, extinction ratio, shot noise and thermal noise are compared and studied in detail. It was shown that the Q factor produced by each detector is heavily affected by the thermal noise in the PIN device, and by both the thermal and shot noise in the APD. It was also found that the APD's gain plays a significant role, and the shot noise has to be carefully dealt with. Additionally, the relationship of receiver sensitivity with thermal and shot noise was investigated and compared. {\textcopyright} 2012 Elsevier GmbH. All rights reserved.},
author = {Kharraz, Osayd and Forsyth, David},
doi = {10.1016/j.ijleo.2012.04.008},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kharraz, Forsyth - 2013 - Performance comparisons between PIN and APD photodetectors for use in optical communication systems.pdf:pdf},
issn = {00304026},
journal = {Optik},
keywords = {APD,Extinction ratio,Gain,PIN,Q factor,Shot noise,Thermal noise},
month = {jul},
number = {13},
pages = {1493--1498},
publisher = {Urban {\&} Fischer},
title = {{Performance comparisons between PIN and APD photodetectors for use in optical communication systems}},
url = {https://www.sciencedirect.com/science/article/pii/S0030402612002859},
volume = {124},
year = {2013}
}
@article{Ataman2012,
abstract = {A large-aperture and large-angle MEMS-based 2D pointing mirror is presented. The device is electromagnetically actuated by a moving-magnet/ stationary-coil pair and potentially suited for high power laser beam shaping and beam pointing applications, such as LIDAR. The 4×4 mm 2 mirror, the radially symmetric compliant membrane, and the off-the-shelf permanent magnet are manually assembled, with the planar coil kept at a well-defined vertical distance from the permanent magnet by simple alignment pins. The mirror and the compliant membrane structures are separately microfabricated on bulk silicon and SOI wafers, respectively. The hybrid integration of microfabricated and off-the-shelf components enable low-risk/high-yield fabrication, while limiting the throughput. The device features minimum inter-axis cross coupling and good linearity and is highly immune to alignment and assembly imperfections, thanks to the robust actuation principle. All the components including the bi-axial electromagnetic actuator provide a device footprint as small as the top mirror, allowing the design to be used in compact and high-fill-factor mirror arrays. With a drive coil of 400 mA and 5.12 W drive power, the total uniaxial dc rotation exceeds ±16° (optical) for both axes with good decoupling. At maximum measured angle (biaxial 10° (mechanical)), a position stability better than 0.05° over 7 h, and a position repeatability of 0.04° over 5000 switching cycles is reported. Thermally, the simulated mirror temperature increases to 64 K above the heat sink temperature with a thermal in-flux of 1 kW m -2 , under absolute vacuum. {\textcopyright} 2013 IOP Publishing Ltd.},
author = {Ataman, {\c{C}}aǧlar and Lani, S{\'{e}}bastien and Noell, Wilfried and {De Rooij}, Nico},
doi = {10.1088/0960-1317/23/2/025002},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ataman et al. - 2013 - A dual-axis pointing mirror with moving-magnet actuation.pdf:pdf},
issn = {09601317},
journal = {Journal of Micromechanics and Microengineering},
number = {2},
pages = {13},
title = {{A dual-axis pointing mirror with moving-magnet actuation}},
url = {http://iopscience.iop.org/0960-1317/23/2/025002},
volume = {23},
year = {2013}
}
@inproceedings{Skinner2016,
abstract = {Achieving real-time perception is critical to developing a fully autonomous system that can sense, navigate, and interact with its environment. Perception tasks such as online 3D reconstruction and mapping have been intensely studied for terrestrial robotics applications. However, characteristics of the underwater domain such as light attenuation and light scattering violate the brightness constancy constraint, which is an underlying assumption in methods developed for land based applications. Furthermore, the complex nature of light propagation underwater limits or even prevents subsea use of real-time depth sensors used in state-of-the-art terrestrial mapping techniques. There have been recent advances in the development of plenoptic (also known as light field) cameras, which use an array of micro lenses capturing both intensity and ray direction to enable color and depth measurement from a single passive sensor. This paper presents an end-to-end system to harness these cameras to produce real-time 3D reconstructions underwater. Our system builds upon the state-of-the-art in online terrestrial 3D reconstruction, transferring these approaches to the underwater domain by gathering real-time color and depth (RGB-D) data underwater using a plenoptic camera, and performing dense 3D reconstruction while compensating for attenuation effects of the underwater environment simultaneously, using a graphics processing unit (GPU) to achieve real-time performance. Results are presented for data gathered in a water tank and the proposed technique is validated quantitatively through comparison with a ground truth 3D model gathered in air to demonstrate that the proposed approach can generate accurate 3D models of objects underwater in real-time.},
author = {Skinner, Katherine A. and Johnson-Roberson, Matthew},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759317},
file = {:home/miguel/Dropbox/Mendeley Desktop/Skinner, Johnson-Roberson - 2016 - Towards real-time underwater 3D reconstruction with plenoptic cameras.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
keywords = {Computer vision,Light field cameras,Plenoptic cameras,Real-time 3D reconstruction,Underwater 3D reconstruction},
month = {oct},
pages = {2014--2021},
publisher = {IEEE},
title = {{Towards real-time underwater 3D reconstruction with plenoptic cameras}},
url = {http://ieeexplore.ieee.org/document/7759317/},
volume = {2016-Novem},
year = {2016}
}
@article{Tan2011,
abstract = {Underwater Wireless Sensor Networks (UWSNs) are expected to support a variety of civilian and military applications. Sensed data can only be interpreted meaningfully when referenced to the location of the sensor, making localization an important problem. While global positioning system (GPS) receivers are commonly used in terrestrial WSNs to achieve this, this is infeasible in UWSNs as GPS signals do not propagate through water. Acoustic communications is the most promising mode of communication underwater. However, underwater acoustic channels are characterized by harsh physical layer conditions with low bandwidth, high propagation delay and high bit error rate. Moreover, the variable speed of sound and the non-negligible node mobility due to water currents pose a unique set of challenges for localization in UWSNs. In this paper, we provide a survey of techniques and challenges in localization specifically for UWSNs. We categorize them into (i) range-based vs. range-free techniques; (ii) techniques that rely on static reference nodes vs. those who also rely on mobile reference nodes, and (iii) single-stage vs. multi-stage schemes. We compare the schemes in terms of localization speed, accuracy, coverage and communication costs. Finally, we provide an outlook on the challenges that should be, but have yet been, addressed. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Tan, Hwee Pink and Diamant, Roee and Seah, Winston K.G. and Waldmeyer, Marc},
doi = {10.1016/j.oceaneng.2011.07.017},
file = {:home/miguel/Dropbox/Mendeley Desktop/Tan et al. - 2011 - A survey of techniques and challenges in underwater localization.pdf:pdf},
isbn = {6564082274},
issn = {00298018},
journal = {Ocean Engineering},
keywords = {Acoustic communications,Underwater localization,Underwater sensor networks},
number = {14-15},
pages = {1663--1676},
title = {{A survey of techniques and challenges in underwater localization}},
url = {www.elsevier.com/locate/oceaneng},
volume = {38},
year = {2011}
}
@techreport{Hamamatsu,
author = {Hamamatsu},
file = {:home/miguel/Dropbox/Mendeley Desktop/Hamamatsu - 2019 - MEMS mirrors technical information.pdf:pdf},
title = {{MEMS mirrors: technical information}},
url = {https://www.hamamatsu.com/resources/pdf/ssd/mems{\_}mirror{\_}koth9003e.pdf},
year = {2019}
}
@article{Himri2018,
abstract = {This paper presents a navigation and mapping system for an autonomous underwater vehicle (AUV) while operating near a man-made underwater environment. The objective is to recognize objects (or object parts) and use these as landmarks for simultaneous localization and mapping (SLAM). This approach is intended as the first step towards autonomous object manipulation, to be carried out at a later stage. The approach contains two main components: Object recognition from range data, and feature-based semantic SLAM. For the first component we propose an automatic method for the recognition and location of 3D objects using 3D point clouds as input, extracted from a laser scanner. Since it is common in inspection maintenance and repair (IMR) applications to have access to the 3D models of the objects of interest, the proposed method assumes a priori knowledge of the 3D models of these objects. In typical man-made environments, such objects can be distinct components of a structure, such as valves, pipes and wet-mateable connectors. The object recognition is based on recently proposed global descriptors for point clouds, that allow a compact description of the object shape, which is independent of the object view point. Once an object is recognized, its pose with respect to the AUV is determined using an ICP-based method. The second component of the approach is a feature based SLAM algorithm that uses the recognized objects as landmarks to improve the AUV navigation. The paper presents preliminary results obtained with the Girona 500 AUV, equipped with a fast laser scanner recently developed at the University of Girona. Tests conducted in a controlled environment (water tank) illustrate the suitability of the approach.},
author = {Himri, Khadidja and Ridao, Pere and Gracias, Nuno and Palomer, Albert and Palomeras, Narc{\'{i}}s and Pi, Roger},
doi = {10.1016/j.ifacol.2018.09.497},
file = {:home/miguel/Dropbox/Mendeley Desktop/Himri et al. - 2018 - Semantic SLAM for an AUV using object recognition from point clouds.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {3D global descriptor,3D object recognition,Ensemble of Shape Functions (ESF),Viewpoint Features Histogram (VFH),autonomous underwater vehicles (AUVs),laser scanner,mapping (SLAM),simultaneous localization},
number = {29},
pages = {360--365},
title = {{Semantic SLAM for an AUV using object recognition from point clouds}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2405896318321864},
volume = {51},
year = {2018}
}
@article{Schwarze2006,
author = {Schwarze, Craig},
journal = {Photonics Spectra},
pages = {67----71},
title = {{A new look at Risley prisms}},
volume = {40},
year = {2006}
}
@article{Smith2011,
abstract = {Ocean processes are dynamic, complex, and occur on multiple spatial and temporal scales. To obtain a synoptic view of such processes, ocean scientists collect data over long time periods. Historically, measurements were continually provided by fixed sensors, e.g., moorings, or gathered from ships. Recently, an increase in the utilization of autonomous underwater vehicles has enabled a more dynamic data acquisition approach. However, we still do not utilize the full capabilities of these vehicles. Here we present algorithms that produce persis- tent monitoring missions for underwater vehicles by balancing path following accuracy and sampling resolution for a given region of interest, which addresses a pressing need among ocean scientists to efficiently and effectively collect high-value data. More specifically, this paper proposes a path planning algorithm and a speed control algo- rithm for underwater gliders, which together give informative trajectories for the glider to persistently monitor a patch of ocean. We optimize a cost function that blends two com- peting factors: maximize the information value along the path, while minimizing deviation from the planned path due to ocean currents. Speed is controlled along the planned path by adjusting the pitch angle of the underwater glider, so that higher resolution samples are collected in areas of higher information value. The resulting paths are closed circuits that can be repeatedly traversed to collect long-term ocean data in dynamic environments. The algorithms were tested during sea trials on an underwater glider operating off the coast of southern California, as well as in Monterey Bay, California. The experimental results show improvements in both data resolution and path},
author = {Smith, Ryan N. and Schwager, Mac and Smith, Stephen L and Jones, Burton H and Rus, Daniela and Sukhatme, Gaurav S},
doi = {10.1002/rob.20405},
file = {:home/miguel/Dropbox/Mendeley Desktop/Smith et al. - 2011 - Persistent ocean monitoring with underwater gliders Adapting sampling resolution.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {5},
pages = {714--741},
title = {{Persistent ocean monitoring with underwater gliders: Adapting sampling resolution}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20405},
volume = {28},
year = {2011}
}
@article{Chi2016,
abstract = {In this study, a laser line auto-scanning system was designed to perform underwater close-range 3D reconstructions with high accuracy and resolution. The system changes the laser plane direction with a galvanometer to perform automatic scanning and obtain continuous laser strips for underwater 3D reconstruction. The system parameters were calibrated with the homography constraints between the target plane and image plane. A cost function was defined to optimize the galvanometer's rotating axis equation. Compensation was carried out for the refraction of the incident and emitted light at the interface. The accuracy and the spatial measurement capability of the system were tested and analyzed with standard balls under laboratory underwater conditions, and the 3D surface reconstruction for a sealing cover of an underwater instrument was proved to be satisfactory.},
author = {Chi, Shukai and Xie, Zexiao and Chen, Wenzhu},
doi = {10.3390/s16091534},
file = {:home/miguel/Dropbox/Mendeley Desktop/Chi, Xie, Chen - 2016 - A Laser Line auto-scanning system for underwater 3D reconstruction.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Auto-scanning,Laser line system,Underwater 3D reconstruction},
month = {sep},
number = {9},
pages = {1534},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{A Laser Line auto-scanning system for underwater 3D reconstruction}},
url = {http://www.mdpi.com/1424-8220/16/9/1534},
volume = {16},
year = {2016}
}
@techreport{Corrigan2001,
author = {Corrigan, Robert and Cook, Randy and Favotte, Olivier},
file = {:home/miguel/Dropbox/Mendeley Desktop/Corrigan, Cook, Favotte - 2001 - Silicon Light Machines™-Grating Light Valve™ Technology Brief Breakthrough MEMS Component Technolog.pdf:pdf},
institution = {Silicon Light Machines},
title = {{Silicon Light Machines™-Grating Light Valve™ Technology Brief Breakthrough MEMS Component Technology for Optical Networks}},
url = {http://www.siliconlight.com/wp-content/themes/siliconlight/pdf/glv-opcom-ver.pdf},
year = {2001}
}
@inproceedings{Dubrovinskaya2018,
abstract = {The in situ detection, recognition and tracking of marine animal species is a very important step of field research in the domains of, among others, biology and ecology. Still, the direct observation of marine wildlife through equipment operating in the visible light spectrum is often impaired by the challenging conditions offered by ocean waters, where light can be subject to scattering and attenuation phenomena due to the water turbidity. As the use of powerful lighting may prove ineffective and even induce behavioral changes in marine animals, the design of minimally or non-invasive observation instruments becomes particularly important. In this paper, we consider the serial Light Detection And Ranging (LiDAR) system under development at the Florida Atlantic University (FAU). This LiDAR design is based on inexpensive components and on low average power red lasers which are subject to significant attenuation in water, but are both eye-safe and invisible to marine life. Considering the challenge of detecting and evaluating the presence of marine wildlife, we present a full processing pipeline for LiDAR data, that includes water turbidity detection, non-gated backscattering compensation, contrast enhancement, and the construction of a three-dimensional model of the detected target. The pipeline is applied to a number of tank test data, under different turbidity conditions.},
author = {Dubrovinskaya, Elizaveta and Dalgleish, Fraser and Ouyang, Bing and Casari, Paolo},
booktitle = {2018 OCEANS - MTS/IEEE Kobe Techno-Oceans, OCEANS - Kobe 2018},
doi = {10.1109/OCEANSKOBE.2018.8559113},
file = {:home/miguel/Dropbox/Mendeley Desktop/Dubrovinskaya et al. - 2018 - Underwater LiDAR signal processing for enhanced detection and localization of marine life.pdf:pdf},
isbn = {9781538616543},
title = {{Underwater LiDAR signal processing for enhanced detection and localization of marine life}},
url = {http://eprints.networks.imdea.org/1817/1/PID5286019-1.pdf},
year = {2018}
}
@article{Maldonado1995,
abstract = {This volume features discussion of: Fundamental optics principles, including geometric, physical, and quantum optics; Optical sources (such as light emitting diodes and lasers) and detectors (including energy, waveform, and imaging detection techniques); Detection and processing by human vision; Information and image processing; Design and fabrication, including optical layout techniques and lens design programs; Optical thin film coatings; Terrestrial coatings.},
author = {Maldonado, Theresa A},
doi = {10.5860/CHOICE.32-5715},
file = {:home/miguel/Dropbox/Mendeley Desktop/Maldonado - 1995 - Electro-Optic modulators.pdf:pdf},
isbn = {007047740X},
issn = {0009-4978},
journal = {Handbook of optics},
title = {{Electro-Optic modulators}},
url = {http://photonics.intec.ugent.be/education/ivpv/res{\_}handbook/v2ch13.pdf},
volume = {2},
year = {1995}
}
@article{Ye2017a,
abstract = {Amicro scanning mirror is an optical device used to scan laser beams which can be used for Light Detection and Ranging (LiDAR) in applications like unmanned driving or Unmanned Aerial Vehicle (UAV). The MEMS scanning mirror's light-weight and low-power make it a useful device in LiDAR applications. However, the MEMS scanning mirror's small aperture limits its application because it is too small to deflect faint receiving light. In this paper, we present a Ti-alloy-based electromagnetic micro scanning mirror with very large-aperture (12 mm) and rapid scanning frequency (1.24 kHz). The size of micro-scanner's mirror plate reached 12 mm, which is much larger than familiar MEMS scanning mirror. The scanner is designed using MEMS design method and fabricated by electro-sparking manufacture method. As the experimental results show, the resonant frequency of the micro scanning mirror is 1240 Hz and the optical scanning angle can reach 26 degrees at resonance frequency when the actuation current is 250 mApp},
author = {Ye, Liangchen and Zhang, Gaofei and You, Zheng},
doi = {10.3390/mi8040120},
file = {:home/miguel/Dropbox/Mendeley Desktop/Ye, Zhang, You - 2017 - Large-aperture kHz operating frequency ti-alloy based optical micro scanning mirror for lidar Application.pdf:pdf},
issn = {2072666X},
journal = {Micromachines},
keywords = {Large-aperture,LiDAR,Micro scanner,Micro scanning mirror,Ti-alloy},
month = {apr},
number = {4},
pages = {120},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Large-aperture kHz operating frequency ti-alloy based optical micro scanning mirror for lidar Application}},
url = {http://www.mdpi.com/2072-666X/8/4/120},
volume = {8},
year = {2017}
}
@book{NRC2014,
author = {{National Research Council}},
booktitle = {Physics in Technology},
doi = {10.1088/0305-4624/12/2/408},
file = {:home/miguel/Dropbox/Mendeley Desktop/National Research Council - 2014 - Laser radar progress and opportunities in active electro-optical sensing.pdf:pdf},
isbn = {9780309302166},
issn = {03054624},
publisher = {National Academies Press},
title = {{Laser radar: progress and opportunities in active electro-optical sensing}},
year = {2014}
}
@inproceedings{Lopes2015,
abstract = {{\textcopyright} 2015 IEEE. In this work we propose the development of a stereo SLS system for underwater inspection operations. We demonstrate how to perform a SLS calibration both in dry and underwater environments using two different methods. The proposed methodology is able to achieve quite accurate results, lower than 1 mm in dry environments. We also display a 3D underwater scan of a known object size, a sea scallop, where the system is able to perform a scan with a global error lower than 2{\%} of the object size.},
author = {Lopes, Flavio and Silva, Hugo and Almeida, Jose Miguel and Martins, Alfredo and Silva, Eduardo},
booktitle = {MTS/IEEE OCEANS 2015 - Genova: Discovering Sustainable Ocean Energy for a New World},
doi = {10.1109/OCEANS-Genova.2015.7271564},
file = {:home/miguel/Dropbox/Mendeley Desktop/Lopes et al. - 2015 - Structured light system for underwater inspection operations.pdf:pdf},
isbn = {9781479987368},
keywords = {Camera,Line laser,Structured light system,Underwater vision system},
month = {may},
pages = {1--6},
publisher = {IEEE},
title = {{Structured light system for underwater inspection operations}},
url = {http://ieeexplore.ieee.org/document/7271564/},
year = {2015}
}
@article{Eustice2008,
abstract = {As autonomous underwater vehicles (AUVs) are becoming routinely used in an exploratory context for ocean science, the goal of visually augmented navigation (VAN) is to improve the near-seafloor navigation precision of such vehicles without imposing the burden of having to deploy additional infrastructure. This is in contrast to traditional acoustic long baseline navigation techniques, which require the deployment, calibration, and eventual recovery of a transponder network. To achieve this goal, VAN is formulated within a vision-based simultaneous localization and mapping (SLAM) framework that exploits the systems-level complementary aspects of a camera and strap-down sensor suite. The result is an environmentally based navigation technique robust to the peculiarities of low-overlap underwater imagery. The method employs a view-based representation where camera-derived relative-pose measurements provide spatial constraints, which enforce trajectory consistency and also serve as a mechanism for loop closure, allowing for error growth to be independent of time for revisited imagery. This article outlines the multisensor VAN framework and demonstrates it to have compelling advantages over a purely vision-only approach by: 1) improving the robustness of low-overlap underwater image registration; 2) setting the free gauge scale; and 3) allowing for a disconnected camera-constraint topology.},
author = {Eustice, Ryan M. and Pizarro, Oscar and Singh, Hanumant},
doi = {10.1109/JOE.2008.923547},
file = {:home/miguel/Dropbox/Mendeley Desktop/Eustice, Pizarro, Singh - 2008 - Visually augmented navigation for autonomous underwater vehicles.pdf:pdf},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Computer vision,Mobile robotics,Navigation,Robotic perception,Simultaneous localization and mapping SLAM),Underwater vehicles},
month = {apr},
number = {2},
pages = {103--122},
title = {{Visually augmented navigation for autonomous underwater vehicles}},
url = {http://ieeexplore.ieee.org/document/4625213/},
volume = {33},
year = {2008}
}
@article{Eric2013,
abstract = {Documenting underwater cultural heritage is a challenging undertaking. Underwater environment is not a man's natural habitat and special equipment and devices had to be invented so that he could enter and study this environment. Several decades of underwater research and many sacrifices were needed to fully understand the importance of underwater heritage and its protection. The means for accurate documentation underwater are very limited and demanding, due to required technical equipment it is also expensive. Emergence of modern 3D methods and accompanying software tools for processing of 3D data is therefore of utmost importance for documenting and protection of underwater cultural heritage. In comparison to manual and analog methods, 3D methods offer much better accuracy, they substantially shorten the necessary time spent underwater and in this way improve the safety at work as well as lower the entire cost of field work. For illustration of the above development we discuss archeological case studies from the North East Adriatic.},
author = {Eri{\v{c}}, Miran and Kova{\v{c}}i{\v{c}}, Rok and Berginc, Gregor and Pugelj, Mitja and Stopin{\v{s}}ek, {\v{Z}}iga and Solina, Franc},
doi = {10.1109/DigitalHeritage.2013.6744765},
file = {:home/miguel/Dropbox/Mendeley Desktop/Eri{\v{c}} et al. - 2013 - The impact of the latest 3D technologies on the documentation of underwater heritage sites.pdf:pdf},
isbn = {9781479931699},
journal = {Proceedings of the DigitalHeritage 2013 - Federating the 19th Int'l VSMM, 10th Eurographics GCH, and 2nd UNESCO Memory of the World Conferences, Plus Special Sessions fromCAA, Arqueologica 2.0 et al.},
pages = {281--288},
title = {{The impact of the latest 3D technologies on the documentation of underwater heritage sites}},
volume = {2},
year = {2013}
}
@article{Guo2017,
abstract = {With the goal of supporting close-range observation tasks of a spherical amphibious robot, such as ecological observations and intelligent surveillance, a moving target detection and tracking system was designed and implemented in this study. Given the restrictions presented by the amphibious environment and the small-sized spherical amphibious robot, an industrial camera and vision algorithms using adaptive appearance models were adopted to construct the proposed system. To handle the problem of light scattering and absorption in the underwater environment, the multi-scale retinex with color restoration algorithm was used for image enhancement. Given the environmental disturbances in practical amphibious scenarios, the Gaussian mixture model was used to detect moving targets entering the field of view of the robot. A fast compressive tracker with a Kalman prediction mechanism was used to track the specified target. Considering the limited load space and the unique mechanical structure of the robot, the proposed vision system was fabricated with a low power system-on-chip using an asymmetric and heterogeneous computing architecture. Experimental results confirmed the validity and high efficiency of the proposed system. The design presented in this paper is able to meet future demands of spherical amphibious robots in biological monitoring and multi-robot cooperation.},
author = {Guo, Shuxiang and Pan, Shaowu and Shi, Liwei and Guo, Ping and He, Yanlin and Tang, Kun},
doi = {10.3390/s17040870},
file = {:home/miguel/Dropbox/Mendeley Desktop/Guo et al. - 2017 - Visual detection and tracking system for a spherical amphibious robot.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Gaussian mixture model,Moving target detection,Spherical amphibious robot,System-on-chip (SoC),Visual tracking},
month = {apr},
number = {4},
pages = {870},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Visual detection and tracking system for a spherical amphibious robot}},
url = {http://www.mdpi.com/1424-8220/17/4/870},
volume = {17},
year = {2017}
}
@article{Andersson2005,
abstract = {The purpose of this master thesis, performed at FOI, was to evaluate a range gated underwater camera, for the application identification of bottom objects. The master thesis was supported by FMV wi ...},
author = {Andersson, Adam},
file = {:home/miguel/Dropbox/Mendeley Desktop/Andersson - 2005 - Range Gated Viewing with Underwater Camera.pdf:pdf},
journal = {Master's Thesis, Link{\"{o}}ping University},
keywords = {Computer Vision and Robotics (Autonomous Systems),Datorseende och robotik (autonoma system),Technology,teknik},
publisher = {Institutionen f{\"{o}}r systemteknik},
title = {{Range Gated Viewing with Underwater Camera}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2{\%}3A20570{\&}dswid=1789},
year = {2005}
}
@article{Skarlatos2012,
abstract = {Underwater photogrammetry is a difficult task by definition. When time limits are imposed on both acquisition and processing time, it becomes even more complicated. In addition when resources are limited and the main deliverable should be a complete 3D model, only a novel method may match the requirements. Under the aforementioned conditions, a new method using a combination of photogrammetry and computer vision techniques was utilised using open source software to face demands.},
author = {Skarlatos, D and Demestiha, S and Kiparissi, S},
doi = {10.1260/2047-4970.1.1.1},
file = {:home/miguel/Dropbox/Mendeley Desktop/Skarlatos, Demestiha, Kiparissi - 2012 - An ‘Open' Method for 3D Modelling and Mapping in Underwater Archaeological Sites.pdf:pdf},
issn = {2047-4970},
journal = {International Journal of Heritage in the Digital Era},
keywords = {3D reconstruction,Underwater,archaeology,computer vision,open software,photogrammetry},
number = {1},
pages = {1--24},
title = {{An ‘Open' Method for 3D Modelling and Mapping in Underwater Archaeological Sites}},
volume = {1},
year = {2012}
}
@article{Schiller2016,
abstract = {In the course of extended hydrological studies in the coastal Karst plain of Yucatan, near the town of Tulum amongst others, a novel laser scanning device was developed and applied for the acquisition of the 3d-geometry of ground water conduits. The method is derived from similar industrial systems and for the first time adapted to the specific measurement conditions in underwater cave systems. The device projects a laser line over the whole perimeter at a certain position. This line represents the intersection of a plane with the cave walls. The line is imaged with a wide angle camera system. Through proper design and calibration of the device it is possible to derive the true scale geometry of the perimeter via special image processing techniques. By acquiring regularly spaced images it is possible to reconstruct the true scale and 3 d-shape of a tunnel through the incorporation of location and attitude data. In a first test in the Ox Bel Ha underwater cave system, about 800 metres of tunnels have been scanned down to water depths of 20 metres. The raw data is further interpolated using the ODSIM-algorithm in order to delineate the 3D geometry of the cave system. The method provides easy, operable acquisition of the 3-D geometry of caves in clear water with superior resolution and speed and significantly facilitates the measurement in underwater tunnels as well as in dry tunnels. The data gathered represents crucial input to the study of the state, dynamics and genesis of the complex karst water regime. Un dispositivo l{\'{a}}ser {\'{o}}ptico para la cartograf{\'{i}}a 3D de la geometr{\'{i}}a de estructuras k{\'{a}}rsticas submarinas: primeros resultados en el sistema de Ox Bel'Ha, Yucat{\'{a}}n, M{\'{e}}xico. RESUMEN Durante el transcurso de intensivos estudios hidrol{\'{o}}gicos realizados en la llanura costera k{\'{a}}rstica de Yucat{\'{a}}n, cerca de la ciudad de Tulum entre otras, se desarroll{\'{o}} un novedoso dispositivo de escaneo l{\'{a}}ser, que se apli-c{\'{o}} a la adquisici{\'{o}}n de la geometr{\'{i}}a 3D de conductos de agua subterr{\'{a}}nea. El m{\'{e}}todo se deriva de sistemas industriales similares y que ha sido adaptado por primera vez a las condiciones de medici{\'{o}}n espec{\'{i}}ficas de los sistemas de cuevas submarinas. El dispositivo proyecta una l{\'{i}}nea l{\'{a}}ser sobre todo el per{\'{i}}metro en una localizaci{\'{o}}n dada. Esta l{\'{i}}nea representa la intersecci{\'{o}}n de un plano con las paredes de las cuevas. La l{\'{i}}nea es fotografiada con un sistema de c{\'{a}}mara de gran angular. A trav{\'{e}}s de un apropiado dise{\~{n}}o y calibraci{\'{o}}n del dispositivo es posible obtener la geometr{\'{i}}a verdadera del per{\'{i}}metro a trav{\'{e}}s de t{\'{e}}cnicas especiales de procesamiento de im{\'{a}}genes. De este modo, adquiriendo regularmente im{\'{a}}genes a intervalos espaciados es posible reconstruir la escala verdadera y la forma 3D de un t{\'{u}}nel con la incorporaci{\'{o}}n de los datos de posici{\'{o}}n e inclinaci{\'{o}}n. En una primera prueba en el sistema de la cueva submarina Ox Bel Ha, se escanearon},
author = {Schiller, Arnulf and Renard, Philippe},
file = {:home/miguel/Dropbox/Mendeley Desktop/Schiller, Renard - 2016 - An optical laser device for mapping 3D geometry of underwater karst structures First tests in the Ox Bel'Ha sy.pdf:pdf},
journal = {Bolet{\'{i}}n Geol{\'{o}}gico y Minero},
keywords = {3D,Karst,cave,geomorphology,geostatistics,laser scanner},
number = {1},
pages = {99--110},
title = {{An optical laser device for mapping 3D geometry of underwater karst structures: First tests in the Ox Bel'Ha system, Yucatan, Mexico.}},
url = {http://doc.rero.ch/record/306721/files/Schiller{\_}A-An{\_}optical{\_}laser{\_}device-20180119.pdf},
volume = {127},
year = {2016}
}
@inproceedings{Patterson2004,
abstract = {An overview of the current state of the art in scanning micromirror technology for switching, imaging, and beam steering applications is presented. The requirements that drive the design and fabrication technology are covered. Electrostatic, electromagnetic, and magnetic actuation techniques are discussed as well as the motivation toward combdrive configurations from parallel plate configurations for large diameter (mm range) scanners. Suitability of surface micromachining, bulk micromachining, and silicon on insulator (SOI) micromachining technology is presented in the context of the length scale and performance for given scanner applications.},
author = {Patterson, Pamela R and Fujino, Makoto and Piyawattanametha, Wibool and Hah, Dooyoung and Wu, Ming C},
booktitle = {Optomechatronic Micro/Nano Components, Devices, and Systems},
doi = {10.1117/12.582849},
file = {:home/miguel/Dropbox/Mendeley Desktop/Patterson et al. - 2004 - Scanning micromirrors an overview.pdf:pdf},
keywords = {MEMS,MOEMS,Scanning micromirror,optical switching,vertical combdrive},
pages = {195},
title = {{Scanning micromirrors: an overview}},
url = {http://spiedl.org/terms},
volume = {5604},
year = {2004}
}
@misc{Stettner1993,
author = {Stettner, Roger and Bailey, Howard W},
file = {:home/miguel/Dropbox/Mendeley Desktop/Stettner, Bailey - 1993 - 3D IMAGING UNDERWATER LASER RADAR, US005446529A(2).pdf:pdf},
number = {15},
pages = {225},
title = {{3D IMAGING UNDERWATER LASER RADAR, US005446529A}},
url = {https://patentimages.storage.googleapis.com/07/9c/b4/3adf0a7aca0714/US5696577.pdf https://patentimages.storage.googleapis.com/f8/1c/f7/e100067dba3361/US5446529.pdf},
volume = {627},
year = {1993}
}
@article{Chirayath2016,
abstract = {* The use of fluid lensing technology on unmanned aerial vehicles (UAVs, or drones) is presented as a novel means for 3D imaging of aquatic ecosystems from above the water's surface at the centimetre scale. Preliminary results are presented from airborne fluid lensing campaigns conducted over the coral reefs of Ofu Island, American Samoa (2013) and the stromatolite reefs of Shark Bay, Western Australia (2014), covering a combined area of 15 km2. These reef ecosystems were revealed with centimetre-scale 2D resolution, and an accompanying 3D bathymetry model was derived using fluid lensing, Structure from Motion and UAV position data. Data products were validated from in situ survey methods including underwater calibration targets, depth measurements and millimetre-scale high-dynamic-range gigapixel photogrammetry.$\backslash$n$\backslash$n* Fluid lensing is an experimental technology that uses water-transmitting wavelengths to passively image underwater objects at high-resolution by exploiting time-varying optical lensing events caused by surface waves. Fluid lensing data are captured from low-altitude, cost-effective electric UAVs to achieve multispectral imagery and bathymetry models at the centimetre scale over regional areas. As a passive system, fluid lensing is presently limited by signal-to-noise ratio and water column inherent optical properties to {\~{}}10 m depth over visible wavelengths in clear waters.$\backslash$n$\backslash$n* The datasets derived from fluid lensing present the first centimetre-scale images of a reef acquired from above the ocean surface, without wave distortion. The 3D multispectral data distinguish coral, fish and invertebrates in American Samoa, and reveal previously undocumented, morphologically distinct, stromatolite structures in Shark Bay. These findings suggest fluid lensing and multirotor electric drones represent a promising advance in the remote sensing of aquatic environments at the centimetre scale, or ‘reef scale' relevant to the conservation of reef ecosystems. Pending further development and validation of fluid lensing methods, these technologies present a solution for large-scale 3D surveys of shallow aquatic habitats with centimetre-scale spatial resolution and hourly temporal sampling.$\backslash$nCopyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
author = {Chirayath, Ved and Earle, Sylvia A},
doi = {10.1002/aqc.2654},
file = {:home/miguel/Dropbox/Mendeley Desktop/Chirayath, Earle - 2016 - Drones that see through waves – preliminary results from airborne fluid lensing for centimetre-scale aquatic.pdf:pdf},
issn = {10990755},
journal = {Aquatic Conservation: Marine and Freshwater Ecosystems},
keywords = {American Samoa,Shark Bay,airborne remote sensing,coastal bathymetry,coral reef,fluid lensing,stromatolite},
pages = {237--250},
title = {{Drones that see through waves – preliminary results from airborne fluid lensing for centimetre-scale aquatic conservation}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aqc.2654},
volume = {26},
year = {2016}
}
@misc{Taketomi2017,
abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
booktitle = {IPSJ Transactions on Computer Vision and Applications},
doi = {10.1186/s41074-017-0027-2},
file = {:home/miguel/Dropbox/Mendeley Desktop/Taketomi, Uchiyama, Ikeda - 2017 - Visual SLAM algorithms A survey from 2010 to 2016.pdf:pdf},
issn = {18826695},
keywords = {Augmented reality,Computer vision,Robotics,Survey,Visual SLAM},
pages = {16},
title = {{Visual SLAM algorithms: A survey from 2010 to 2016}},
volume = {9},
year = {2017}
}
@inproceedings{Bodenmann2011,
abstract = {This paper extends the method introduced in [1] to generate 3D colour reconstructions of the seafloor, to account for the attenuation of light for each individual point in the reconstruction and so improve the colour accuracy of the reconstructed seafloor. The method is applied to data obtained using an AUV during the survey of a hydrothermally active area in Kagoshima Bay, Japan. The generated 3D reconstruction is compared to a 2D photomosaic taken at the same time. The results are discussed in terms of accuracy of dimensions and colour and a method to extract scientifically useful information from the 3D data is demonstrated.},
author = {Bodenmann, Adrian and Thornton, Blair and Nakatani, Takeshi and Ura, Tamaki},
booktitle = {OCEANS'11 MTS/IEEE KONA},
doi = {10.23919/oceans.2011.6107059},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bodenmann et al. - 2011 - 3D colour reconstruction of a hydrothermally active area using an underwater robot.pdf:pdf},
isbn = {978-1-4577-1427-6},
month = {sep},
pages = {1----6},
publisher = {IEEE},
title = {{3D colour reconstruction of a hydrothermally active area using an underwater robot}},
url = {http://ieeexplore.ieee.org/document/6107059/},
year = {2011}
}
@article{Caimi2007,
abstract = {Laser line scan (LLS) systems rely upon narrow angular field apertures and displacement between emitter and receiver to reduce volume backscatter from the common volume near the target. With this configuration, multiple volume scattering near the sensor becomes a significant contribution to the total backscatter return. Detection of continuous wave laser return signals is complicated at the range limit of these systems due to the temporal overlap between the target return and the backscatter return. In the compact implementation of these systems, necessary when the deployment platform is the 12" or 21" diameter AUV, this detection ambiguity can lead to a significant degradation in imaging performance. Detection methods must therefore separate the two signals and estimate the energy returning from the target alone. Indeed, the use of pulsed laser illumination under some conditions allows for temporal separation of the target and volume scattering return signals. However, this method relies upon knowledge of the target distance and establishment of a gated detection scheme. We present a comparison of simulation results for both the CW and pulsed-laser cases in a range of turbid water conditions and at differing optical attenuation lengths approaching the limiting case for these types of underwater imagers. These simulations are performed using models developed by Metron Inc. (Reston, VA), and allow both a time history and image quality comparison. The goal of this simulation work is to identify the operational and environmental conditions under which each scheme is usable for the AUV-deployed scenario. Experimental results supporting these findings will be presented as available.},
author = {Caimi, Frank Michael and Dalgleish, Fraser R. and Giddings, Thomas E. and Shirron, Joseph J. and Mazel, Charles and Chiang, Kophu},
doi = {10.1109/OCEANSE.2007.4302476},
file = {:home/miguel/Dropbox/Mendeley Desktop/Caimi et al. - 2007 - Pulse versus CW Laser Line Scan Imaging Detection Methods Simulation Results.pdf:pdf},
isbn = {1424406358},
journal = {OCEANS 2007 - Europe},
month = {jun},
pages = {1--4},
publisher = {IEEE},
title = {{Pulse versus CW Laser Line Scan Imaging Detection Methods: Simulation Results}},
url = {http://ieeexplore.ieee.org/document/4302476/ git},
year = {2007}
}
@misc{Song2018,
abstract = {The aim of this paper is to provide a review of micromirror array (MMA) technologies (2631 MMA research papers and patents were reviewed for this effort). The performance capabilities of 277 MMA designs from 49 companies and 23 academic research groups are categorized and compared. The designs are categorized according to (i) their array's dimension (e.g., 2D arrays consisting of mirrors that cover a surface, 1D arrays consisting of mirrors in a row, and 0D arrays consisting of only a single mirror), (ii) the nature of the surface of their mirrors (e.g., continuous or discrete), (iii) what combination of tip, tilt, and/or piston degrees of freedom (DOFs) they achieve, and (iv) how they are actuated. Standardized performance metrics that can be systematically applied to every MMA design (e.g., mirror area, fill factor, pitch, range of motion, maximum acceleration, actuator energy density, and number of uncontrolled DOFs) are defined and plotted for existing designs to enable their fair comparison. Theoretical bounds on what is physically possible for MMAs to achieve are also derived and depicted in these plots to highlight the amount of performance improvement that remains to be achieved by future designs and guidelines are provided to aid in the development of these future designs.},
author = {Song, Yuanping and Panas, Robert M. and Hopkins, Jonathan B.},
booktitle = {Precision Engineering},
doi = {10.1016/j.precisioneng.2017.08.012},
file = {:home/miguel/Dropbox/Mendeley Desktop/Song, Panas, Hopkins - 2018 - A review of micromirror arrays.pdf:pdf},
issn = {01416359},
keywords = {Arrays of mirrors,Flexures,MEMS,Micro-actuators,Micro-reflectors,Micromirrors},
month = {jan},
pages = {729--761},
publisher = {Elsevier},
title = {{A review of micromirror arrays}},
url = {https://www.sciencedirect.com/science/article/pii/S0141635917302210},
volume = {51},
year = {2018}
}
@inproceedings{Stachniss,
abstract = {The pose graph is a central data structure in graph-based SLAM approaches. It encodes the poses of the robot during data acquisition as well as spatial constraints between them. The size of the pose graph has a direct influence on the runtime and the memory requirements of a SLAM system since it is typically used to make data associations and within the optimization procedure. In this paper, we address the problem of efficient, information-theoretic compression of such pose graphs. The central question is which sensor measurements can be removed from the graph without loosing too much information. Our approach estimates the expected information gain of laser measurements with respect to the resulting occupancy grid map. It allows us to restrict the size of the pose graph depending on the information that the robot acquires about the environment. Alternatively, we can enforce a maximum number of laser scans the robot is allowed to store, which results in an any-space SLAM system. Real world experiments suggest that our approach efficiently reduces the growth of the pose graph while minimizing the loss of information in the resulting grid map.},
author = {Stachniss, Cyrill and Kretzschmar, Henrik},
booktitle = {Springer Tracts in Advanced Robotics},
doi = {10.1007/978-3-319-29363-9_16},
file = {:home/miguel/Dropbox/Mendeley Desktop/Stachniss, Kretzschmar - 2017 - Pose graph compression for laser-based SLAM.pdf:pdf},
isbn = {9783319293622},
issn = {1610742X},
pages = {271--287},
title = {{Pose graph compression for laser-based SLAM}},
volume = {100},
year = {2017}
}
@article{Zohrabi2016,
abstract = {Nonmechanical beam steering is a rapidly growing branch of adaptive optics with applications such as light detection and ranging, imaging, optical communications, and atomic physics. Here, we present an innovative technique for one- and two-dimensional beam steering using multiple tunable liquid lenses. We use an approach in which one lens controls the spot divergence, and one to two decentered lenses act as prisms and steer the beam. Continuous 1D beam steering was demonstrated, achieving steering angles of ±39° using two tunable liquid lenses. The beam scanning angle was further enhanced to ±75° using a fisheye lens. By adding a third tunable liquid lens, we achieved 2D beam steering of ±75°. In this approach, the divergence of the scanning beam is controlled at all steering angles.},
author = {Zohrabi, Mo and Cormack, Robert H and Gopinath, Juliet T.},
doi = {10.1364/oe.24.023798},
file = {:home/miguel/Dropbox/Mendeley Desktop/Zohrabi, Cormack, Gopinath - 2016 - Wide-angle nonmechanical beam steering using liquid lenses.pdf:pdf},
isbn = {1094-4087 (Electronic) 1094-4087 (Linking)},
journal = {Optics Express},
number = {21},
pages = {23798},
pmid = {27828216},
title = {{Wide-angle nonmechanical beam steering using liquid lenses}},
url = {https://www.osapublishing.org/DirectPDFAccess/A23B9C84-C8D8-C59E-D996F23A4CBE60C3{\_}350914/oe-24-21-23798.pdf?da=1{\&}id=350914{\&}seq=0{\&}mobile=no},
volume = {24},
year = {2016}
}
@incollection{Dalgleish2018,
author = {Dalgleish, Fraser R. and Ouyang, Bing and Vuorenkoski, Anni K. and Ramos, Brian and Li, Yanjun and Cao, Zheng and Principe, Jose},
booktitle = {Unconventional Optical Imaging},
doi = {10.1117/12.2309997},
file = {:home/miguel/Dropbox/Mendeley Desktop/Dalgleish et al. - 2018 - MEMS-based serial LiDAR detection and imaging architecture for automated surveillance of undersea marine li(2).pdf:pdf},
pages = {413 ---- 419},
publisher = {SPIE},
title = {{MEMS-based serial LiDAR detection and imaging architecture for automated surveillance of undersea marine life}},
url = {https://www.researchgate.net/profile/Fraser{\_}Dalgleish/publication/324896824{\_}MEMS-based{\_}serial{\_}LiDAR{\_}detection{\_}and{\_}imaging{\_}architecture{\_}for{\_}automated{\_}surveillance{\_}of{\_}undersea{\_}marine{\_}life/links/5bce1c7e4585152b144e7168/MEMS-based-serial-LiDAR-detection-and-},
year = {2018}
}
@inproceedings{Milanovic2017,
abstract = {Coughlan, A.T., Schmidt, R.M., 1985. Executive compensation, management turnover, and firm performance. Journal of Accounting and Economics 7, 43–66.},
author = {Milanovi{\'{c}}, Veljko and Kasturi, Abhishek and Yang, James and Hu, Frank},
booktitle = {Laser Radar Technology and Applications XXII},
doi = {10.1117/12.2264069},
editor = {Turner, Monte D. and Kamerman, Gary W.},
file = {:home/miguel/Dropbox/Mendeley Desktop/Milanovi{\'{c}} et al. - 2017 - Closed-loop control of gimbal-less MEMS mirrors for increased bandwidth in LiDAR applications.pdf:pdf},
isbn = {9781510608832},
issn = {1996756X},
keywords = {LiDAR,MEMS mirror,beam steering,closed-loop control,laser range finder,micromirror,scanned LiDAR},
month = {may},
pages = {101910N},
publisher = {International Society for Optics and Photonics},
title = {{Closed-loop control of gimbal-less MEMS mirrors for increased bandwidth in LiDAR applications}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2264069},
volume = {10191},
year = {2017}
}
@inproceedings{Li1985,
abstract = {A low dark current, high speed InGaAs PIN photodiode suitable for optoelectroinc integration has been fabricated by LPE method on sem-insulating InP substrate. The photodiode has a low dark current density of 2.5x10-6A/cm2 at -10V. At the operating voltage of -5V, external quantum efficiency of {\textgreater}90{\%} at 1.3um and {\textgreater}83{\%} at 1.55um, a rise time of {\textless}35ps and a FWHM of {\textless}45 ps have been measured. The approach and limitations to the design of a higher speed PIN photodiode will be discussed. The importance of packaging is emphasized, and pulse distortion due to dispersion in miscrostrip line will be discussed.},
author = {Li, Kenneth K. and Law, H.David},
booktitle = {Ultrashort Pulse Spectroscopy and Applications},
doi = {10.1117/12.946550},
editor = {Soileau, M. J.},
month = {apr},
pages = {126},
publisher = {International Society for Optics and Photonics},
title = {{Picosecond Ingaas PIN Photodiode For 0.95um-1.65um Operation}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.946550},
volume = {0533},
year = {2012}
}
@inproceedings{Narasimhan2005a,
abstract = {Virtually all structured light methods in computer vision assume that the scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing application in underwater and aerial imaging, where scattering effects cannot be ignored. In this paper, we present a comprehensive analysis of two representative methods - light stripe range scanning and photometric stereo - in the presence of scattering. For both methods, we derive physical models for the appearances of a surface immersed in a scattering medium. Based on these models, we present results on (a) the condition for object detectability in light striping and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while traditional methods fail when scattering is significant, our methods accurately recover the scene (depths, normals, albedos) as well as the properties of the medium. These results are in turn used to restore the appearances of scenes as if they were captured in clear air. Although we have focused on light striping and photometric stereo, our approach can also be extended to other methods such as grid coding, gated and active polarization imaging.},
author = {Narasimhan, Srinivasa G. and Nayar, Shree K.},
booktitle = {Proceedings of MTS/IEEE OCEANS, 2005},
doi = {10.1109/OCEANS.2005.1640165},
file = {:home/miguel/Dropbox/Mendeley Desktop/Narasimhan, Nayar - 2005 - Structured light methods for underwater imaging Light stripe scanning and photometric stereo.pdf:pdf},
isbn = {0933957343},
pages = {1--8},
publisher = {IEEE},
title = {{Structured light methods for underwater imaging: Light stripe scanning and photometric stereo}},
url = {http://ieeexplore.ieee.org/document/1640165/},
volume = {2005},
year = {2005}
}
@techreport{Bellet2013,
abstract = {In this paper, we are interested in imaging a 3D scene from a collection of 2D laser images, using advanced tomographic methods. We rst recall classical results about transmission tomography, including the famous FDK algorithm for cone-beam scanning. Then we use a pinhole camera model to describe the laser experiment of interest. Since this model is geometrically speaking similar with the cone-beam scanning, we use the FDK algorithm as a heuristic to reconstruct the scene. We show numerically, on real data, that this heuristic reconstructs the skeleton of an object to be imaged, even if the object is occulted.},
author = {Bellet, Jean-baptiste and Berginc, G{\'{e}}rard},
file = {:home/miguel/Dropbox/Mendeley Desktop/Bellet, Berginc - 2013 - 3D laser imaging by backprojection.pdf:pdf},
institution = {HAL},
title = {{3D laser imaging by backprojection}},
url = {https://www.researchgate.net/publication/266137973},
year = {2013}
}
@article{Yang2014,
abstract = {Laser line scanning (LLS) as an optical 3D survey method is recently used for underwater 3D imaging. This paper presents an approach for both 3D reconstruction and color restoration based on the LLS system. The system is set up with RGB laser scanning components for acquiring 3D data and color data at the same time. Direct calibration algorithms and RGB additive color model are involved for processing 3D color data. The experiment result shows that underwater targets can be feasibly 3D reconstructed with color texture simultaneously, at millimeter scales.},
author = {Yang, Yu and Zheng, Bing and Kan, Ling Yan and Yu, Jia and Wang, Jin Cheng},
doi = {10.1016/j.ijleo.2014.07.072},
file = {:home/miguel/Dropbox/Mendeley Desktop/Yang et al. - 2014 - 3D color reconstruction based on underwater RGB laser line scanning system.pdf:pdf},
issn = {00304026},
journal = {Optik},
keywords = {Calibration,Color measurement,Three-dimensional image acquisition,Three-dimensional reconstruction,Underwater imaging},
number = {20},
pages = {6074--6077},
title = {{3D color reconstruction based on underwater RGB laser line scanning system}},
volume = {125},
year = {2014}
}
@article{Kocak2008,
abstract = {Advancements in the field of underwater optical imaging are reviewed for the years 2005 to present. A synopsis of research and technical innovations is presented, organized in much the same way as the previous report (Kocak and Caimi, 2005). Several recent applications of novel systems are shown as examples, and trends in emerging underwater imaging research and development are briefly summarized.},
author = {Kocak, Donna M. and Dalgleish, Fraser R. and Caimi, Frank Michael and Schechner, Yoav Y},
doi = {10.4031/002533208786861209},
file = {:home/miguel/Dropbox/Mendeley Desktop/Kocak et al. - 2008 - A Focus on Recent Developments and Trends in Underwater Imaging.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {00253324},
journal = {Marine Technology Society Journal},
number = {1},
pages = {52--67},
pmid = {18787165},
title = {{A Focus on Recent Developments and Trends in Underwater Imaging}},
url = {https://www.researchgate.net/publication/233622946 http://openurl.ingenta.com/content/xref?genre=article{\&}issn=0025-3324{\&}volume=42{\&}issue=1{\&}spage=52},
volume = {42},
year = {2008}
}
@inproceedings{Behley,
abstract = {Accurate and reliable localization and mapping is a fundamental building block for most autonomous robots. For this purpose, we propose a novel, dense approach to laserbased mapping that operates on three-dimensional point clouds obtained from rotating laser sensors. We construct a surfel-based map and estimate the changes in the robot's pose by exploiting the projective data association between the current scan and a rendered model view from that surfel map. For detection and veriﬁcation of a loop closure, we leverage the map representation to compose a virtual view of the map before a potential loop closure, which enables a more robust detection even with low overlap between the scan and the already mapped areas. Our approach is efﬁcient and enables real-time capable registration. At the same time, it is able to detect loop closures and to perform map updates in an online fashion. Our experiments show that we are able to estimate globally consistent maps in large scale environments solely based on point cloud data.},
author = {Behley, Jens and Stachniss, Cyrill},
doi = {10.15607/rss.2018.xiv.016},
file = {:home/miguel/Dropbox/Mendeley Desktop/Behley, Stachniss - 2018 - Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments.pdf:pdf},
title = {{Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments}},
url = {https://jbehley.github.io/projects/surfel{\_}mapping/},
year = {2018}
}
